[{"title":"短链接系统设计","date":"2020-09-24T13:45:20.000Z","path":"2020/09/24/短链系统设计/","text":"本文会讨论短链接系统设计中的一些关键细节。 作用短链系统用于为长链接创建较短的别名，这些别名叫做短链接。当用户点击短链接时，他们会被重定向到原始 URL。 短链接在展示、发送博客时可以节省大量空间，另外，较短的 URL 更方便用户输入。 例如我们可以通过 tinyUrl 缩短链接 https://store.steampowered.com/app/1375870/Comrade_Trumps_Reelection/ 为 https://tinyurl.com/yx93wn76 。 系统的要求与目标功能需求 对于给定 URL，服务应该生成一个较短并且唯一的别名。 用户访问短链接时，服务应该将其重定向到原始链接。 用户可以为原 URL选择自定义的短链接。 短链接在默认时间间隔后过期，用户应该能够指定到期时间。 非功能需求： 系统高可用，因为如果服务不可用，会导致所有 URL 重定向失败。 URL 重定向应该以最小的延迟进行。 生成的短链接应该是不可预测的。 扩展要求： 分析每个短链接的重定向会发生了多少次？ 服务应该提供 REST API 接口给其他服务调用。 流量估算与约束短链接系统是多读少写的系统。对于生成的短链接会被重定向很多次，假设每个短链接的读写比为 100:1。 流量估算假设每个月会生成 500M 个新的短链接，由于读写比例为 100:1，所以一个月期望的重定向次数为 50B。 估算*短链接生成接口 QPS 为 * $$500M/ (30 days * 24 hours * 3600 second) ≈ 200 URLs/s$$URL 重定向接口 QPS 为 $$100 * 200URLs/s = 20K/s$$ 容量估算假设每个原始链接及其短链接被存储 5 years，每个月会有 500M 个新链接产生，所以总共需要存储的对象为：$$500 million * 5 years * 12 months = 30 billion$$ 假设每个原始链接及其短链接大约为 500 个字节，需要的总存储空间为 $$30 billion * 500 bytes = 15 TB$$ 带宽估算：每秒会传入 200 个新 URL，因此每秒传入服务的数据流量为： $$200 * 500 bytes = 100 KB/s$$ 每秒传出的数据流量为： $$100 * 100 KB/s = 10MB/s$$ 内存估算对于经常访问的热点数据，需要进行缓存。每秒有 20K 的读取请求，因此每天收到的读取总请求数为： $$20 K * 24 hours * 3600 seconds ≈ 1.7 billion$$根据二八定律，我们选择缓存其中 20% 的请求，使用到的内存为： $$0.2 * 1.7 billion * 500 bytes ≈ 170 GB$$因为在 170 亿请求中，会有大量的重复请求，所以实际使用到的内存会小于 170GB。 估算汇总汇总上述估算数据，得到如下表格： 指标 数据量 新网址数量 200 /s 重定向次数 20 K/s 输入流量 100 KB/s 输出流量 10 MB/s 存储五年所需空间 15 TB 缓存内存 170 GB API 设计当我们确定需求之后，接下来应该开始设计系统 API。 创建短链接1String createUrl(String devKey,String originalUrl,String customAlias,String userName,LocalDateTime expireDate) 参数意义： devKey： API 调用方密钥，用来限制服务调用次数。 originalUrl：待缩短的原始 URL。 customAlias：URL 中的自定义字段（可选） userName：编码中使用的用户名（可选） expireDate：生成的短链接的到期实际（可选） 成功时返回生成的短链接，失败时返回对应的错误编码。 删除短链接1boolean deleteUrl(String devKey,String urlKey) 参数意义： devKey：API 调用方密钥 urlKey：待删除的短链接 URL 通过 boolean 值返回是否成功，失败时再补充失败消息。 恶意访问控制为了控制暴露的 API 接口被恶意访问，我们通过 devKey 限制用户调用接口数量。 每个 devKey 只能在一定时间段内创建一定数量的短链接并且会限制短链接的重定向次数。（可以会不同的 devKey 设置不同的限制时间） 数据库设计我们的系统存储的数据的性质包括： 每个月产生 5亿个新的 URL，假设在数据库中存储最近一年的数据，则存储的记录总数为 60亿。 存储的每个对象都很小（1个英文或数字占 1个字节，因此原始链接应该都小于 1K）。 各个记录之间没有关系。 记录读写比例很大。 设计表包括两个：短链接映射表以及用户表 表设计 由于存在数十亿的数据，并且记录之间没有依赖关系，所以应该使用 DynamoDB/Cassandra/Riak 这样的 NoSQL 更合适。 基础系统设计与算法对于短链接系统来说，一个重要的xx问题是对于给定的原始链接，如何生成一个唯一并且足够短的链接。 编码实际的 URL我们可以使用 MD5 或 SHA256 计算原始 URL 的唯一哈希，之后再对哈希散列值进行编码。我们使用 MD5 作为哈希函数。 编码方式可以是 base36([a-z,0-9]) 或 base62([A-Z,a-z,0-9])，如果需要使用 ‘+’ 与 ‘/‘，则需要使用 Base64 进行编码。我们选择使用 base64 编码。 对于长 6 个字母的键，会有 $64^6 ≈ 687$ 亿个不同字符串。 对于长 8 个字母的键，会有 $64^8 ≈ 281$ 万亿个不同字符串。 假设 6 个字母的键已满足我们的要求。 MD5 产生的哈希值有 128 位。每个 base64 字符会编码 6 位哈希值，所以使用 base64 编码 128 位哈希值，会获得一个超过 21 个字符的字符串。由于短链接只有 6 个字符，我们可以使用编码字符串的前 6 位作为密钥。这样会导致重复密钥，为了解决这个问题，我们可以选择编码字符串中的其它字符或者交换一些字符。 算法缺陷我们的方案存在以下几个问题： 不同用户输入相同 URL 时，他们会获得相同的短链接，这样是不能接受的。 未考虑 URL 中的一部分经过编码的问题。 解决方案当用户输入相同 URL（包括不同用户输入相同 URL，以及相同用户输入部分编码后的 URL）时，我们可以在每个原始 URL 后增加一个递增的序列号使其唯一，然后生成一个哈希值。在存储时不需要存储添加到尾部的序列号。 这个方案需要注意控制不断递增的序列号不溢出，另外增加序列号会影响服务的性能。 另一个解决方案是将用户 ID（唯一）附加到原始 URL 中。如果用户未登录，则需要为用户生成唯一密钥，如果生成密钥之后依然存在重复链接，需要继续生成密钥，直到获得唯一的短链接。 生成离线密钥我们可以创建一个独立的密钥生成服务（KGS），类似于发号器。 该服务可以提前生成随机的六位字符串并将其存在数据库中，每次当用户要生成短链接时，都将返回数据库中的一个字符串。这种方式不用对 URL 进行编码，并且不用担心重复。 KGS 可以使用两个表来存储密匙：一个表用于存储未使用的密钥，另一个存储已使用的密钥。当 KGS 将密钥提供给某个服务器之后，就可以将该密钥移动至已使用密钥表中。 KGS 可以始终将一些密钥保留在内存中，提高访问速度。对于加载到内存中的密钥，我们需要先将其移动到已使用密钥表中。如果 KGS 在将密钥标记为已使用与分配至服务器内存之间死亡，将会浪费一部分密钥。不过由于我们可以提前生成大量密钥，对于一部分丢失，是完全可以接受的。 关注问题 预先存储密钥的数据库大小 KGS 单点故障问题 当多个服务器同时读取密钥时，如何解决并发问题，以防止取到重复的密钥 解决方案 使用 base64 编码，可以生成 67.8 billion个唯一的六位字符串密钥。假设我们存储数字字母都只需要一个字节，存储这些键需要的容量为： $$6 * 68.7 billion = 412 GB$$ 可以为 KGS 准备一个备用副本。当主服务器死机时，备用服务器就可以接管服务。 由于 KGS 会预先分配一些密钥到内存中，所以单个服务器分配密钥时不会出现并发问题。关键在于 KGS 服务器向不同服务器分配密钥时，需要利用分布式锁控制分配密钥至不同服务器内存的操作。 数据分区为了扩展数据库，我们需要进行数据分区，以便存储数十亿 URL 信息。 基于范围的分区我们可以基于键的首字母分区。将字母 A 与 a 存在一个分区中，将 B 与 b 存在另一个分区中，依此类推。这种方法称为基于范围的分区。对于不经常出现的字母可以先组合到同一个数据库分区中。 这种方法的问题在于，它可能会导致数据库服务器不平衡，因为某个字母开头的链接会很多。 基于散列的分区我们可以先对要存储的对象进行哈希计算，之后再根据分区数将 URL 随机分配到不同的分区中，比如设计哈希函数将任意对象映射为 [1…256] 之间的整数，该数字就代表存储对象的分区编号。 这种方法依赖于哈希算法的随机性来保障分区数据平衡性，不过我们也可以使用一致性哈希来解决哈希分布不均的问题。 缓存我们可以缓存经常访问的 URL，使用 Redis 存储原始链接及其对应的短链接。在访问数据库之前，应用服务器可以先检查缓存中是否具有所需的 URL。 由计算得知，大概需要 170 GB 的内存来缓存 20% 的每日流量，这对于服务器来说不算问题。当数据占满缓存，并且我们要用较新的 URL 替换缓存时，我们需要使用缓存淘汰策略。对于我们的系统，使用 LRU 是合理的策略，这样我们会丢弃最近最少使用的 URL。 为了保证缓存服务器性能，我们可以使用 Redis Cluster 均衡负载。 负载均衡系统中由三个位置可以设置负载均衡管理： 客户端与应用服务器之间 应用服务器与数据库服务器之间 应用服务器与缓存服务器之间 前期可以采用简单的轮询策略，这个策略实现很简单。对于已经宕机的服务器，负载均衡也会将其从循环中移除，停止向其发送任何流量。 但是当某个服务器过载或者运行缓慢时，轮询策略仍然会向其发送新请求。所以后期我们可以使用一致性哈希或者最小连接数法作为负载均衡算法。 数据清理由于生成的短链接都是有一定有效期的，如果达到用户指定的到期时间时，链接该如何处理？ 如果我们主动定时搜索过期链接并将其删除，会对数据库造成很大的压力。所以我们可以采取缓慢搜索加上延迟清理策略。 每当用户尝试访问过期的链接时，都可以删除该过期链接并返回错误给用户 可以在服务器用户流量较低时，运行一个轻量级的清理服务，从缓存及存储中删除过期链接 删除过期链接后，可以将密钥放回密钥数据库以便重新使用","tags":[{"name":"System Design","slug":"System-Design","permalink":"http://changleamazing.com/tags/System-Design/"}]},{"title":"线程池深入剖析","date":"2020-05-24T08:23:57.000Z","path":"2020/05/24/线程池深入剖析/","text":"从底层深入剖析线程池。 线程池参数ThreadPoolExecutor 是线程池的核心类，要想深入理解线程池，必须先弄懂这个类。 构造函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125;public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 前面三个实际调用的都是第四个构造函数。我们来分析一下第四个构造函数的每个参数： corePoolSize 核心线程数，当线程池中的线程数达到这个值时，就会将新的任务放入到缓存队列中 maximumPoolSize 线程池最大线程数，表示线程池中最多能创建的线程个数 keepAliveTime 表示线程池中的非核心线程处于闲暇状态多长时间会被终止。默认情况下，当线程数量大于 corePoolSize 时，该值才会起作用。 如果调用 allowCoreThreadTimeOut(boolean) 方法，keepAliveTime 不会考虑当前线程池中线程数量 unit 参数 keepAliveTime 的时间单位 workQueue 阻塞队列，用来存储等待执行的任务。包括 ArrayBlockQueue、LinkedBlockingQueue、SynchronousQueue 和 PriorityBlockingQueue。 threadFactory 用于创建线程的线程工厂 handler 拒绝处理任务时的策略，总共有四种取值： AbortPolicy：拒绝提交的任务，并抛出 RejectedExecutionException 异常 CallerRunsPolicy：使用调用者所在线程执行任务 DiscardPolicy：丢弃任务 DiscardOldestPolicy：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务 重要字段123456789101112private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bits(高位存储状态)private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; ctl 是一个 32 位的数字，类型为 AtomicInteger。它包括两部分的信息：线程池的运行状态和活跃线程的数量。 根据定义的状态常量的值可以判断出 ctl 高 3 位保存运行运行状态。根据 CAPACITY 的值可以知道低 29 位用来保存线程池内活跃线程数，这个常量表示最大线程数量，值为 536870911，实际上开启线程数量不可能达到这么多。 线程池的运行状态包括以下五种： RUNNING：可以接受新提交的任务，并且也能处理阻塞队列中的任务 SHUTDOWN：不再接受新提交的任务，可以继续处理阻塞队列中保存的任务。调用 shutdown() 方法会使线程池进入此状态 STOP：不接受新任务也不处理队列中的任务，会中断正在处理任务的线程。调用 shutdownNow() 方法会使线程池进入该状态 TIDYING：当所有任务终止之后，workCount 为 0，线程池进入该状态后会调用 terminated() 方法进入 TERMINATED 状态 TERMINATED：在 terminated() 方法执行完成后进入该状态 ThreadPoolExecutor 还提供了几个与 ctl 相关的方法用来获取不同的数据： 1234567891011121314// 获取运行状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY;&#125;// 获取活动线程数量private static int workerCountOf(int c) &#123; return c &amp; CAPACITY;&#125;// 获取运行状态和活动线程数的值private static int ctlOf(int rs, int wc) &#123; return rs | wc;&#125; 核心方法executeexecute 方法用来向线程池中提交任务。ThreadPoolExecutor 类中还提供了 submit 用来提交任务，submit 其实也调用了 execute 方法，不过它会将任务包装成 FutureTask，这样可以在之后获取到该线程的执行结果。 1234567891011121314151617181920212223242526272829303132333435public void execute(Runnable command) &#123; if (command == null) &#123; throw new NullPointerException(); &#125; // 获取 ctl 值，它记录 runState 和 workerCount int c = ctl.get(); // workerCountOf(c) 会返回线程池当前活动线程数，如果当前活动线程数小于 corePoolSize，则直接创建新线程 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) &#123; return; &#125; // 如果创建新线程失败，说明有其它线程被创建了，此时 ctl 值肯定发生改变，所以重新获取 c = ctl.get(); &#125; // 如果当前线程池是运行状态并且任务添加到队列成功 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 重新获取 ctl int recheck = ctl.get(); // 如果线程池不处于运行状态则需要移除任务，移除成功之后需要使用拒绝策略处理任务。 if (!isRunning(recheck) &amp;&amp; remove(command)) &#123; reject(command); &#125; // 如果线程池中的有效线程数数量是 0，则创建一个活跃线程 else if (workerCountOf(recheck) == 0) &#123; // 第一个参数为 null 表示不指定任务 addWorker(null, false); &#125; &#125; // 如果线程池不是 running 状态或者加入阻塞队列失败，则创建新线程，扩容至 maxPoolSize。 else if (!addWorker(command, false)) &#123; // 创建新线程失败，则执行拒绝策略 reject(command); &#125;&#125; 线程池执行 execute 的逻辑包括下面几步： 判断当前运行的线程数是否少于 corePoolSize，如果是，则直接创建新线程来执行新任务 如果当前运行的线程数量大于或等于 corePoolSize，则将提交的任务存放至阻塞队列中 如果当前阻塞队列已满且线程个数没有达到最大线程数，则创建新的线程来执行任务 如果线程池中线程个数超过 maximumPoolsize，则使用拒绝策略 RejectedExecutionHandler 来进行处理 addWorkeraddWorker 主要工作是在线程池中创建一个新线程并执行。firstTask 参数用于指定新增线程执行的第一个任务，core 参数表示新增线程是否是核心线程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: // 循环判断状态 for (; ; ) &#123; int c = ctl.get(); int rs = runStateOf(c);//线程池运行状态 //1. 如果任务状态值大于等于 SHUTDOWN(SHUTDOWN/STOP/TIDYING/TERMINATED) if (rs &gt;= SHUTDOWN &amp;&amp; //2. 任务状态等于 SHUTDOWN 时，仍然在提交任务或者阻塞队列为空 // 上面两种情况同时满足时，直接返回 false，无法再新增加线程 !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) &#123; return false; &#125; // 内层循环，负责新建线程 for (; ; ) &#123; // 活跃线程数量 int wc = workerCountOf(c); // 判断线程数量是否超过限制。如果 core 为 true，则限制为 corePoolSize，否则限制为 maximumPoolSize if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) &#123; return false; &#125; // 使用 CAS 增加活跃线程数量，成功则跳出循环 if (compareAndIncrementWorkerCount(c)) &#123; break retry; &#125; // 重新读取 ctl c = ctl.get(); // 如果状态不等于之前获取的 state，跳出内层循环 if (runStateOf(c) != rs) &#123; continue retry; &#125; &#125; &#125; // workerCount 数量增加后续操作 boolean workerStarted = false;// 线程启动标识 boolean workerAdded = false;// 线程增加到 workerSet 集合标识 Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 获取运行状态 int rs = runStateOf(ctl.get()); // 如果线程池在运行或者线程池关闭并且不需要创建新任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; // 如果线程已经启动了，抛异常 if (t.isAlive()) &#123; throw new IllegalThreadStateException(); &#125; workers.add(w); // largestPoolSize 用来记录线程池容纳的最大线程数 int s = workers.size(); if (s &gt; largestPoolSize) &#123; largestPoolSize = s; &#125; // 设置线程为已添加至集合中 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // 集合中添加线程成功，则开启线程 if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (!workerStarted) &#123; addWorkerFailed(w); &#125; &#125; return workerStarted;&#125; 该方法的执行流程为： 判断线程池当前是否处于可以添加线程的状态，如满足以下三种情况的任意一种，则不允许创建线程，直接返回： rs &gt; shutdown。可能为 stop、tidying、terminated rs == shutdown，并且 firstTask != null rs == shutdown，firstTask == null 而 workQueue.isEmpty == true 判断线程池中线程数是否已经达到上限(core ? corePoolSize : maxiumPoolSize)，达到上限则返回 false 使用 ReentrantLock 向 workerSet 中添加新创建的 worker 线程，添加成功之后启动 worker 线程 参数配置我们需要根据任务的特性来配置线程池参数，才能更好的利用线程池。需要考虑以下几点： 任务性质：CPU 密集型任务或 IO 密集型任务或混合型任务 CPU 密集型任务是指 CPU 使用率比较高的一些任务，包括计算、内存分配等，这种情况下不应分配大量线程来占用 CPU 资源，所以线程池一般配置 ${N_{cpu}} + 1$ 个线程。对于 IO 密集型任务来说，长时间处于等于等待 IO 操作中，IO 操作只会占用很少的 CPU 资源，所以我们可以分配较多的线程来利用 CPU 处理其他任务，线程池一般配置$2 * {N_{cpu}}$ 个线程。对于混合型的任务，可以将其拆分成一个 CPU 密集型任务与一个 IO 密集型任务，如果这两个任务相差时间不大，我们可以将它们拆分成两个任务，拆分后的吞吐量会高于原来串行执行的吞吐率；如果两个任务时间相差较大，则没有必要分解 任务优先级：高或中或低 对于区分优先级的任务来说，我们可以利用优先队列 PriorityBlockingQueue 来处理，让优先级更高的任务先执行 任务执行时间：长或中或短 执行时间差异较大的任务可以交给不同配置的线程池处理，或者采用优先级队列，让执行时间短的任务先执行 任务依赖项：是否依赖其他系统资源 依赖数据库连接池的任务，同样也是需要等待数据库返回结果，这段时间内 CPU 空闲。如果等待时间越长那么设置的线程数就应该越大，让其它线程利用 CPU。 四种内置线程池在知道了线程池的所有参数之后，只要看内置线程池初始化时传的参数就知道这种线程池的运行过程了。 newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 这种线程池使用 LinkedBlockingQueue 作为阻塞队列，它默认容量为 Integer.MAX_VALUE ，近乎无界队列，因此线程池拒绝策略没有意义。 这种线程池的线程数可以设置为 ${N_{cpu}}$ + 1，可以充分利用 CPU 资源，而 IO 较多的任务由于执行时间。由于使用了无界队列，如果任务执行时间过长，而新增任务速率较快，会导致阻塞队列中任务积压过大，最后导致服务器 OOM。所以这种线程池只适用于任务生产与消费速率相对稳定的并发场景。 newCachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 核心线程数设置为 0，所以当有任务提交时就会进入阻塞队列中。keepAliveTime 设置为 60s，意味着当线程池中线程在 60s 内没有执行任务就会被销毁。阻塞队列使用 SynchronousQueue，该队列没有存储空间，当有新请求进来时，会去寻找空闲线程执行该任务，如果没有找到，则创建一个新线程来执行任务。这些特性使得该线程池在空闲时占用资源较少，因为会自动回收线程。它适合处理执行时间少且任务速度提交较慢的场景，否则会不断创建新线程执行任务，可能导致系统 CPU 和内存资源耗尽。 这种线程池不推荐在生产环境中使用，如果在 60s 内任务激增，可能会导致服务器宕机。 newSingleThreadExecutor123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 核心线程数与最大线程数都设置为 1，阻塞队列为 LinkedBlockingQueue ，所以性质与 newFixedThreadPool 差不多。不过该线程池会始终保持线程池中活跃线程数为 1。所以如果这个线程在执行任务时发生异常，也会创建新的线程来继续完成任务。所有的任务会按照顺序被执行。 这种线程池适合需要按顺序执行的非核心逻辑的任务，比如日志记录等功能。 newScheduledThreadPool1234public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue());&#125; 核心线程数为设置的值，最大线程数为 Integer.MAX_VALUE。阻塞队列为 DelayedWorkQueue，同样为无界队列。该队列中封装了一个优先级队列，会对队列中的任务进行排序，距离执行时间短的任务优先级高。 该线程池提供了两个方法： scheduledAtFixedRate：任务开始执行时开始计算时间 scheduledWithFixedDelay：任务执行完成后开始计算时间 该线程池用于需要多个后台线程执行周期任务，同时需要限制线程数量的场景。不过在生产中执行周期任务一般使用定时任务来处理。 线程池的工作队列ArrayBlockingQueueArrayBlockingQueue 是有界队列，容量是自定义的，使用数组实现，执行顺序为 FIFO。 LinkedBlockingQueueLinkedBlockingQueue 的基于链表实现，它的容量也是自定义的，默认为 Integer.MAX_VALUE，相当于一个无界队列。 DelayQueueDelayQueue 是一个任务按固定周期执行的优先级队列。根据任务将要被执行的时间远近排序，否则根据插入到队列的先后顺序排序。 PriorityBlockingQueuePriorityBlockingQueue 是具有优先级队列的无界阻塞队列。 SynchronousQueueSynchronousQueue 是不存储元素的队列，每个插入操作必须等待另一个线程调用取出操作，否则后续插入操作一直被阻塞。","tags":[{"name":"ThreadPool","slug":"ThreadPool","permalink":"http://changleamazing.com/tags/ThreadPool/"}]},{"title":"一致性协议","date":"2020-05-16T09:04:24.000Z","path":"2020/05/16/一致性协议/","text":"本文会分析经典的 2PC、3PC 以及 Paxos 算法来了解分布式系统的数据一致性协议。 在分布式系统中，通过网络请求其它服务的接口，总共会有三种情况：正确、失败和超时。其中正确和失败我们都能确定被调用服务的执行情况，但是在接口调用超时的情况下，调用方不能确定被调用方是否成功处理了请求，这一点造成了分布式系统中的许多问题。 通过对 CAP 定理的分析，可以知道分布式系统主要是对系统的可用性和数据一致性之间进行权衡，因此产生了一系列的一致性协议。我们来看看一些经典的一致性协议是如何解决上述的服务调用超时问题的。 2PC 与 3PC在一个分布式系统中，每个节点都只知道自己的执行情况，并不知道其他节点的执行情况，所以我们需要引入协调者（Coordinator）来统一调度所有节点的执行逻辑，这些被调度的分布式节点则被称为参与者（Participant）。 协调者负责调度参与者的行为，并最终决定这些参与者是否需要提交事务。基于这个思想，产生了二阶段提交（2PC）与三阶段提交（3PC）两种协议。 2PC分布式系统中的一致性协议都是基于两阶段提交进行的改进。 2PC 将执行过程分为了两个阶段，投票阶段与提交阶段。 投票阶段 如图所示，在投票阶段分为三个步骤： 协调者会询问所有的参与者是否可以执行提交 各参与者节点执行事务操作，并记录Undo（回滚）和 Redo（重试）日志 各参与者向协调者反馈执行操作的结果 提交阶段当所有的参与者都返回了确定的结果（同意或终止）时，2PC 进入提交阶段。 提交阶段分为四个步骤： 协调者根据投票阶段是否有参与者返回失败来决定发送 Commit 请求还是 Rollback 请求 参与者接收到协调者发送的请求后，如果是 Commit，会执行该节点的事务提交操作，并释放占用的资源；如果是 Rollback，会根据投票阶段该节点记录的 Undo 日志执行事务回滚操作，并释放占用的资源 参与在完成相应的事务操作之后，向协调者发送 ACK 消息 协调者接收到所有参与者反馈的 ACK 消息之后，完成操作 优缺点2PC 的优点在于原理简单且实现方便。 但是缺点也很明显，由于 2PC 是同步阻塞协议，如果在投票阶段，当参与者处于事务执行阶段时，协调者发生了永久宕机，此时参与者将永远无法完成事务，因为它们会一直等待 Commit/Rollback 请求。 另一点是数据不一致问题，如果在提交阶段，当协调者向所有参与者发送 Commit 请求时，如果此时部分参与者出现故障，或者协调者在向部分节点发送 Commit 请求后自身故障，会出现系统中部分参与者提交了事务，而其他参与者仍然没有进行事务提交，导致系统中出现了数据不一致的现象。 3PC 3PC 是在 2PC 的基础上改进的，它将 2PC 的投票阶段中的询问与执行事务拆分为两个阶段。形成了由 canCommit、preCommit 和 doCommit 三个阶段组成的协议。 3PC 主要对 2PC 进行了两处改动，首先是将 2PC 投票阶段拆分成两个阶段之后，降低了阻塞范围。3PC 最大的改动是引入了超时和节点宕机的处理机制。 节点在超时后会根据阶段来决定是否提交事务，比如在第三阶段中，某个参与者没有收到协调者的 commit 请求，它会自动提交事务。因为在第二阶段协调者既然能发出 commit 请求，就说明第一阶段中所有节点都认为自己能够执行事务，所以该参与者会认为在绝大概率情况下，其它参与者第二阶段也能正确执行事务，并向协调者发送 ACK 。 节点在宕机后会根据另一方接下来的操作来决定重启后的操作。依然用第三阶段举例，假设当所有参与者都向协调者发送了 ACK 之后，协调者将向参与者发送 commit 请求，而此时协调者直接宕机。由于存在超时处理机制，在第三阶段各个参与者一直收不到 commit 请求也会自己提交事务。所以协调者在宕机恢复之后默认所有的参与者已经自动提交事务，因此协调者会完成事务，正确结束。 对于参与者来说，则是判断下一步协调者会发出的指令，并作出对应的操作。 这篇来自弗吉尼亚理工大学的名为 Three-Phase Commit Protocol 的文章对 3PC 的各种情形都做了详细的描述，推荐一看。 缺点3PC 仍然存在数据不一致的问题，由于第三阶段中参与者的超时处理逻辑为自动提交，如果协调者真正向其他参与者发送的请求为 rollback，那么就会导致该自动提交的参与者与其他参与者数据不一致。 引用 Three-Phase Commit Protocol","tags":[{"name":"Distributed Theory","slug":"Distributed-Theory","permalink":"http://changleamazing.com/tags/Distributed-Theory/"}]},{"title":"CAP 定理与 BASE 理论","date":"2020-05-14T12:29:56.000Z","path":"2020/05/14/CAP 定理与 BASE 理论/","text":"CAP 定理CAP 定理原来是科学家埃里克·布鲁尔提出的一个猜想，之后被赛斯·吉尔伯特和南希·林奇证明，成为了一个定理。 CAP 定理指出对于一个分布式系统，不可能同时满足以下三点： 一致性（Consistence）：数据在多个节点之间能保持一致的特性， 可用性（Availability）：系统必须一直处于可用的状态，每次请求都能获取到非错的响应，但是不保证获取的数据为最新数据 分区容错性（Partition tolerance）：系统节点之间由于网络问题产生了分区时仍然能保持正常服务 证明下面证明一下为什么三个特性不能同时存在。 保证 AP：在分布式系统产生分区时，保持每个节点都对外提供服务，但是这种情况下节点内部通信时间延长或者失败，导致它们无法同步最新的数据，无法满足 C。 保证 CP：在分布式系统产生分区时，保证不同节点中的数据一致。如果节点对外继续提供服务，则可能产生新的数据，导致与其他节点数据不一致，为了保证一致性，需要对外提供服务的节点返回系统处理失败，无法满足 A。 对于分布式系统来说，保证 CA 的系统比较特殊，由上面两种情况可以知道，无法在存在分区的情况下还能保证系统的 CA 特性。那存不存在完全不会产生分区的分布式系统呢？假设一台机器每年宕机一天，那么一个包含 365 个节点的系统就可能每天都会产生宕机，除此之外还有运营商网络通信的异常，所以 P 是无法忽视的。 由于分区是否产生不受我们的控制，所以一般认为 P 是必须提供的，因此这个 CAP 定理可以转化为以下描述： 在系统满足分区容忍性的情况下，要么保证数据一致性，要么保证可用性。 如上图所示，P 存在的情况下，AC 只能二选一。 AP 与 CP 的应用AP 与 CP 的选择我们可以从 C 来考虑，CAP 中的 C 是数据的强一致性，一般来说，涉及到金融相关的系统是必须保证数据的强一致性的，否则会造成经济损失。 对于一般的系统，由于互联网应用主机众多，并且部署分散，集群规模越来越大，所以节点故障与网络故障是常态。为了保证服务可用性达到 N 个 9 的指标，则需要舍弃 C，保证 AP，通过后续的处理保证数据达到最终一致性即可。 BASE 理论BASE 表示 Basically Available（基本可用）、Soft state（软状态）和 Eventually Consistent（最终一致性）。 BASE 来源于对大规模互联网系统分布式实践的总结， 是对 CAP 中一致性与可用性权衡的结果。 BASE 理论核心思想是 即使无法做到强一致性，但每个应用都可以根据业务特点，采用适当的方式使系统达到最终一致性。 基本可用 基本可用是指分布式系统在出现不可预知的故障时，允许损失部分可用性。 例如： 响应时间损失：当系统部分机房出现故障时，查询接口的响应时间从原来的 0.5s 延长到 1~2s 功能损失：当电商系统抢购时，由于并发数量激增，为了保护系统的稳定性，部分消费者可能会被引导到一个降级页面 软状态 软状态是指允许系统中的数据存在中间状态，并且该中间状态不会影响系统整体可用性，即允许系统在不同节点的数据副本之间的同步存在延时。 最终一致性 最终一致性是指系统中所有的数据副本，在经过一段时间的同步后，最终能保证所有副本数据一致。在进行同步的这段时间内，系统中的数据就会存在中间状态，系统处于软状态。 关系型数据库大多都会采用同步和异步的方式来实现主备数据复制技术。对于异步方式来说，备库的更新往往会存在延时，这取决于事务日志在主备数据库之间传输的时间长短。如果传输时间过长或者传输过程中出现异常，都会导致这一段时间内主备数据不一致。我们可以采用多次重试或者人工操作来订正备库数据，最终主备库数据能够达成一致。需要注意的是，多次重试一般都是有次数限制的，所以完善的分布式系统一定要对系统异常进行监控，及时采用人工措施去处理，否则系统处于软状态的时间过长，就可能会造成严重的损失。","tags":[{"name":"Distributed Theory","slug":"Distributed-Theory","permalink":"http://changleamazing.com/tags/Distributed-Theory/"}]},{"title":"MySQL BTREE 索引分析","date":"2020-05-10T17:11:18.000Z","path":"2020/05/11/BTREE 索引/","text":"本文主要基于 InnoDB 的 BTREE 索引来分析索引。 索引是帮助 MySQL 高效获取数据的数据结构 前置知识磁盘 IO先看一下磁盘的结构： 一块磁盘有多个磁道组成，磁道上的每一个弧段即为一个扇区，扇区大小一般为 512B。 InnoDB 的数据存储在磁盘上，磁盘读取数据依靠的是机械运动。每次读取数据时间 = 寻道时间 + 旋转延迟 + 传输时间。 寻道时间指磁头移动到指定磁道所需要的时间，一般在 5ms 左右； 旋转延迟是指磁盘在磁道上找到指定扇区所需要花费的时间，假设磁盘转速为 7200 转/min，则每秒钟可以转 120 圈，磁头旋转到离当前位置最远的扇区（即磁盘对面）需要转半圈，所以旋转延迟最大为：1/2/120 = 4.17 ms； 传输时间是指与磁盘进行读写交互的时间，系统每一次读取磁盘是以页(逻辑概念，对应物理概念为盘块，即多个扇区,大小默认为 4KB)为单位,读取花费时间大概在 0.3 ms 左右。 页大小通过命令 getconf PAGE_SIZE 获取： 根据 局部性原理 来说，计算机访问一个地址的数据时，与其相邻的数据很快也会被访问到，所以操作系统针对此进行了优化，在传输数据时，以页为单位读取数据，这样很在访问同一页内的数据时，只会发生一次磁盘 IO，大大减少了读取数据总耗时。这个理论对于索引的数据结构设计非常有帮助。 索引数据结构 B+树 索引的结构如图所示，是一颗 b+ 树。图中每一个节点称为一个数据页（默认为 16K，这是 MySQL 磁盘管理的最小单位，与操作系统页大小不同），每个数据页中都包含若干个数据项，对于非叶子节点来说，数据项为指引搜索方向的指针；对于叶子节点来说，存储的是实际数据。 从根节点开始从 b+ 树中找到所需数据需要的 IO 次数不超过层数。而当数据量一定时，每个节点中存储的指针项越多，b+ 树的层数越少，这样查找数据的 IO 次数也会变少，性能大幅提升。 有了上述理论之后，这里引入一个 b+ 树与 b 树的区别：b+ 树中非叶子节点只存储指针，不存储实际数据；而 b 树的非叶子节点中既存储指针，又存储数据。 因为 b+ 树这样的设计，可以保证在同一个数据页中，可以存储更多的数据项，从而使得 b+ 树层数更少，因此 IO 次数更少，查找数据性能更强。 聚集索引与辅助索引存在一张表，建表语句如下： 123456789CREATE TABLE users( id INT NOT NULL, first_name VARCHAR(20) NOT NULL, last_name VARCHAR(20) NOT NULL, age INT NOT NULL, PRIMARY KEY(id), KEY(last_name, first_name, age) KEY(first_name)); 聚集索引是指根据表中主键顺序构建的一颗 b+ 树，并且在叶子节点中存放表的行记录。 表中没有指定主键时，根据顺序取唯一索引的第一列作为聚集索引的键；如果表中不存在唯一索引时，MySQL 会新建一个隐藏列，对不同的行设定序号来构建聚集索引。 这里只是为了表现聚集索引的特点，所以结构上忽略了一些特性，如连接叶子节点的链表等。 从图上可以知道，聚集索引的叶子节点保存了整行数据，而不是其中的一部分，所以聚集索引构成的 b+ 树也就是表数据存储的形式。所有正常的表有且仅有一个聚集索引。 辅助索引也是通过 b+ 树实现的，但是其叶子节点并不包括行记录全部数据，只会包括索引中的键以及聚集索引中的键。辅助索引用于加速数据查找，所以一张表上往往建立多个辅助索引来提升数据库的性能。但是辅助索引数量一般不超过 5 个，否则修改数据时，维护索引的时间会大大影响数据库操作的性能。 假设数据库存在辅助索引（first_name,age)，则该索引构成的 b+ 树会如下图所示。 辅助索引 可以看到，其叶子节点中存放的数据包括索引列 first_name,age 与该表的聚集索引键 id。当查找到 id 后，会再去聚集索引中根据 id 获取整行记录。 索引注意事项 遵循最左前缀匹配原则 MySQL 会一直向右匹配直到遇到范围查询（&gt;、&lt;、between、like）就停止匹配。比如存在（a,b,c,d）顺序的索引，查询条件为 a = 1 and b = 2 and c &gt; 3 and d = 4 ，d 无法使用索引。 选择区分度高的列作为索引列 区分度计算公式为 count(distinct col)/count(*)，表示字段不重复的比例。如果某索引列中重复数据很多，需要扫描很多次辅助索引表，再去主键索引中查找对应数据，很可能超过了全表扫描获取数据的时间。 索引不会包含含有 NULL 值的列 因为 NULL 无法参与计算，因此也无法构建 b+ 树 使用短索引 对一个字符串列创建索引时，很可能前几位就能区别出大多数数据，那就根据这几位来创建索引，而不是用整列创建索引。因为索引键也要存储在磁盘上，减少索引键长度可以节省磁盘并且减少 IO 操作 索引列不应该参与运算 因为索引键存的是列原始值，如果对列进行运算，得到的结果是无法在索引中直接找到的，需要对索引所有键都进行运算，而这个开销可能早已超过全表扫描了 参考链接 『浅入浅出』MySQL 和 InnoDB MySQL索引原理及慢查询优化","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://changleamazing.com/tags/MySQL/"},{"name":"DataBase","slug":"DataBase","permalink":"http://changleamazing.com/tags/DataBase/"}]},{"title":"AQS 源码分析","date":"2020-05-06T23:31:17.000Z","path":"2020/05/07/AQS 源码分析/","text":"AQS（AbstractQueuedSynchronizer）是一个用来构建锁和同步器的框架，它底层使用 CAS 技术来保证操作的原子性，同时利用 FIFO 队列实现线程间的锁竞争，它是 J.U.C 并发包同步的核心基础组件，是 ReentrantLock、CountDownLatch 等同步工具的底层实现机制。 AQS 结构先介绍 AQS 的重要属性。 123456789101112131415/** * 头结点，即当前持有锁的线程 */private transient volatile Node head;/** * 阻塞的尾结点，每次有新的节点，都会被插入到队列尾 */private transient volatile Node tail;/** * 代表当前锁的状态 * 0：没有被占用；&gt;0：表示有线程持有当前锁（&gt;1 表示有锁被某线程重入） */private volatile int state; 这几个属性都被 volatile 修饰，确保多线程间字段的可见性。 head 与 tail 容易理解，state 用来记录锁的同步状态，等于 0 表示无锁；大于 0 表示有锁，并且每次加锁 state 会增加 1，释放锁则会减 1。 AQS 提供了独占锁与共享锁，ReentrantLock 就是一种独占锁的实现，而 CountDownLatch 和 Semaphore 则是共享锁的实现。 接下来介绍 AQS 中阻塞队列的结构。 CLH 如图所示，这是一个双向队列，其中每个节点表示等待获取锁的线程,对应 AQS 中的内部类 Node，它们都有一个 waitStatus 属性，表示等待状态，共包括四种值： CANCELLED 1 取消状态，表示当前节点已经等待超时或者已经被中断了，需要被移除。 SIGNAL -1 等待触发状态，表示后继节点对应的线程需要被唤醒 CONDITION -2 等待条件状态，表示当前节点在 condition 队列中 PROPAGATE -3 状态向后传播，表示状态会被传播给后续节点，仅在共享锁模式下使用 独占锁获取锁如果某个线程获取到了独占锁，那么其它线程初次尝试获取锁时会失败，然后进入等待队列中。 1234567public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; // 放入阻塞队列 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) &#123; selfInterrupt(); &#125;&#125; 方法进来就会调用 tryAcquire 尝试获取锁，如果获取锁失败，则将当前线程封装成节点加入队列尾部。 跟踪将节点加入队尾的方法。 123456789101112131415161718192021private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); Node pred = tail; // 尝试直接设置为队列尾节点（因为大部分情况下尾节点不为空，CAS 设置也不会失败） if (pred != null) &#123; // pred（此处即 tail）!= null 说明队列不为空 // 维护 node 前置节点 node.prev = pred; // 使用 CAS 将节点设置为队列尾节点 if (compareAndSetTail(pred, node)) &#123; // 维护 pred 的后置节点 pred.next = node; return node; &#125; &#125; // 进入此处说明阻塞队列为空或者 CAS 失败（其它线程在竞争入队） enq(node); return node;&#125; addWaiter 方法在调用 enq 方法之前进行了一次尾节点不为空的判断，事实上没有这次判断 enq 方法也能保证该节点加入等待队列。但是由于在实际使用场景中，大多数情况下尾节点都不为空且使用 CAS 操作都不会失败，所以这里直接将新节点设置为尾节点的操作就会成功，也不用再进入 enq 方法使用自旋来添加尾节点。 我们再进入 enq 方法。 1234567891011121314151617181920private Node enq(final Node node) &#123; // 循环保证节点一定添加成功 for (; ; ) &#123; Node t = tail; if (t == null) &#123; // 尾节点为空说明队列为空，使用 CAS 设置 head if (compareAndSetHead(new Node())) &#123; // 暂时设置 tail = head，但是没有 return，继续循环，进入 else 分支 tail = head; &#125; &#125; else &#123; // 将当前线程对应的节点插入队列尾，自旋保证一定成功 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; enq 方法直接开始自旋，保证节点一定入队成功。 addWaiter 中尝试直接设置尾结点失败总共只有两种原因，enq 方法就对这两种原因进行处理。针对尾节点为空的情况，使用 CAS 设置头节点，之后会开启下次循环。针对尾节点不为空而 CAS 失败的情况，则继续尝试使用 CAS 来插入队尾。 执行完该方法，节点已经被添加到等待队列队尾了，接下来执行 acquireQueued 方法。 1234567891011121314151617181920212223242526272829final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (; ; ) &#123; final Node p = node.predecessor(); // 如果 node 前置节点为 head ，则尝试获取锁，因为 head 节点有可能是在 enq 方法中刚刚初始化， if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 获取锁成功需要将该节点设置为 head setHead(node); // 将原头节点与其后继节点断联 p.next = null; failed = false; return interrupted; &#125; // 如果 node 前置节点不是 head 或者获取锁失败，进入挂起逻辑 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; //阻塞线程并返回线程是否被中断 parkAndCheckInterrupt()) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; // 在 tryAcquire(arg) 抛异常时，failed = true if (failed) &#123; cancelAcquire(node); &#125; &#125;&#125; acquireQueued 方法主要包括两个步骤： 如果 node 前置节点为 head，则尝试获取锁 如果 node 前置节点不是 head 或者获取锁失败，则进入挂起逻辑 进入判断是否需要被挂起的方法。 123456789101112131415161718192021222324private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; // 如果前置节点状态 ws = -1，需要挂起当前线程，直接返回 true if (ws == Node.SIGNAL) &#123; return true; &#125; // ws &gt; 0，则需要删除前置节点 if (ws &gt; 0) &#123; /* 由于队列中的线程被挂起之后，是由前置节点唤醒的。所以需要将该节点的前置节点设置为一个正常状态的节点 从 pred 开始向前遍历，删除 waitStatus &gt; 0 的结点，直到找到某个线程未被取消的节点 */ do &#123; // 等价于 pred = pred.prev; node.prev = pred; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * 进入这里说明 ws 只可能是 0，-2，-3 * 每个 node 进入队列时，waitStatus 都是默认值 0，在这里将前驱节点的 waitStatus 设置为 -1。 */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; shouldParkAfterFailedAcquire 用来判断当前节点是否应当挂起。 主要包括以下几个操作： 判断前置节点 pred 状态。如果为 SIGNAL，则直接返回 true，表示应该挂起该节点 如果 pred 节点状态为 CANCELLED ，从前置节点 pred 开始向前删除状态为 CANCELLED 的节点 如果 pred 状态不是上述状态，则将它设置为 SIGNAL，之后会在 acquireQueued 中再循环一次 进入挂起方法。 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 通过 LockSupport 原语来阻塞线程。 释放锁123456789101112public final boolean release(int arg) &#123; // 尝试释放锁定同步状态 if (tryRelease(arg)) &#123; Node h = head; // head 不为空且 waitStaus != 0 则唤醒后继节点（waitStatus == 0 表示没有后继节点） if (h != null &amp;&amp; h.waitStatus != 0) &#123; unparkSuccessor(h); &#125; return true; &#125; return false;&#125; 进入释放后继节点的方法 123456789101112131415161718192021222324252627282930private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) &#123; // 如果 head 节点 waitStatus &lt; 0 ，使用 CAS 修改为 0 compareAndSetWaitStatus(node, ws, 0); &#125; // 获取直接后继节点 Node s = node.next; /* * 如果后继节点为空或者后继节点取消了等待（waitStatus == 1），则从尾部开始向前遍历，直到找到距离 node 节点最近的 ws&lt;=0 的节点 */ if (s == null || s.waitStatus &gt; 0) &#123; s = null; /** * 从后往前遍历可以确保访问到新增加的结点。 * &#123;@link #addWaiter(Node)&#125; 方法中，新节点插入的时候，首先就维护了前置结点，再设置到队列中， * 而设置 pred.next = node 并不是原子操作，所以从后遍历，能够保证访问到刚入队的结点。 */ for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) &#123; if (t.waitStatus &lt;= 0) &#123; s = t; &#125; &#125; &#125; if (s != null) &#123; //唤醒线程 LockSupport.unpark(s.thread); &#125;&#125; 如果 head.waitStatus &lt; 0，会使用 CAS 将它设置为初始状态。之后唤醒后继节点，如果直接后继节点为空，则尝试从队尾开始向前寻找第一个符合条件的节点。 共享锁获取锁1234567public final void acquireShared(int arg) &#123; // 尝试获取共享锁 if (tryAcquireShared(arg) &lt; 0) &#123; // 执行获取锁失败的逻辑 doAcquireShared(arg); &#125;&#125; 尝试获取锁，失败则进入获取锁失败的逻辑中。 12345678910111213141516171819202122232425262728293031323334private void doAcquireShared(int arg) &#123; // 构建共享锁节点，准备添加到队列中 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (; ; ) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 与独占锁类似，前置结点为头节点，直接尝试获取锁 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 共享锁唤醒逻辑 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) &#123; selfInterrupt(); &#125; failed = false; return; &#125; &#125; // 挂起逻辑，与独占锁相同 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) &#123; cancelAcquire(node); &#125; &#125;&#125; 整个流程与独占锁构建节点并入队的逻辑一致，只是在此处，获取锁成功之后，会调用 setHeadAndPropagate 方法尝试唤醒后继节点，实现共享。 进入 setHeadAndPropagate 方法。 123456789101112private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node);// 设置头节点 // 根据状态判断是否唤醒后继线程 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) &#123; doReleaseShared(); &#125; &#125;&#125; 该方法主要是设置头节点，并唤醒头节点的后继线程。此处最重要的是进入唤醒后继线程的条件，后续会分析。 进入唤醒后继线程的方法。 1234567891011121314151617181920212223242526private void doReleaseShared() &#123; for (; ; ) &#123; Node h = head; // 如果队列中存在后继节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; // 更新状态为 0 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) &#123; continue; &#125; // 唤醒直接后继节点 unparkSuccessor(h); &#125; // 如果 head 状态为 0，使用 CAS 更新为 PROPAGATE 保证唤醒后继节点 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) &#123; continue; // loop on failed CAS &#125; &#125; // 循环过程中 head 节点是否变化，如果不是，需要重新开始循环 if (h == head) &#123; break; &#125; &#125;&#125; 释放锁1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; 这里释放成功会调用 doReleaseShared 释放后继节点 细节节点入队顺序问题在 enq 方法中，存在以下代码片段 12345node.prev = t;if (compareAndSetTail(t, node)) &#123; t.next = node; return t;&#125; 这里是先将元素入队，再通过 CAS 设置尾结点。这样能保证队列数据统一。如果先通过 CAS 设置尾节点，再将其与前面的元素绑定，则在绑定前的一瞬间内，CAS 队列中存在尾节点，其前置节点为空的情况，这与实际数据不符。 节点遍历问题唤醒节点的方法 unparkSuccessor 如下： 123456789101112131415161718192021222324252627282930private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) &#123; // 如果 head 节点 waitStatus &lt; 0 ，使用 CAS 修改为 0 compareAndSetWaitStatus(node, ws, 0); &#125; // 获取直接后继节点 Node s = node.next; /* * 如果后继节点为空或者后继节点取消了等待（waitStatus == 1），则从尾部开始向前遍历，直到找到距离 node 节点最近的 ws&lt;=0 的节点 */ if (s == null || s.waitStatus &gt; 0) &#123; s = null; /** * 从后往前遍历可以确保访问到新增加的结点。 * &#123;@link #addWaiter(Node)&#125; 方法中，新节点插入的时候，首先就维护了前置结点，再设置到队列中， * 而设置 pred.next = node 并不是原子操作，所以从后遍历，能够保证访问到刚入队的结点。 */ for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) &#123; if (t.waitStatus &lt;= 0) &#123; s = t; &#125; &#125; &#125; if (s != null) &#123; //唤醒线程 LockSupport.unpark(s.thread); &#125;&#125; 从后向前的遍历顺序其实也要关注插入节点的顺序： 设置新节点的前置节点为原尾节点 CAS 设置新节点为尾节点 设置原尾节点的后继节点为新节点 考虑以下情况： node 在此刻为 tail，因此存在 s == null 有新节点通过 enq 方法入队 在入队时只执行了插入节点三个步骤中的前两步，还没有执行第三步 此刻如果从 node 开始向后遍历，会得到 node 的后继节点为空，所以直接返回 由于节点入队时优先设置的前继节点，所以从 tail 开始向前遍历，可以保证在 CAS 设置 tail 元素成功之后即可遍历到最新入队的节点，而不需要三步都执行完，这样尽可能的保证了队列元素的完整性。 总结AQS 的设计十分精妙，这样简单的分析无法体会到精髓，也很难深入理解，后续会继续剖析基于 AQS 实现的 JUC 同步工具类，弄懂如何基于 AQS 简单的实现同步。 参考资料 AQS源码解读","tags":[{"name":"JDK","slug":"JDK","permalink":"http://changleamazing.com/tags/JDK/"},{"name":"SourceCode","slug":"SourceCode","permalink":"http://changleamazing.com/tags/SourceCode/"}]},{"title":"Spring IoC 容器源码分析","date":"2020-04-22T20:04:29.000Z","path":"2020/04/23/SpringIoC 源码解析-上/","text":"Ioc 是 Spring 中重要的概念，本文以 ClassPathXmlApplicationContext 为例，结合源代码深入分析 IoC 容器的实现。 实例新建一个 maven 项目来启动 Spring 容器，只需要在 pom.xml 中引入 spring-context 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.16.RELEASE&lt;/version&gt;&lt;/dependency&gt; 构建 ApplicationContext 有多种方式，首先看一下 ApplicationContext 的类结构。 我们可以选用三种方式来启动 Spring 容器： ClassPathXmlApplicationContext：在 ClassPath 中加载 xml 配置文件。 FileSystemXmlApplicationContext：通过系统路径加载 xml 配置文件。 AnnotationConfigApplicationContext：基于注解使用 本文选择使用 ClassPathXmlApplicationContext 来启动 Spring 容器。 首先定义一个接口以及对应的实现类： 12345678910public interface MessageService &#123; String getMessage();&#125;public class MessageServiceImpl implements MessageService &#123; @Override public String getMessage() &#123; return \"hello world\"; &#125;&#125; 接下来在 resource 目录下新建 xml 配置文件，文件名为：application-context.xml： 1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"messageService\" class=\"com.zcl.service.impl.MessageServiceImpl\"/&gt;&lt;/beans&gt; 接下来就可以使用 Demo 从 xml 文件中获取 ApplicationContext： 123456789public class App &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"classpath:applicationfile.xml\"); MessageService messageService = context.getBean(MessageService.class); System.out.println(messageService.getMessage()); &#125;&#125; 源码分析12345678910111213141516171819202122232425public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext &#123; @Nullable private Resource[] configResources; /** * Demo 中调用的构造函数 */ public ClassPathXmlApplicationContext(String configLocation) throws BeansException &#123; this(new String[] &#123;configLocation&#125;, true, null); &#125; public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; super(parent); //根据提供的路径，处理成配置文件数组 setConfigLocations(configLocations); if (refresh) &#123; // IoC 核心方法 refresh(); &#125; &#125;&#125; ClassPathXmlApplicationContext 类代码比较简单,只包括两个方法 setConfigLocations 与 refresh，这两个方法在该类中都没有实现。 setConfigLocations1234567891011121314151617public void setConfigLocations(@Nullable String... locations) &#123; if (locations != null) &#123; Assert.noNullElements(locations, \"Config locations must not be null\"); this.configLocations = new String[locations.length]; // 根据传入的路径解析成配置文件数组 for (int i = 0; i &lt; locations.length; i++) &#123; this.configLocations[i] = resolvePath(locations[i]).trim(); &#125; &#125; else &#123; this.configLocations = null; &#125;&#125;protected String resolvePath(String path) &#123; return getEnvironment().resolveRequiredPlaceholders(path);&#125; getEnvironment123456public ConfigurableEnvironment getEnvironment() &#123; if (this.environment == null) &#123; this.environment = createEnvironment(); &#125; return this.environment;&#125; 获取环境变量。如果为空则调用方法新建。 createEnvironment123protected ConfigurableEnvironment createEnvironment() &#123; return new StandardEnvironment();&#125; 直接返回了一个 StandardEnvironment 类， 123456789101112131415161718192021222324252627public class StandardEnvironment extends AbstractEnvironment &#123; /** * 系统环境变量名称 */ public static final String SYSTEM_ENVIRONMENT_PROPERTY_SOURCE_NAME = \"systemEnvironment\"; /** * JVM 变量 */ public static final String SYSTEM_PROPERTIES_PROPERTY_SOURCE_NAME = \"systemProperties\"; /** * 根据系统环境变量与 JVM 变量设置资源列表 */ @Override protected void customizePropertySources(MutablePropertySources propertySources) &#123; propertySources.addLast( new PropertiesPropertySource(SYSTEM_PROPERTIES_PROPERTY_SOURCE_NAME, getSystemProperties())); propertySources.addLast( new SystemEnvironmentPropertySource(SYSTEM_ENVIRONMENT_PROPERTY_SOURCE_NAME, getSystemEnvironment())); &#125;&#125; customizePropertySources 方法会往资源列表中添加 Java 进程中的变量和系统的环境变量。 resolveRequiredPlaceholders1234567891011121314151617181920212223242526/** * org.springframework.core.env.AbstractPropertyResolver#resolveRequiredPlaceholders */public String resolveRequiredPlaceholders(String text) throws IllegalArgumentException &#123; if (this.strictHelper == null) &#123; this.strictHelper = createPlaceholderHelper(false); &#125; return doResolvePlaceholders(text, this.strictHelper);&#125;/** * org.springframework.core.env.AbstractPropertyResolver#doResolvePlaceholders * 方法引用，调用 placeholderResolver.resolvePlaceholder(String) 时， * 会调用 PropertySourcesPropertyResolver#getPropertyAsRawString(String) */private String doResolvePlaceholders(String text, PropertyPlaceholderHelper helper) &#123; return helper.replacePlaceholders(text, this::getPropertyAsRawString);&#125;/** * org.springframework.util.PropertyPlaceholderHelper#replacePlaceholders */public String replacePlaceholders(String value, PlaceholderResolver placeholderResolver) &#123; Assert.notNull(value, \"'value' must not be null\"); return parseStringValue(value, placeholderResolver, new HashSet&lt;&gt;());&#125; resolveRequiredPlaceholders 方法处理流程比较长，最后进入到 PropertyPlaceholderHelper 类 parseStringValue 方法来替换占位符。 这里我们传入的 value 是路径，并没有占位符，所以不会进入到替换占位符的过程中，所以这里暂时不分析 parseStringValue方法。 到这里 setConfigLocations 方法流程就结束了，最后将路径放入到存放配置文件路径的字符串数组 configLocations 中。 refreshrefresh 方法是容器初始化的过程，涵盖流程非常多。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; // 保证该方法不会被同时访问，否则会出现在启动时同时销毁了原容器的现象 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录容器的启动时间，标记”已启动“ 状态、处理配置文件中的占位符 prepareRefresh(); // 将配置文件解析为 BeanDefinition 注册到 BeanFactory 中，但是没有初始化 Bean ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加 BeanPostProcessor，注册特殊 Bean prepareBeanFactory(beanFactory); try &#123; // 为容器的某些子类指定特殊的 BeanPost 事件处理器 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 实现类 registerBeanPostProcessors(beanFactory); // 初始化 MessageSource，与国际化相关 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 模板方法，初始化特殊 Bean onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有非懒加载的 singleton bean finishBeanFactoryInitialization(beanFactory); // 初始化容器的生命周期事件处理器，并发布容器的生命周期事件 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; // 销毁已创建的单例 Bean destroyBeans(); // 取消 refresh 操作，重置容器的同步标识 cancelRefresh(ex); throw ex; &#125; finally &#123; resetCommonCaches(); &#125; &#125;&#125; 该方法用 synchronized 关键字将方法体包裹，避免在refresh 方法未执行完毕时，被其他线程发起启动或销毁容器的操作所干扰。 prepareRefresh容器启动准备工作，包括记录容器启动的时间、设置容器状态为已启动、检查环境变量等 1234567891011121314151617181920212223242526protected void prepareRefresh() &#123; // 记录启动时间，设置容器状态为已激活 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isInfoEnabled()) &#123; logger.info(\"Refreshing \" + this); &#125; // 初始化占位符属性源。该类中没有实现该方法，可以根据需要自行扩展实现 initPropertySources(); // 校验必要属性是否为空，主要是环境变量中配置的值（包括系统环境变量与进程环境变量） getEnvironment().validateRequiredProperties(); // 存储 refresh 前的应用监听器 if (this.earlyApplicationListeners == null) &#123; this.earlyApplicationListeners = new LinkedHashSet&lt;&gt;(this.applicationListeners); &#125; else &#123; this.applicationListeners.clear(); this.applicationListeners.addAll(this.earlyApplicationListeners); &#125; this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; validateRequiredProperties1234567891011121314public void validateRequiredProperties() throws MissingRequiredPropertiesException &#123; this.propertyResolver.validateRequiredProperties();&#125;public void validateRequiredProperties() &#123; MissingRequiredPropertiesException ex = new MissingRequiredPropertiesException(); for (String key : this.requiredProperties) &#123; if (this.getProperty(key) == null) &#123; ex.addMissingRequiredProperty(key); &#125; &#125; if (!ex.getMissingRequiredProperties().isEmpty()) &#123; throw ex; &#125; 校验设置为必要属性的字段对应的值是否为空，为空直接抛出异常。 如果项目存在启动必须依赖的变量，我们可以在环境变量中添加 key 与 value，并且调用 org.springframework.core.env.ConfigurablePropertyResolver#setRequiredProperties 方法设置变量对应的值必须不为空。 我们可以自定义一个 AbstractApplicationContext 的子类，并重写上面提到的 initPropertySources 方法，将字段加入校验集合中。 obtainFreshBeanFactory该方法负责 BeanFactory 的初始化、BeanDefinition 的加载和注册等操作。 12345678910protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 关闭旧的 BeanFactory，创建新的 BeanFactory，加载并注册 BeanDefinition。抽象方法，交由子类实现 refreshBeanFactory(); // 获取上一步中创建的 BeanFactory。抽象方法，交由子类实现 ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(\"Bean factory for \" + getDisplayName() + \": \" + beanFactory); &#125; return beanFactory;&#125; obtainFreshBeanFactory 方法中调用的两个方法都是抽象方法，交由子类实现，这是典型的模板方法模式。 refreshBeanFactory1234567891011121314151617181920212223protected final void refreshBeanFactory() throws BeansException &#123; // 如果当前 ApplicationContext 中已经加载过 BeanFactory，则销毁所有 Bean 并且关闭 BeanFactory if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; // 创建 DefaultListableBeanFactory DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); // 设置 BeanFactory 的两个配置属性：是否允许 Bean 覆盖，是否允许循环引用 customizeBeanFactory(beanFactory); // 抽象方法，加载 BeanDefinition 到 BeanFactory 中 loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException( \"I/O error parsing bean definition source for \" + getDisplayName(), ex); &#125;&#125; 这个方法创建了一个 DefaultListableBeanFactory，其类结构图如下图所示： DefaultListableBeanFactory 可以看到这个类继承了所有关于容器的接口与抽象类。 customizeBeanFactory12345678910protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; // 是否允许 BeanDefinition 覆盖 if (this.allowBeanDefinitionOverriding != null) &#123; beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; // 是否允许 Bean 循环依赖 if (this.allowCircularReferences != null) &#123; beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125;&#125; 该方法用于设置刚刚创建的 BeanFactory 的两个属性：是否允许 BeanDefinition 覆盖，是否允许 Bean 循环引用。 再进行下一步之前，需要先观看关于BeanDefinition 的简介，了解 BeanFactory 中存放的究竟是什么，才能更好的理解接下来的内容。 loadBeanDefinitions该方法也是十分重要的方法，它通过 XML 加载 BeanDefinition，存放至刚刚创建的 BeanFactory 中。 1234567891011121314151617181920212223242526272829protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 为 BeanFactory 实例化一个 XmlBeanDefinitionReader XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); beanDefinitionReader.setEnvironment(this.getEnvironment()); // 注意此处，XmlBeanDefinitionReader 将当前类设置为了 ResourceLoader beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化 BeanDefinitionReader，默认实现里面仅开启了 XML 校验 initBeanDefinitionReader(beanDefinitionReader); // 使用 beanDefinitionReader 加载 BeanDefinition loadBeanDefinitions(beanDefinitionReader);&#125;protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; // 从 resource 中加载 beanDefinition reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; // 根据配置文件路径加载 BeanDefinition（也会先根据路径获取对应的 Resource） reader.loadBeanDefinitions(configLocations); &#125;&#125; 构造了一个 BeanDefinitionReader 用于读取配置文件中的 BeanDefinition，注意此处，XmlBeanDefinitionReader 将当前类设置为了 ResourceLoader，用于将配置文件转换为 Spring 中的 Resource 对象。 第二个方法中，首先尝试从 configResource 中加载 BeanDefinition，这个值可以在构造 ClassPathXmlApplicationContext 时传入，不过一般不会使用，且 AbstractXmlApplicationContext#getConfigResources 默认返回空值。 之后就是从我们传入的配置文件路径 classpath:applicationfile.xml 加载 BeanDefinition。 下面贴出根据配置文件路径加载 BeanDefinition 的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; Assert.notNull(locations, \"Location array must not be null\"); int counter = 0; for (String location : locations) &#123; counter += loadBeanDefinitions(location); &#125; // 返回加载的 BeanDefinition 的数量 return counter;&#125;public int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(location, null);&#125;public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; // 在实例化 XmlBeanDefinitionReader 后，IoC 容器将自己注入该读取器作为 resourceLoader ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( \"Cannot import bean definitions from location [\" + location + \"]: no ResourceLoader available\"); &#125; if (resourceLoader instanceof ResourcePatternResolver) &#123; try &#123; // 将配置文件转换为 Resource 对象 Resource[] resources = ((ResourcePatternResolver) resourceLoader) .getResources(location); // 委派调用其子类 XmlBeanDefinitionReader 的方法，实现加载功能 int loadCount = loadBeanDefinitions(resources); if (actualResources != null) &#123; for (Resource resource : resources) &#123; actualResources.add(resource); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(\"Loaded \" + loadCount + \" bean definitions from location pattern [\" + location + \"]\"); &#125; return loadCount; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex); &#125; &#125; else &#123; // 将指定位置的资源文件解析为 Resource，至此完成了对 BeanDefinition 的资源定位 Resource resource = resourceLoader.getResource(location); // 从 resource 中加载 BeanDefinition，loadCount 表示加载的 BeanDefinition 的个数 int loadCount = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(\"Loaded \" + loadCount + \" bean definitions from location [\" + location + \"]\"); &#125; return loadCount; &#125;&#125;@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, \"Resource array must not be null\"); // 加载的 BeanDefinition 总数 int counter = 0; for (Resource resource : resources) &#123; counter += loadBeanDefinitions(resource); &#125; return counter;&#125; 下面由子类 XmlBeanDefinitionReader 对资源文件进行加载： 12345678910111213141516171819202122232425262728293031323334353637383940414243public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(new EncodedResource(resource));&#125;public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; Assert.notNull(encodedResource, \"EncodedResource must not be null\"); if (logger.isInfoEnabled()) &#123; logger.info(\"Loading XML bean definitions from \" + encodedResource); &#125; Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); &#125; try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; // 真正的加载 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( \"IOException parsing XML document from \" + encodedResource.getResource(), ex); &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125; 注意，到此时，还没有实际加载配置文件，而该方法中调用了 doLoadBeanDefinitions ，Spring 中方法命名用 do 开头的才是实际进行操作的方法。 doLoadBeanDefinitions123456789101112131415161718192021222324252627protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; // 将 XML 文件转化为 Document 对象 Document doc = doLoadDocument(inputSource, resource); // 根据 Document 对象注册 BeanDefinition return registerBeanDefinitions(doc, resource); &#125; catch (BeanDefinitionStoreException ex) &#123; throw ex; &#125; catch (SAXParseException ex) &#123; throw new XmlBeanDefinitionStoreException(resource.getDescription(), \"Line \" + ex.getLineNumber() + \" in XML document from \" + resource + \" is invalid\", ex); &#125; catch (SAXException ex) &#123; throw new XmlBeanDefinitionStoreException(resource.getDescription(), \"XML document from \" + resource + \" is invalid\", ex); &#125; catch (ParserConfigurationException ex) &#123; throw new BeanDefinitionStoreException(resource.getDescription(), \"Parser configuration exception parsing XML from \" + resource, ex); &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException(resource.getDescription(), \"IOException parsing XML document from \" + resource, ex); &#125; catch (Throwable ex) &#123; throw new BeanDefinitionStoreException(resource.getDescription(), \"Unexpected exception parsing XML document from \" + resource, ex); &#125;&#125; 该方法分为两步：将 XML 文件转化为 Document 对象，根据 Document 对象注册 BeanDefinition。文件转换为 Document 对象不是重点，这里略过。 registerBeanDefinitions12345678910111213141516171819202122232425public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); // 获取已注册的 BeanDefinition 数量 int countBefore = getRegistry().getBeanDefinitionCount(); // 加载 Document 对象中的 BeanDefinition documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); // 获取本次注册的 BeanDefinition 数量 return getRegistry().getBeanDefinitionCount() - countBefore;&#125;public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug(\"Loading bean definitions\"); Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root);&#125;public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug(\"Loading bean definitions\"); // 获取根节点，从根节点开始加载 Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root);&#125; doRegisterBeanDefinitions12345678910111213141516171819202122232425262728293031protected void doRegisterBeanDefinitions(Element root) &#123; // 根节点 BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); // 是否是默认 namespace（http://www.springframework.org/schema/beans） if (this.delegate.isDefaultNamespace(root)) &#123; String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); // 解析配置文件中的 profile 属性，&lt;beans ... profile=\"***\" /&gt; if (StringUtils.hasText(profileSpec)) &#123; String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123; if (logger.isInfoEnabled()) &#123; logger.info(\"Skipped XML bean definition file due to specified profiles [\" + profileSpec + \"] not matching: \" + getReaderContext().getResource()); &#125; return; &#125; &#125; &#125; // 前置扩展 preProcessXml(root); // 解析 XML，转换 BeanDefinition parseBeanDefinitions(root, this.delegate); // 后置扩展 postProcessXml(root); this.delegate = parent;&#125; preProcessXml 与 postProcessXml 方法是 DefaultBeanDefinitionDocumentReader 留给我们自定义实现的在注册 BeanDefinition 前后的处理逻辑。 parseBeanDefinitions123456789101112131415161718192021protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; // default namespace 包括四个标签：&lt;import/&gt;、&lt;alias/&gt;、&lt;beans/&gt; 和 &lt;bean/&gt; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; // 解析 default namespace 下面的元素 parseDefaultElement(ele, delegate); &#125; else &#123; // 解析其它 namespace 元素 delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 一般情况下我们项目配置文件都是配置的 default namespace，所以进入 parseDefaultElement 。 parseDefaultElement123456789101112131415161718private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; // import 标签处理 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; // alias 标签处理 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; // bean 标签处理 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; // beans 标签递归处理 else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; doRegisterBeanDefinitions(ele); &#125;&#125; 我们只分析注册 BeanDefinition 的过程，所以进入 bean 标签处理方法 processBeanDefinition。 processBeanDefinition12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 提取 bean 节点中的信息，封装到 BeanDefinitionHolder 中 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; // 解析 &lt;bean/&gt; 节点自定义属性与自定义子节点，设置 BeanDefinition bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; BeanDefinitionReaderUtils .registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); &#125; getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 第一步从 bean 节点中提取 BeanDefinitionHolder，BeanDefinitionHolder 包括 BeanDefinition，beanName 和 aliases，其中 beanName 多数情况下为 id，如果没有指定 id 则为别名集合中的第一个别名。 parseBeanDefinitionElement1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null);&#125;public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; // 如果没有指定 id，则设置为别名列表中的第一个别名 beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug(\"No XML 'id' specified - using '\" + beanName + \"' as bean name and \" + aliases + \" as aliases\"); &#125; &#125; if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; // 根据 &lt;bean/&gt; 节点创建 BeanDefinition，然后将配置信息设置到实例中。经过该行，BeanDefinition 就创建完成 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 下面进入单独创建 BeanDefinition 的方法 parseBeanDefinitionElement。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, @Nullable BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; try &#123; // 创建 BeanDefinition，然后设置类信息 AbstractBeanDefinition bd = createBeanDefinition(className, parent); // 设置 BeanDefinition 的属性 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); // 解析 &lt;bean/&gt; 节点中的子节点 parseMetaElements(ele, bd); parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); parseConstructorArgElements(ele, bd); parsePropertyElements(ele, bd); parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; error(\"Bean class [\" + className + \"] not found\", ele, ex); &#125; catch (NoClassDefFoundError err) &#123; error(\"Class that bean class [\" + className + \"] depends on not found\", ele, err); &#125; catch (Throwable ex) &#123; error(\"Unexpected failure during bean definition parsing\", ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; return null;&#125; 进入创建 BeanDefinition 的方法 createBeanDefinition。 createBeanDefinition123456789101112131415161718192021protected AbstractBeanDefinition createBeanDefinition(@Nullable String className, @Nullable String parentName) throws ClassNotFoundException &#123; return BeanDefinitionReaderUtils.createBeanDefinition( parentName, className, this.readerContext.getBeanClassLoader());&#125;public static AbstractBeanDefinition createBeanDefinition( @Nullable String parentName, @Nullable String className, @Nullable ClassLoader classLoader) throws ClassNotFoundException &#123; GenericBeanDefinition bd = new GenericBeanDefinition(); bd.setParentName(parentName); if (className != null) &#123; if (classLoader != null) &#123; bd.setBeanClass(ClassUtils.forName(className, classLoader)); &#125; else &#123; bd.setBeanClassName(className); &#125; &#125; return bd; 第二个方法就是实际创建 BeanDefinition 的方法，这里设置了 BeanDefinition 的一些基本信息，返回给上面的方法，之后根据配置对 BeanDefinition 属性进行补充。 到这里为止，终于根据配置文件 节点获取到了 BeanDefinition。 下面分析注册 BeanDefinition 的过程。 回到创建完 BeanDefinition 之后的处理逻辑中： 12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 提取 bean 节点中的信息，封装到 BeanDefinitionHolder 中 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; // 解析 &lt;bean/&gt; 节点自定义属性与自定义子节点，设置 BeanDefinition bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; BeanDefinitionReaderUtils .registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); &#125; getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 进入 registerBeanDefinition： 12345678910111213141516public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; String beanName = definitionHolder.getBeanName(); // 根据 beanName 注册 BeanDefinition registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // 根据 aliases 注册 BeanDefinition String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; registry.registerAlias(beanName, alias); &#125; &#125;&#125; 我们最开始创建的 BeanFactory 类型是 DefaultListableBeanFactory，所以进入DefaultListableBeanFactory#registerBeanDefinition： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; Assert.hasText(beanName, \"Bean name must not be empty\"); Assert.notNull(beanDefinition, \"BeanDefinition must not be null\"); if (beanDefinition instanceof AbstractBeanDefinition) &#123; try &#123; ((AbstractBeanDefinition) beanDefinition).validate(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Validation of bean definition failed\", ex); &#125; &#125; // 根据 beanName 查看之前是否注册过相同 beanName 的 BeanDefinition BeanDefinition existingDefinition = this.beanDefinitionMap.get(beanName); if (existingDefinition != null) &#123; // 是否允许覆盖，在创建 DefaultListableBeanFactory 时设置，默认允许覆盖 if (!isAllowBeanDefinitionOverriding()) &#123; throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Cannot register bean definition [\" + beanDefinition + \"] for bean '\" + beanName + \"': There is already [\" + existingDefinition + \"] bound.\"); &#125; else if (existingDefinition.getRole() &lt; beanDefinition.getRole()) &#123; // 框架定义的 BeanDefinition 可以直接覆盖用户自定义的 BeanDefinition if (logger.isWarnEnabled()) &#123; logger.warn(\"Overriding user-defined bean definition for bean '\" + beanName + \"' with a framework-generated bean definition: replacing [\" + existingDefinition + \"] with [\" + beanDefinition + \"]\"); &#125; &#125; else if (!beanDefinition.equals(existingDefinition)) &#123; // 新的 BeanDefinition 覆盖 if (logger.isInfoEnabled()) &#123; logger.info(\"Overriding bean definition for bean '\" + beanName + \"' with a different definition: replacing [\" + existingDefinition + \"] with [\" + beanDefinition + \"]\"); &#125; &#125; else &#123; // beanDefinition.equals(existingDefinition) if (logger.isDebugEnabled()) &#123; logger.debug(\"Overriding bean definition for bean '\" + beanName + \"' with an equivalent definition: replacing [\" + existingDefinition + \"] with [\" + beanDefinition + \"]\"); &#125; &#125; // 覆盖 this.beanDefinitionMap.put(beanName, beanDefinition); &#125; else &#123; // 没有相同 beanName 的 BeanDefinition，进入该分支 // 判断是否有其他 Bean 已经开始初始化。 // 注意，注册 BeanDefinition 后 Bean 没有被初始化，在 Spring 容器启动的最后，Spring 会根据 BeanDefinition 预初始化所有单例 Bean if (hasBeanCreationStarted()) &#123; // 进入此处，说明容器已经至少在 Bean 创建阶段了。这个时候需要获取锁来保证操作的安全。 synchronized (this.beanDefinitionMap) &#123; this.beanDefinitionMap.put(beanName, beanDefinition); List&lt;String&gt; updatedDefinitions = new ArrayList&lt;&gt;( this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) &#123; Set&lt;String&gt; updatedSingletons = new LinkedHashSet&lt;&gt;( this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; &#125; &#125; &#125; else &#123; // 将 BeanDefinition 放入 BeanDefinitionMap 中，该 Map 保存了所有的 BeanDefinition this.beanDefinitionMap.put(beanName, beanDefinition); // 将 beanName 放入 beanDefinitionNames 中，该 List 保存了所有 BeanDefinition 的名字 this.beanDefinitionNames.add(beanName); // 从手动注册的单例 Bean 集合中移除该 BeanDefinition this.manualSingletonNames.remove(beanName); &#125; this.frozenBeanDefinitionNames = null; &#125; if (existingDefinition != null || containsSingleton(beanName)) &#123; resetBeanDefinition(beanName); &#125;&#125; 方法很长，但是很多是为了日志打印，其中的重点是保存 BeanDefinition 相关信息的集合 beanDefinitionMap 会保存所有注册的 BeanDefinition，beanDefinitionNames 则会放入所有注册的 BeanDefinition 的 beanName。 到这里为止 BeanDefinition 已经注册到容器中 之后应该回到 org.springframework.context.support.AbstractApplicationContext#obtainFreshBeanFactory： 12345678910protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 关闭旧的 BeanFactory，创建新的 BeanFactory，加载并注册 BeanDefinition。抽象方法，交由子类实现 refreshBeanFactory(); // 获取上一步中创建的 BeanFactory。抽象方法，交由子类实现 ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(\"Bean factory for \" + getDisplayName() + \": \" + beanFactory); &#125; return beanFactory;&#125; 注册 BeanDefinition 之后，refreshBeanFactory 方法就执行完毕，之后将新的 BeanFactory 返回至 refresh 方法中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void refresh() throws BeansException, IllegalStateException &#123; // 保证该方法不会被同时访问，否则会出现在启动时同时销毁了原容器的现象 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录容器的启动时间，标记”已启动“ 状态、处理配置文件中的占位符 prepareRefresh(); // 将配置文件解析为 BeanDefinition 注册到 BeanFactory 中，但是没有初始化 Bean ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加 BeanPostProcessor，注册特殊 Bean prepareBeanFactory(beanFactory); try &#123; // 为容器的某些子类指定特殊的 BeanPost 事件处理器 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 实现类 registerBeanPostProcessors(beanFactory); // 初始化 MessageSource，与国际化相关 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 模板方法，初始化特殊 Bean onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有非懒加载的 singleton bean finishBeanFactoryInitialization(beanFactory); // 初始化容器的生命周期事件处理器，并发布容器的生命周期事件 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; // 销毁已创建的单例 Bean destroyBeans(); // 取消 refresh 操作，重置容器的同步标识 cancelRefresh(ex); throw ex; &#125; finally &#123; resetCommonCaches(); &#125; &#125;&#125; 进行完第二步 obtainFreshBeanFactory，下面进入第三步 prepareBeanFactory。 prepareBeanFactory对容器进行一些设置，例如类加载器、事件处理器等，以及注册一些特殊的 Bean。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 设置 BeanFactory 的类加载器为加载当前 ApplicationContext 的加载器，用于加载 BeanFactory 中注册的 BeanDefinition beanFactory.setBeanClassLoader(getClassLoader()); beanFactory.setBeanExpressionResolver( new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加一个 BeanPostProcessor，用于回调实现了 Aware 接口的 beans beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 如果某些 Bean 依赖于以下几个接口的实现类，在自动装配时会忽略它们，Spring 会通过其它方式处理依赖 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); // 为特殊 Bean 赋值，如果其它 Bean 依赖了以下的类，会注入这里的值。 beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 注册事件监听器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // loadTimeWeaver 是 AspectJ 的概念 if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader( new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; // 如果没有 environment、systemProperties、systemEnvironment 这些 Bean，Spring 会自己注册 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125;&#125; BeanDefinition在 Java 中， java.lang.Class 类来描述其它的类，可以依靠该类创建一个类的实例。 而在 Spring 中，它使用 BeanDefinition 来描述 Bean， BeanFactory 中存储的实际上是 BeanDefinition，而不是直接存储实例。当我们项目中需要某个单例类时，Spring 会检索容器中是否已经存在创建好的该类的实例，如果没有则依据 BeanFactory 中该类的 BeanDefinition 来创建一个实例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; /** * Bean 的作用域，默认只提供 singleton 和 prototype 两种 */ String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; /** * 设置父 Bean，继承父 Bean 的配置信息 */ void setParentName(@Nullable String parentName); /** * 获取父 Bean */ String getParentName(); /** * 设置 Bean 的类名称，之后通过反射来生成实例 */ void setBeanClassName(@Nullable String beanClassName); /** * 获取 Bean 的类名称 */ @Nullable String getBeanClassName(); /** * 设置 bean 的作用域 */ void setScope(@Nullable String scope); /** * 获取 bean 作用域 */ @Nullable String getScope(); /** * 设置是否懒加载 */ void setLazyInit(boolean lazyInit); /** * 是否懒加载 */ boolean isLazyInit(); /** * 设置 Bean 的依赖（对应 depends-on= 设置的值） */ void setDependsOn(@Nullable String... dependsOn); /** * 返回该 Bean 的所有依赖 */ @Nullable String[] getDependsOn(); /** * 设置 Bean 是否可以注入到其它 Bean 中，只对根据类型注入有效 * 如果根据名称注入，即使这里设置为 false 也能够注入 */ void setAutowireCandidate(boolean autowireCandidate); /** * Bean 是否可以注入到其它 Bean 中 */ boolean isAutowireCandidate(); /** * 当同一个接口有多个实现时，如果不指定名称，Spring 会优先选择 primary 为 true 的 bean */ void setPrimary(boolean primary); /** * 是否是 primary bean */ boolean isPrimary(); /** * 如果该 Bean 采用工厂方法生成，指定工厂名称 * * @see #setFactoryMethodName */ void setFactoryBeanName(@Nullable String factoryBeanName); /** * 获取工厂名称 */ @Nullable String getFactoryBeanName(); /** * 指定工厂类中的工厂方法名称 * * @see #setFactoryBeanName * @see #setBeanClassName */ void setFactoryMethodName(@Nullable String factoryMethodName); /** * 获取工程类中的工厂方法名称 */ @Nullable String getFactoryMethodName(); /** * 获取构造器参数 * * @return the ConstructorArgumentValues object (never &#123;@code null&#125;) */ ConstructorArgumentValues getConstructorArgumentValues(); /** * 该 Bean 的构造器是否带参数 * * @since 5.0.2 */ default boolean hasConstructorArgumentValues() &#123; return !getConstructorArgumentValues().isEmpty(); &#125; /** * Bean 中的属性 * * @return the MutablePropertyValues object (never &#123;@code null&#125;) */ MutablePropertyValues getPropertyValues(); /** * 该 Bean 是否有属性值 * * @since 5.0.2 */ default boolean hasPropertyValues() &#123; return !getPropertyValues().isEmpty(); &#125; /** * 是否为 singleton * * @see #SCOPE_SINGLETON */ boolean isSingleton(); /** * 是否为 prototype * * @see #SCOPE_PROTOTYPE * @since 3.0 */ boolean isPrototype(); /** * 是否为抽象 Bean，如果是则不能被实例化 */ boolean isAbstract(); int getRole(); /** * Bean 描述 */ @Nullable String getDescription(); /** * 定义 BeanDefinition 的资源描述 */ @Nullable String getResourceDescription(); @Nullable BeanDefinition getOriginatingBeanDefinition();&#125; 可以看到 BeanDefinition 中包含了很多的信息，都是用来描述 Bean 的各种特征。 Spring 也是基于该类来管理容器中的 Bean。 总结SpringIoC 对于 Bean 的管理流程非常长，贴了完整的代码便于跟踪，字符数接近 5w，所以需要拆出来作为上半部分。上半部分主要讲了 refresh 方法中关于从配置文件加载 BeanDefinition 以及注册到 BeanFactory 中的流程，在下半部分中会分析根据 BeanDefinition 创建 Bean 实例的过程。 本篇中加载及注册 BeanDefinition 几乎全部集中在 refreshBeanFactory 方法中，理解该方法也不容易，我也是跟着各种文章看了三遍左右才能在不看文章的情况下说出整个过程，还是需要跟着 Demo 多 Debug，有些难理解的点看看参数对应的值也许就能懂了。 这里贴一个简易的流程图，其中绘制了 ClassPathXmlApplicationContext 管理 Bean 的核心流程中的主要方法，没有写明每个方法的作用，用于在弄懂流程之后回顾。如果能根据这些方法名想起其中的过程，也就算是理解了。 参考链接 芋道源码 Javadoop Java 学习录","tags":[{"name":"SourceCode","slug":"SourceCode","permalink":"http://changleamazing.com/tags/SourceCode/"},{"name":"Ioc","slug":"Ioc","permalink":"http://changleamazing.com/tags/Ioc/"},{"name":"Spring","slug":"Spring","permalink":"http://changleamazing.com/tags/Spring/"}]},{"title":"消息消费","date":"2020-04-19T16:45:00.000Z","path":"2020/04/20/消息消费/","text":"消息拉取消息消费一般有两种模式，推模式与拉模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。Kafka 中的消息消费基于拉模式。 拉模式主要方法为 poll() 方法： 1ConsumerRecords&lt;K, V&gt; poll(Duration timeout); 当有消息待消费时，该方法会立即返回。否则，会等待参数 timeout 指定的时间。从 Kafka 2.0.0 开始，timeout 类型从 long 变为 JDK8 中新的时间类型 Duration。 Kafka 的消息消费是不断轮询的过程，消费者会重复调用 poll() 方法，获取所订阅的主题的一组消息。 消费者消费到的每条消息类型为 ConsumerRecord，与发送者发送的消息类型 ProducerRecord 对应。 消费位移Kafka 的分区中，每条消息都有唯一的 offset。这个 offset 在消费端也存在，用来表示消费到的消息在所在分区中的位置。 消费位移对应图中的 position，表示下一条需要拉取的消息的位置。在消费端拉取消息时，返回的是没有被消费过的消费集，这需要记录已经被消费的消息的位移，Kafka 将该位移持久化保存，否则当消费者重启之后就无法知道消费位移。另一种情况是当消费端有新的消费者加入之后，分区与消费者的关系会被重新分配，如果不保存分区的消费位移，新绑定的消费者就无法知道从哪个消息开始消费。 消费位移的存储从 Zookeeper 中转移到了 Kafka 内部主题 _consumer_offsets 中。 位移提交将消费位移持久化的动作称为“提交”，消费者在消费完消息之后需要执行消费位移的提交。 确定位移提交的时机是一个难题。 位移提交 在上图情况下，消费者拉取到了区间 [x+2,x+7] 的消息，并且正在处理 x+5 对应的消息。 消息处理前提交假设在 poll() 方法拉取到消息之后立刻进行位移提交，即消费位移置为 x+8。如果对 x+5 对应消息处理出现异常，在故障恢复之后，重新拉取的消息会从 x+8 开始的，这会导致区间 [x+5,x+7] 的消息未被消费，发生消息丢失的情况。 消息处理后提交假设在 poll() 方法拉取到消息并且将消息处理完毕再提交，如果仍然是处理到 x+5 时发生异常，故障恢复后从 x+2 位置重新开始消费，那么区间 [x+2,x+4] 的消息会被二次消费，发生重复消费的情况。 自动提交Kafka 默认的位移提交方式是自动提交，对应的参数为 enable.auto.commit，值为 true。自动提交的周期由参数 auto.commit.interval.ms 控制，默认值为 5s。 在默认的方式下，消费者每隔 5s 会将每个分区中最大的消息位移进行提交。提交的动作是在 poll() 方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交。 自动提交不需要我们进行额外的处理，但是自动提交也会带来重复消费与消息丢失的问题。当消费一批消息后而在位移提交前消费者崩溃了，那么故障恢复后，这些消息又会被重新消费一次。消息丢失的发生条件会苛刻一些。假设消费者端在拉取到了消息之后，将消息不断地放入本地缓存中，比如 BlockingQueue 中，而另一个线程对 BlockingQueue 中的数据进行处理。此时拉取线程直接返回处理完毕，在下一次拉取时进行位移提交。如果此时 BlockingQueue 中上一次拉取的数据还未被处理，且此时处理线程发生了异常，会导致之前被拉取的消息丢失了。 控制消费在某些应用场景下需要暂停某些分区消费先消费其它分区，之后再恢复该分区的消费。KafkaConsumer 中使用 pause() 与 resume() 方法实现暂停分区消费与恢复分区消费的操作，除此之外，还提供了 paused() 方法返回被暂停消费的分区集合。 KafkaConsumer 提供了 wakeup() 方法让其他线程安全调用，退出拉取消息的逻辑，抛出 WakeupException。 当消费端发生异常跳出循环之后，我们必须显示地执行关闭动作来释放占用的资源。KafkaConsumer 提供了 close() 方法来实现资源释放。 123public void close();public void close(Duration timeout); 第一种方法调用之后，会等待 Kafka 进行一些需要的清理操作，如果开启了自动提交消费位移，这里还会触发一次提交，虽然方法参数没有设置时间，但是 Kafka 内部默认等待时间为 30s。第二种方式则限制了在指定时间内完成对 Kafka 关闭的收尾工作，如果时间很短，消费者会在没有自动提交消费位移的情况下被强制关闭。wakeup() 方法不应该被用来中断 close() 方法。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"序列化与反序列化","date":"2020-04-18T16:45:00.000Z","path":"2020/04/19/序列化与反序列化/","text":"Kafka 内部都是以字节数组的形式来传播消息的。生产者使用序列化器将对象转换成字节数组，消费者使用反序列化器将字节数组转换成相应的对象。 序列化接口最常使用的是 StringSerializer，Kafka 也提供了对于 ByteArray、ByteBuffer、Bytes、Double、Integer、Long 这几种类型的序列化器，它们都实现了 org.apache.kafka.common.serialization.Serializer 接口。 该接口有四个方法： 1234567891011121314151617181920212223242526272829303132333435363738394041public interface Serializer&lt;T&gt; extends Closeable &#123; /** * Configure this class. * @param configs configs in key/value pairs * @param isKey whether is for key or value */ default void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; &#125; /** * Convert &#123;@code data&#125; into a byte array. * * @param topic topic associated with data * @param data typed data * @return serialized bytes */ byte[] serialize(String topic, T data); /** * Convert &#123;@code data&#125; into a byte array. * * @param topic topic associated with data * @param headers headers associated with the record * @param data typed data * @return serialized bytes */ default byte[] serialize(String topic, Headers headers, T data) &#123; return serialize(topic, data); &#125; /** * Close this serializer. * &lt;p&gt; * This method must be idempotent as it may be called multiple times. */ @Override default void close() &#123; &#125; configure() 方法用来配置当前类，serialize() 方法用来执行序列化操作，close() 方法用来关闭当前的序列化器，一般情况下实现类中都不会重写close() 方法，如果重写该方法，必须确保该方法的幂等性，因为这个方法很可能会被 KafkaProducer 调用多次。 自定义序列化器自定义序列化器非常简单，当 Kafka 提供的序列化器的序列化方法不满足我们的需求时，我们可以通过实现 org.apache.kafka.common.serialization.Serializer 接口，并重写其中的 serialize 方法即可。 之后只需要定义 KafkaProducer 的序列化配置。 1props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, CustomSerializer.class.getName()); 总结上述都是以生产者序列化器作为例子来说明，反序列化器与序列化器的逻辑及自定义方式一致。如果没有特殊需要，不建议自定义序列化器与反序列化器，这样会增加生产者与消费者的耦合度，升级换代容易出错。如果需要自定义序列化器与反序列化器，那么尽量在序列化方法中使用通用的序列化工具来包装，例如 Thrift、ProtoBuf 等。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"消费组与消费者","date":"2020-04-17T16:45:00.000Z","path":"2020/04/18/消费者与消费组/","text":"消费者负责订阅 Kafka 中的主题，并且从订阅的主题上拉取消息。每个消费者都有一个对应的消费组，当消息发布到主题后，只会被投递给订阅它的消费组中的一个消费者。 如上图所示，某个主题中共有 4 个分区。消费组 A 和 B 都订阅了这个主题，消费组 A 中有4个消费者，消费组B中有 2 个消费者。按照 Kafka 默认的规则，最后的分配结果是消费组 A 中的每一个消费者分配到 1 个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息，即每一个分区只能被一个消费组中的一个消费者所消费。 对于需要订阅同一个主题的不同服务来说，需要配置不同的消费组，否则会导致同一个消费组内的不同服务都只能消费到该主题中部分分区的消息，导致数据不完整或者状态错误。 消费组会根据组内消费者的个数，为每个消费者动态分配消费的主题分区个数。这种模型可以让整体消费能力具备横向伸缩性，可以通过增减消费者的个数来控制整体的消费能力。 如果出现消费组中消费者个数大于主题分区数的情况，会导致某些消费者分配不到分区而无法消费任何消息。 partition.assignment.strategy 参数用来指定分区分配策略。 Kafka 支持两种消息投递模式： 点对点模式（P2P） 点对点基于队列，消息生产者发送消息到队列中，消费者从队列中接收消息。一个队列可以存在多个消费者，但是一条消息只有一个消费者能消费到。 发布订阅模式（Pub/Sub） 发布订阅模式基于中间节点，对于 Kafka 来说即主题，这种模式下发布的消息能被所有订阅了该主题的消费者消费。 当所有的消费者都属于同一个消费组时，每个分区的消息只会被一个消费者处理，这属于点对点模式的应用。 当同一个主题的消费者不属于同一个消费组时，消息会被广播给所有的消费者，即每条消息会被所有的消费者处理，这属于发布/订阅模式的应用。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"生产者整体架构","date":"2020-04-14T01:11:00.000Z","path":"2020/04/14/生产者客户端原理/","text":"消息在发往 Kafka 之前，可能需要经历拦截器、序列化器和分区器等一系列的处理，生产者客户端的整体架构如下图所示： 生产者整体架构 可以看到，在生产者端主要有两个线程协调运行，分别为主线程与 Sender 线程。 其中主线程的作用是处理 KafkaProducer 创建消息，通过拦截器、序列化器和分区器的作用之后缓存消息到消息累加器（也称为消息收集器）中；而 Sender 线程则负责从消息收集器中获取消息并将其发送到 Kafka 中，其中 InFlightRequests 用来缓存已经被 Sender 线程发送但是还没有收到相应的请求。 RecordAccumulatorRecordAccumulator 是消息收集器，主要用来缓存要被发送至 Kafka 的消息，便于 Sender 线程批量发送，这样可以减少网络传输的资源消耗。RecordAccumulator 缓存的大小通过生产者客户端参数 buffer.memory 配置，默认大小为 32MB。 当生产者生产消息速度超过 Sender 线程发送至服务器的速度时，会导致消息积压在消息收集器中。当消息收集器缓存空间被填满时，KafkaProducer 调用 sender() 方法会被阻塞，当阻塞时间超过 max.block.ms 指定的时间就会抛出异常，默认为 60 秒。 RecordAccumulator 内部为每个分区都维护了一个双端队列，队列中存放的内容为 ProducerBatch，ProducerBatch 是一个消息批次，可以包含一个或多个 ProducerRecord。主线程中发送过来的消息都会被追加到双端队列中，其中较小的 ProducerRecord 会被拼凑成一个较大的 ProducerBatch。 消息都是以字节的形式传输的，在发送之前需要创建一块内存区域保存对应的消息。内存频繁创建与释放非常耗资源，在 RecordAccumulator 内部有一个 BufferPool，用来实现 ByteBuffer 的复用。BufferPool 只管理 batch.size 指定大小以下的 ByteBuffer，默认值为 16KB。 消息发送至 RecordAccumulator 中包括以下几个步骤： 当一条消息（ProducerRecord）流入 RecordAccumulator 时，会先寻找与消息分区所对应的双端队列（如果没有则新建） 从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建） 查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的 ProducerBatch 在新建 ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch，这样在使用完这段内存区域之后，可以通过 BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建 ProducerBatch，这段内存区域不会被复用。 InFlightRequestsInFlightRequests 用来缓存已经被 Sender 线程发送但是还没有收到响应的请求。 在介绍 InFlightRequests 存放的消息类型之前，要说明消息的保存形式的转换。 在主线程发送消息时，我们关注的是消息发往哪个分区，所以在 RecordAccumulator 中消息的保存形式为 &lt;Partition,Deque&lt;ProducerBatch&gt;&gt;，但是被 Sender 线程处理时，关注的是发送到 broker 集群中的哪个节点，所以消息保存的形式被转为 &lt;Node,List&lt;ProducerBatch&gt;&gt;，之后会被进一步封装成 &lt;Node,Request&gt;，其中 Request 是指 Kafka 的各种协议请求，对于消息发送来说就是 ProduceRequest。 当 Sender 线程将未收到响应的请求保存至 InFlightRequests 中，InFlightRequests 保存对象的形式是 Map&lt;NodeId,Deque&gt;。InFlightRequests 可以通过配置参数 max.in.flight.requests.per.connection 限制每个连接最多缓存的请求数，默认值为 5。即当某个连接中缓存了五个未响应的请求之后，就不能再向该连接发送更多的请求了，除非之后缓存的请求中收到了回复。 InFlightRequests 还可以获得 leastLoadedNode，即负载最小的 Node。负载最小是通过比较 Node 在 InFlightRequests 中未确认的请求决定的，未确认的请求越多，则认为负载越大。所以 Kafka 会选择 leastLoadedNode 发送请求，以便于能够尽快发出。 对于消息发送来说，在 RecordAccumulator 中就已经确定了分区，确定分区之后，要发送的 broker 即为该分区的 leader 副本所在的 broker，所以也就确定了 Node，无法根据 leastLoadedNode 来切换节点。这里 leastLoadedNode 是用来处理元数据请求、消费者组播协议的交互。 元数据是指 Kafka 集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的 leader 副本分配在哪个节点上，follower 副本分配在哪些节点上，哪些副本在 AR、ISR 等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 客户端可以自己发现 broker 节点的地址，这一过程也属于元数据相关的更新操作。与此同时，分区数量及 leader 副本的分布都会动态地变化，客户端也需要动态地捕捉这些变化。 而元数据的更新是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出 leastLoadedNode，然后向这个 Node 发送 MetadataRequest 请求来获取具体的元数据信息。这个更新操作是由 Sender 线程发起的，在创建完 MetadataRequest 之后同样会存入 InFlightRequests，之后各个 broker 会同步元数据。 生产者参数acks指定分区必须有几个副本收到消息生产者才会认为这条消息成功写入。该值设置到消息可靠性和吞吐量之间的权衡。 acks 有三种类型的字符串值： acks = 1。这是默认值。即只要分区的 leader 副本成功写入就会收到来自己服务器的成功响应。如果消息已写入 leader 副本，但是在被其它 follower 拉取之前 leader 节点崩溃了，这条消息就丢失掉了。这是可靠性与吞吐量之间的折中方案。 acks = 0。生产者发送消息之后不需要等待服务端的响应。消息从发送到写入 Kafka 的过程中出现异常就会丢失。当其它配置相同时，acks 设置为 0 可以达到最大的吞吐量。 acks = -1 或 acks = all。需要等待 ISR 中的所有副本都成功写入消息之后才能收到服务器的成功响应。其它配置相同时，acks 设置为 -1 可以保证最强的可靠性。但是有可能出现 ISR 集合中只有 leader 副本的情况，这与 acks = 1 情况一致。 Kafka 可以保证分区消息有序，如果生产者按照一定的顺序发送消息，那么这些消息也会顺序的写入分区。 但是如果 acks 参数为非零值，即必须要有副本确认收到消息，并且 max.in.flight.requests.per.connection 参数（即 InFlightRequests 中缓存的请求数量）配置大于 1，就可能会出现消息错序的情况。 假设第一批次消息写入失败，而第二批次消息发送成功，生产者就会重新发送第一批次的消息，导致第二批次消息比第一批次消息更早写入分区中，消息发生错序。 如果需要保证消息顺序时，建议把 max.in.flight.requests.per.connection 配置为 1，而不是配置 acks = 0，这样第一批次消息被缓存在 InFlightRequests 中时，第二批次消息无法发送，直到第一批次消息重试成功或者超过重试次数时才会发送第二批次消息。 max.request.size限制生产者客户端能发送的消息的最大值，默认为 1048576B，即 1MB。 该值与其它配置值有联动关系，一般情况下不改动。比如 broker 的 message.max.bytes 参数，如果 message.max.bytes 配置为 10B，而 max.request.size 配置为 20B。此时发送一条 15B 的消息时，生产者客户端也会收到异常。 retries指定发生异常时生产者重试的次数，默认为 0，即不进行任何重试。 retry.backoff.ms指定两次重试之间的时间间隔，避免无效的频繁重试。 compression.type指定消息的压缩方式，默认值为 none，即不压缩消息。该参数可以配置为 gzip/snappy/lz4。对消息进行压缩可以减少网络传输量，提高整体性能，但是压缩会耗费一定的时间，如果对时间有要求，则不推荐对消息进行压缩。 connections.max.idle.ms指定连接的最大闲置时间，默认为 540000ms，即 9 分钟。 linger.ms指定生产者发送 ProducerBatch 之前等待更多 ProducerRecord 加入 ProducerBatch 的时间，默认值为 0。即客户端会在ProducerBatch 被填满或者等待时间超过 linger.ms 配置值之后发送出去。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"Kafka 基础","date":"2020-04-12T16:45:00.000Z","path":"2020/04/13/Kafka 基础/","text":"Kafka 是 LinkedIn 开发的一个多分区、多副本且基于 ZooKeeper 协调的分布式消息系统。 Kafka 主要用三个用途： 消息系统Kafka 与消息中间件一样具备系统解耦、流量削峰、异步通信等功能。除此之外，Kafka 还提供了消息顺序性保障以及回溯消费的功能 存储系统消息持久化到磁盘中，相比于内存存储的系统降低了数据丢失的风险 流式处理平台Kafka 为流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库 基本概念 Kafka 结构 如上图，Kafka 体系结构主要包括三个部分： Producer生产者，消息发送方 Consumer消费者，消息接收方 Broker服务代理节点。 Kafka 中还有两个特别重要的概念：主题（Topic）与 分区（Partition）。Kafka 中的消息以 Topic 为单元进行归类，Producer 将消息发送到指定的 Topic，Consumer 则订阅 Topic 消费在该单元上的消息。 主题是逻辑概念，一个主题可以细分为多个分区，每个分区包含的消息是不同的，分区在存储层面相当于可追加的日志文件，消息被追加到分区日志文件的时候都会被分配一个特定的偏移量（offset）。offset 是消息在分区中的唯一标识，Kafka 用它来保证消息在分区中的顺序性(Kafka 保证分区有序而非主题有序）。 主题中的分区 如上图所示，假设主题中有四个分区，消息被顺序追加到分区日志文件尾部，offset 从 0 开始。一个主题中的分区可以分布在不同的 broker 上。 Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。 多副本机制 副本之间的关系是一主多从，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本同步消息，follower 副本中的消息与 leader 副本数据同步存在一定延迟。Kafka 通过算法来实现副本在 broker 上均匀分布，所以当 Kafka 集群中某个 broker 失效时，如果该 broker 中存在某分区的 leader 副本时，多副本机制能从该分区分布在其它 broker 上的 follower 副本中选举出新的 leader 副本，实现了故障的自动转移。 Kafka 消费端也具备一定的容灾能力。Consumer 使用拉模式从服务获取消息，并且会保存消费的具体位置。消费者在宕机后恢复上线时，可以根据之前保存的位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。 分区中所有副本集合称为 AR（Assigned Replicas），所有与 leader 副本保持一定程度同步的副本（包括 leader 副本）集合称为 ISR（In-Sync Replicas），这里的一定程度是指可忍受的同步滞后范围，通过参数可以配置。同步滞后过多的部分组成 OSR（Out-of-Sync Replicas）。ASR = ISR + OSR。正常情况下，所有 follower 副本都应该与 leader 副本保持一定程序的同步，此时 AR = ISR，OSR 集合为空。ISR 与 OSR 集合中的副本状态可能会发生变化，当 ISR 集合中副本滞后太多时，会被转移到 OSR 集合中。当 OSR 集合中副本在可忍受的滞后时间内又同步到了 leader 副本最新状态，它就会被转移到 ISR 集合中。默认情况下，若 leader 副本发生故障时，ISR 集合中副本才有资格被选举为新的 leader。 ISR 与 HW（High Watermark） 和 LEO（Log End Offset） 也有关系。 HW 和 LEO LEO 标识当前日志文件中下一条待写入消息的 offset。ISR 集合中每个副本都会维护自身的 LEO，而这些副本中最小的 LEO 即为分区的 HW，消费者只能消费 HW 之前的消息。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"百度分布式 ID 生成器","date":"2020-02-03T02:18:47.000Z","path":"2020/02/03/分布式 id 生成器/","text":"分布式 ID 生成器是分布式项目开发中的常用工具，弄懂其原理对理解分布式有一定的帮助。 绝大多数公司使用的分布式 ID 生成器都是依赖于雪花算法（snowflake）实现的。 snowflake snowflake 如上图所示，雪花算法生成 id 为 64 位二进制串，由几个部分组成： 1 位标识位，默认为 0，因为二进制首位为符号位，一般生成的 id 都要求是正数，所以固定为 0 41 位时间戳，可以表示的时间为 69 年 10 位工作机器 id，记录工作机器 id。可以部署在 $2^{10}$ (1024) 个节点上，包括 5 位 datacenterId 和 5 位 workerId 12 位自增序列。记录同一时间戳内产生的不同 id，支持的序号为$2^{12}$（4096）个 这样的设计可以保证所有生成的 id 按照时间趋势递增，并且不会产生重复的 id，也可以根据实际节点数扩展或缩减工作机器 id 部分的位数。 雪花算法的主要缺点是时钟回拨问题。 时钟回拨是指服务器时间因为某些原因导致时间回退。可能导致时钟回拨的原因有多种，比如服务器使用了本地时间，然后服务器校时服务修正了系统时间。 这样会导致生成一个已经使用过的 ID。 百度 UidGenerator百度的 UidGenerator 也是基于 snowflake 来实现的，不过调整了生成的 id 中的组成部分顺序。 UIDGenerator 如上图所示，UidGenerator 生成的 64 位二进制串主要包括以下几个部分： sign：1 位，与 snowflake 一致，固定为 1，即生成的 UID 为正数 delta seconds：28 位，时间戳，相对于时间基点 2016-05-20 的增量值，单位：秒，最多可支持约 8.7 年 worker id：22位，机器 id，每次机器启动（包括重启）时由数据库（MySQL 内置 WorkerID 分配器）分配（也可以自定义实现）。 sequence：13位，同一个时间戳的并发序列，可以支持$2^{13}$ (8192) 个并发。 UidGenerator 有两种实现方式：DefaultUidGenerator 和 CachedUidGenerator。 DefaultUIDGeneratordelta seconds这个值是当前时间与 epoch时间的时间差，单位为秒。epoch 时间默认为 2016-09-20，需要将它配置为生成分布式 ID 服务上线的时间。 worker idworker id 是在机器启动时通过 MySQL 的内置 WorkerID 分配器分配的。UidGenerator 会在生成分布式 ID 的实例启动的时候，向数据库的表中插入一行数据，数据的 ID 值就是 workerId 的值。由于 workerId 默认为 22 位，所以所有实例重启次数不超过 $2^{22} - 1$ 次。 sequence生成 sequence 部分的代码通过 synchronized 关键字保证线程安全，通过简单的异常处理来避免时钟回拨问题。 1234567891011121314151617protected synchronized long nextId() &#123; long currentSecond = getCurrentSecond(); if (currentSecond &lt; lastSecond) &#123; long refusedSeconds = lastSecond - currentSecond; throw new UidGenerateException(\"Clock moved backwards. Refusing for %d seconds\", refusedSeconds); &#125; if (currentSecond == lastSecond) &#123; sequence = (sequence + 1) &amp; bitsAllocator.getMaxSequence(); if (sequence == 0) &#123; currentSecond = getNextSecond(lastSecond); &#125; &#125; else &#123; sequence = 0L; &#125; lastSecond = currentSecond; return bitsAllocator.allocate(currentSecond - epochSeconds, workerId, sequence);&#125; 如果当前时间与上一次生成 id 时间为同一个时间戳，则增加 sequence。如果 sequence 自增值超过 $，就会通过自旋等待下一秒，而不是直接抛出异常。 如果当前时间是新的一秒，那么将 sequence 置为 0，重新开始分配该秒对应的 id。 CachedUIDGeneratorCachedUidGenerator 是 UidGenerator 的重要改进实现。它利用了 RingBuffer（与 disruptor 一致)。 RingBufferRingBuffer 本质上是一个数组，数组中的每个项被称为 slot。CachedUidGenerator 设计了两个 RingBuffer，一个用来保存唯一 ID，一个保存 flag。 每个 RingBuffer 容量为 SnowFlake 算法中 sequence 部分最大值，且为 $2^{n}$，对于 UidGenerator 默认设计来说，即为 $2^{13}$。 UID-RingBuffer 中 Tail 与 Cursor 指针用来读写 slot。其中，Tail 指针表示 Producer 生成的最大序号（此序号从 0 开始，持续递增）。Cursor 指针表示 Consumer 消费到的最小序号。 这两个指针不能超过对方。若 Cursor 指针超过 Tail，则说明消费了还未生产序号，所以当 Cursor 赶上 Tail 时，应该通过 RejectedTakeBufferHandler 指定 TakeRejectPolicy。 若 Tail 指针超过 Cursor 指针，则说明生产者覆盖了还未消费的 slot。所以当 Tail 赶上 Cursor 时，应该通过 RejectedPutBufferHandler 指定 PutRejectPolicy。 Flag-Ringbuffer 用来记录每个 slot 的状态（是否可填充、是否可消费）。 由于数组元素在内存中是连续分配的，这样可以最大程度利用 Cpu Cache 提升性能，但是会带来 伪共享 问题。 为了解决该问题，Uid-Generator 在 Tail、Cursor、Flag-RingBuffer 中采用 CacheLine 补齐方式。 FalseSharing 这里的说明可以看 RingBuffer 中补齐问题 。 RingBuffer 填充时机RingBuffer 共有三种填充方式 初始化预填充 RingBuffer 初始化时，预先填充整个 RingBuffer。 即时填充 消费 slot 时，即时检查剩余可以消费的 slot（tail - cursor)。如果小于设定阈值，则填充空余 slots。 周期填充 通过 Schedule 线程，定时补全空闲 slots。 上面分析了 CachedUidGenerator 依赖的数据结构，下面分析它的实现。实际上它继承了 DefaultUidGenerator，所以它是对 DefaultUidGenerator 的增强。 初始化CachedUidGenerator 在初始化时会给 workerId 赋值，方式与 DefaultUidGenerator 一致。还会初始化 RingBuffer，这个过程包括的操作有： 根据 boostPower 确定 RingBuffer 的 size 构造 RingBuffer，默认 paddingFactor 为 50。即当 RingBuffer 中剩余可用 ID 数量少于 50% 时，就触发一个异步线程往 RingBuffer 中填充新的 ID，直到填满为止 判断是否配置了 scheduleInterval 属性值，这个值表示检查填充的周期。默认不配置 初始化 Put 操作拒绝策略，对应属性 rejectedPutBufferHandler。即当 RingBuffer 已满，无法继续填充时的操作策略。默认情况下会丢弃Put 操作，记录日志。如果有需求，可以自定义实现 RejectedPutBufferHandler 接口 初始化 Take 操作拒绝策略，对应属性 rejectedTakeBufferHandler。即 RingBuffer 中没有可以使用的 ID 时的操作策略。默认情况下会记录日志并抛出 UidGenerateException 异常。如果有需求，可以自定义实现 RejectedTakeBufferHandler 接口 初始化填满 RingBuffer 中所有 slot 开启 buffer 补丁线程（需配置 scheduleInterval ） 第二步中的异步线程实现是 UidGenerator 解决时钟回拨的关键。在满足填充新的 ID 条件时，通过时间值递增得到新的时间值，而不是获取当前时间。 取值RingBuffer 初始化之后，就是取值过程了： 如果剩余可用 ID 百分比低于 paddingFactor 参数指定值，就会异步生成若干个 ID 集合，直到将 RingBuffer 填满。 如果获取值的位置追上了 tail 指针，就会执行 Task 操作的拒绝策略。 获取 slot 中的分布式 ID。 将该 slot 对应的 flag 设置为 CAN_PUT_FLAG。 总结绝大多数分布式 ID 生成器都是基于 SnowFlake 来实现的，而 SnowFlake 也有一些缺点。 本文中提到 Uid-Generator 通过自增列、RingBuffer 以及时间递增的措施解决了 SnowFlake 的传统问题。 这其中也涉及到一些计算机底层原理，关于该部分知识的解析会在其它文章中继续分析。","tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://changleamazing.com/tags/Distributed-system/"}]},{"title":"Redis 数据持久化","date":"2020-01-01T18:02:32.000Z","path":"2020/01/02/Redis 数据持久化/","text":"Redis 是一种内存型数据库，一旦服务器进程退出，数据库的数据就会丢失。为了解决这个问题，Redis 提供了 RDB 持久化、AOF 持久化、RDB-AOF 混合持久化等多种持久化方式，将内存中的数据保存到磁盘中，避免数据的丢失。 RDB 持久化RDB 持久化是 Redis 默认的持久化方式。 RDB 持久化会创建一个经过压缩的以 .rdb 结尾的二进制文件，其中包含了服务器在各个数据库中存储的键值对数据等信息。 RDB 文件创建创建 RDB文件有多种方式。用户可以使用 SAVE 或者 BGSAVE 命令手动创建 RDB 文件，也可以通过配置 save 配置项使服务器在满足指定条件时自动执行 BGSAVE 命令。 SAVE用户可以通过 SAVE 命令让 Redis 服务器以同步方式创建 RDB 文件。 SAVE 是无参数命令，如下： 12redis&gt; SAVEOK 在 SAVE 命令执行期间， Redis 服务器将阻塞，直到 RDB 文件创建完毕。 当执行 SAVE 命令时，如果本地已经存在相应的 RDB 文件，则会在新的 RDB 文件创建完成之后删除原有的 RDB 文件。 SAVE 命令的复杂度为 O(N)， N 表示 Redis 服务器所有数据库包含的键值对的总数。 BGSAVE因为 SAVE 命令是同步操作，会阻塞服务器，导致执行此命令期间 Redis 无法执行其它命令。所以 Redis 提供了 SAVE 命令的异步版本 — BGSAVE。BGSAVE 会使用子进程创建 RDB 文件。 BGSAVE 也是无参数命令： 12redis&gt; BGSAVEOK 虽然 BGSAVE 的异步操作不会使服务器在创建 RDB 文件过程中阻塞，但是创建子进程的过程会造成短时间的阻塞。 父进程调用操作系统 fork 函数创建一个子进程，而 fork 函数在父进程占用内存越大时，创建子进程耗时越长。 BGSAVE 命令的复杂度为 O(N)， N 表示 Redis 服务器所有数据库包含的键值对的总数。 配置自动创建 RDB 文件除了通过 SAVE 与 BGSAVE 命令手动创建 RDB 文件外，还可以通过在配置文件中配置 save 选项，让服务器在满足指定条件时自动执行 BGSAVE 命令。 save 配置项选项如下： 1save &lt;seconds&gt; &lt;changes&gt; seconds 参数指定触发持久化操作的周期，changes 参数用来指定触发持久化操作所需要的修改次数。 1save 60 10000 这个配置表示服务器在 60s 内至少执行了 10000 次修改时，服务器会自动执行 BGSAVE 命令。 Redis 默认持久化方式为 RDB，如果不改变默认配置，那么 Redis 使用的 save 选项为： 123save 60 10000save 300 100save 3600 1 这里配置了多个 save 选项，当其中任意一个条件被满足时就会触发服务器执行 BGSAVE 命令。 为了避免由于同时使用多个 RDB 文件创建方式或者配置多个 save 选项导致服务器频繁创建 RDB 文件，Redis 服务器在每次成功创建 RDB 文件后，会将负责自动触发 BGSAVE 命令的时间计数器以及修改次数计数器清零并重新开始计数。 SAVE 与 BGSAVE 的选择由于 SAVE 命令会阻塞 Redis 服务器向其它客户端服务，所以如果我们需要创建 RDB 文件时同时为其它客户端服务，就只能使用 BGSAVE 命令创建 RDB 文件。 而 SAVE 命令更适合维护离线 Redis 服务器，因为它不会创建子进程而消耗额外的内存。 RDB 优缺点RDB 持久化可以生成紧凑的 RDB 文件，并且使用 RDB 文件恢复数据也很快. 但是无论是 SAVE 命令还是 BGSAVE 命令，当服务器停机时，服务器丢失的数据量取决于创建 RDB 文件的时间间隔：间隔越长，丢失数据越多。如果提高执行 SAVE 或者 BGSAVE 命令的频率，会导致 Redis 服务器性能骤降，甚至低于传统关系型数据库。 所以 RDB 持久化更像是一种备份手段而不是一种常规数据持久化方案。 AOF 持久化RDB 持久化是全量式操作，而 AOF 是增量操作。 服务器每次执行完写命令之后，都会以协议文本的方式将被执行的写命令追加到 AOF 文件的结尾。在服务器停机之后，只需要重新执行 AOF 文件中保存的 Redis 命令，就可以将数据库恢复至停机之前的状态。 AOF 文件中唯一不是用户执行的命令是 SELECT，这是服务器根据用户正在使用的数据库号码自动加上的。 同步命令到 AOF 文件的整个过程可以分为三个阶段： 命令传播：Redis 将执行完的命令、命令的参数、命令的参数个数等信息发送到 AOF 程序中。 缓存追加：AOF 程序根据接收到的命令数据，将命令转换为网络通讯协议的格式，然后将协议内容追加到服务器的 AOF 缓存中。 文件写入和保存：AOF 缓存中的内容被写入到 AOF 文件末尾，如果设定的 AOF 保存条件被满足的话， fsync 函数或者 fdatasync 函数会被调用，将写入的内容真正地保存到磁盘中。 由于默认持久化方式为 RDB，所以用户需要配置 appendonly 选项来打开 AOF 持久化功能： 1appendonly yes 打开 AOF 持久化功能之后，Redis 在默认情况下会创建一个名为 appendonly.aof 的文件作为 AOF 文件。 AOF 文件冲洗频率 为了提高程序的写入性能，现代化的操作系统通常会把针对硬盘的多次写操作优化成一次写操作。当程序调用 write 系统调用对文件进行写入时，系统会先将数据写入位于内存的缓冲区中，当到达指定的时限或者满足某些写入条件时，系统才会调用 fsync 或者 fdatasync 函数，将缓冲区数据冲洗至硬盘。 上述机制虽然能提高写入性能，但是对于持久化功能来说，两次执行冲洗操作的间隔会影响持久化的安全性。 Redis 提供了 appendfsync 选项来控制系统冲洗 AOF 文件的频率。 1appendfsync always|everysec|no appendfsync 有三个可选值，分别代表的意义如下： always：每执行一个写命令，就对 AOF 文件执行一次冲洗操作 everysec：每隔 1s，就对 AOF 文件执行一次冲洗操作 no：不主动对 AOF 文件执行冲洗操作，由操作系统决定何时对 AOF 进行冲洗。 对于这三种冲洗策略来说，不同的安全性对应着不同的性能： always：最多只会丢失一个命令的数据，但是由于对磁盘的频繁写入，导致 Redis 服务器性能骤降至关系型数据库的水平 everysec：最多丢失 1s 之内产生的命令数据，这是一种兼顾性能与安全性的折中方案 no：最多丢失服务器最后一次冲洗 AOF 文件之后产生的所有命令数据，数据量的大小取决于系统冲洗 AOF 文件的频率，不安全 对比之下，Redis 选择 everysec 作为默认的冲洗策略，除非有明确的需求，否则也不应该修改该选项值。 AOF 重写由于 AOF 的增量特性，AOF 文件会越来越大，其中也会存在一些对相同键执行过的多次修改操作，导致有一部分命令是冗余的。 例如： 12345SELECT 0SET msg &quot;hello world!&quot;SET msg &quot;good bye&quot; 实际上，上述三条命令可以直接将第二条去掉，执行后最终效果与原来是一致的。这种冗余命令的存在增加了 AOF 文件的体积，恢复数据时耗费时间也越多。 为了减少冗余命令，Redis 提供了 AOF 重写功能，该功能会能够生成一个全新的 AOF 文件，其中只包含恢复当前数据库所需要的尽可能少的命令。 对于上面的三条命令来说，AOF 重写之后就会变成如下所示： 123SELECT 0SET msg &quot;good bye&quot; AOF 重写操作可以通过执行 BGREWRITEAOF 命令或者配置选项来触发。 BGREWRITEAOFBGREWRITEAOF 是一个无命令参数： 12redis&gt; BGREWRITEAOFBackground append only file rewriting started 复杂度为 O(N),N 表示服务器所有数据库包含的键值对总数。 BGREWRITEAOF 是一个异步命令，Redis 服务器接收到该命令之后会创建一个子进程来扫描数据库并生成新的 AOF 文件。当新的 AOF 文件生成完毕，子进程就会退出并通知 Redis，Redis 就会使用新的 AOF 文件代替原有 AOF 文件。 如果发送 BGREWRITEAOF 请求时，服务器正在创建 RDB 文件，那么服务器会将 AOF 重写操作延后到 RDB 文件创建完毕之后再执行，避免两个写操作同时执行导致服务器性能下降。 如果服务器在执行重写操作的过程中，又收到了新的 BGREWRITEAOF 命令，那么会返回以下错误： 12redis&gt; BGREWRITEAOF(error) ERR Background append only file rewriting already in progress AOF 重写配置选项以下两个配置选项可以设置 Redis 触发 BGREWRITEAOF 命令的条件： 12auto-aof-rewrite-min-size &lt;value&gt;auto-aof-rewirte-percentage &lt;value&gt; auto-aof-rewrite-min-size 选项用于设置触发 AOF 重写所需要的最小 AOF 文件体积。 例如对于该选项默认值来说： 1auto-aof-rewrite-min-size 64mb 当 AOF 文件体积小于 64mb 时，服务器不会自动执行 BGREWRITEAOF 命令。 auto-aof-rewirte-percentage 选项配置的值是触发 AOF 重写所需要的文件体积增大比例。 例如对于该选项默认值来说： 1auto-aof-rewirte-percentage 100 表示当前 AOF 文件体积比最后一次 AOF 文件重写之后的体积增大一倍时，会触发 BGREWRITEAOF 命令。 如果 Redis 还没有执行过 AOF 文件重写操作，那就会把启动服务器时使用的 AOF 文件体积当做最后一次 AOF 文件重写的体积。 假设 AOF 文件上次重写之后体积为 300MB，当前 AOF 文件达到 600MB 时,才会触发 AOF 重写操作。 AOF 持久化优缺点AOF 持久化的安全性是 RDB 望尘莫及的，正常情况下配置 appendonly everysec 可以将数据丢失的时间压缩至 1s 以内。 当然，AOF 也有相应的缺点： AOF 使用协议文本来存储操作,所以文件体积相对于包含相同数据的 RDB 文件来说会大得多，生成 AOF 文件所需的时间也比生成 RDB 文件时间更长 AOF 持久化需要通过执行 AOF 文件中保存的命令来恢复数据库，所以 AOF 持久化数据恢复速度比 RDB 文件恢复慢很多，并且数据库体积越大，差距越明显 AOF 使用的 BGREWRITEAOF 命令也需要创建子进程，如果数据库体积较大，进行 AOF 文件重写会占用大量资源，并导致服务器短暂阻塞。 RDB-AOF 混合持久化由于 RDB 持久化与 AOF 持久化都有各自优缺点，用户也较难抉择。 Redis4.0 开始，引入了 RDB-AOF 混合持久化模式，这种模式基于 AOF 持久化模式构建。所以需要用户打开 AOF 持久化功能，并且配置 1aof-use-rdb-preamble yes 此后，当 Redis 执行 AOF 重写操作时，会根据数据库当前的状态生成出对应的 RDB 数据，并且将这部分数据写入新建的 AOF 文件当中，而在此之后执行的写操作，会以协议文本的方式追加到新的 AOF 文件末尾（即 RDB 数据后）。 当支持 RDB-AOF 混合持久化模式的 Redis 服务器启动并载入 AOF 文件时，首先会检查 AOF 文件头部是否包含 RDB 格式的内容。如果包含，那服务器就会先载入 RDB 数据，之后再载入 AOF 数据。 RDB-AOF 混合持久化综合了 RDB 持久化与 AOF 持久化的优点。既可以通过 AOF 文件中的 RDB 数据快速恢复数据，又可以通过 AOF 包含的 AOF 数据将丢失数据的时间压缩至 1s 之内。 Redis 现在已发布 5.0 版本，默认是没有打开 RDB-AOF 混合持久化功能的。不过 Redis 作者声称该持久化方式之后会取代 RDB 持久化成为 Redis 默认持久化方式。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://changleamazing.com/tags/Redis/"},{"name":"Data Persistence","slug":"Data-Persistence","permalink":"http://changleamazing.com/tags/Data-Persistence/"}]},{"title":"HashMap 源码解析","date":"2019-12-11T16:52:50.000Z","path":"2019/12/12/HashMap/","text":"本文基于 JDK8 源码深入分析 HashMap 的结构与重要操作，并梳理一些面试中的常见问题。 类结构 成员变量DEFAULT_INITIAL_CAPACITY1static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; HashMap 中槽数量的默认值，即 HashMap 中 table 数组的 table.length; HashMap 初始化时，如果未指定 capacity 时，即设定 capacity 为此值。 MAXIMUM_CAPACITY1static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; HashMap 中槽数量的最大值。 HashMap 初始化时，如果指定的 capacity 大于该值，则将 capacity 设置为该值。 table1transient Node&lt;K,V&gt;[] table; HashMap 中装载数据的桶的数组。table.length 在分配数据之后长度总是 2 的幂。（除了在自举机制中一些操作允许长度为 0） entrySet1transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; 装载了所有键值对的集合，在遍历 HashMap 时一般使用这个集合，比使用 keySet 集合遍历速度大约快 1 倍。 size1transient int size; HashMap 中目前存放的键值对的个数。 modCount1transient int modCount; HashMap 结构更改的次数（即增删操作的次数）。用于在遍历时保证fail-fast 机制生效。 thresholdHashMap 中存放键值对的阈值，当 size &gt; threshold 时，会触发 table 数组扩容操作。 table 数组没有被分配数据时，threshold 值等于 0 或者是 table.length。而在 table 数组被分配数据之后，它的值等于 table.length * loadFactor。 loadFactor装载因子：即负载率；默认为 0.75。 JDK 1.7 中提到， As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur. 意思是 0.75 在时间和空间上提供了很好的折中。由于 threshold = capacity * loadFactor ，如果 loadFactor 设置过高，可以节省少量空间，但是会导致 threshold 和 capacity 非常接近， Hash 碰撞 的概率增大，一定程度上提高了 put 和 get 操作的耗时；如果 loadFactor 设置过低，则会产生相反的效果。 TREEIFY_THRESHOLD1static final int TREEIFY_THRESHOLD = 8; table数组中的每个槽中存储的数据量，在 大于 该值时，槽中存储数据的数据结构会变为红黑树。 ### UNTREEIFY_THRESHOLD 1static final int UNTREEIFY_THRESHOLD = 6; table`数组中的每个槽中存储的数据量，在 小于 该值时，槽中存储数据的数据结构会变为链表。 构造方法public HashMap()123public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; 最常用的构造方法。 loadFactor 为默认值 0.75 ; public HashMap(int initialCapacity)123public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; 调用 public HashMap(int initialCapacity, float loadFactor); 手动设置 HashMap 的初始化容量。 loadFactor 为默认值 0.75 ; public HashMap(int initialCapacity, float loadFactor)123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; loadFactor 为自定义值 ; static final int tableSizeFor(int cap)123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 从注释可知，该方法的返回值为 2 的幂。 实际上这个算法返回的是，大于 cap 的最接近 cap 的 2 的 次幂。 该构造方法，实际上控制的是 threshold 而不是直接设置 capacity。 public HashMap(Map&lt;? extends K, ? extends V&gt; m)1234public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; 用另一个 map 构造一个新的 hashmap。 loadFactor 为默认值。 final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict)1234567891011121314151617181920/** 若 evict 为 false,代表是在创建 hashMap 时调用了这个函数;若 evict 为true,代表是在创建 hashMap 后才调用这个函数，例如 putAll 函数。*/final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; if (table == null) &#123; // pre-size float ft = ((float)s / loadFactor) + 1.0F;// 阿里推荐初始化值 int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); if (t &gt; threshold) threshold = tableSizeFor(t); &#125; else if (s &gt; threshold) resize(); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; 初始化时， ft = ((float)s / loadFactor) + 1.0F,即为 table.length + 1。然后调用 threshold = tableSizeFor(t)；这样 threshold 的值，是 2 * table.length。所以，新增元素不会立刻导致扩容。 查public V get(Object key)1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; static final int hash(Object key)1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 该函数主要是调用了 key.hashCode( ) 方法，实际就是 Object 类中的 hashCode() 方法。作用是将对象的地址映射成一个整数值，尽量保证随机性。而 HashMap 中没有直接使用 Object 中的 hashCode() 的返回值作为 hash() 函数的结果，而是增加了 (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16) 这一步,将 hashCode() 的返回值与向右移动 16 位的 h 做异或运算。这里，(h &gt;&gt;&gt; 16) 叫做 扰动函数，该扰动函数保证了函数最后的返回值的后十六位中，是高位与低位共同运算出的结果。增加了节点在 table 数组中分布的随机性。 结果显示，当 HashMap 数组长度为 512 时，这个时候会取低 9 位的值来决定新增节点的位置。在有扰动函数的情况下，碰撞会减少 10%。 final Node&lt;K,V&gt; getNode(int hash, Object key)123456789101112131415161718192021222324final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; /* 条件判断，判断 `table` 数组不为空，且有元素存在；在该节点对应的槽里面也要有元素存在 */ if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断 key 是不是该槽位中的第一个元素。 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 如果该槽位中不止存在一个值，判断该槽位的节点是不是树节点 if ((e = first.next) != null) &#123; //如果是树节点，直接调用树查找节点的方法。并返回查找到的值 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 如果该槽位中的数据仍用链表存储。则直接遍历判断元素的 key 是不是等于要查找的 key do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 这是 HashMap 查找方法的主流程，相对比较简单。在进行最外层判断时，使用到了 tab[(n - 1) &amp; hash] 这段代码。tab[(n - 1) &amp; hash] 是 table.length 必须保持 2 的 次幂的关键。在得到一个元素 key 哈希运算返回值 hash 后，为了找到该元素在 table 中具体分布在哪个槽中，一般会使用 hash % table.length。当 table.length 等于 2^n 时， 存在hash % table.length = hash &amp; (table.length - 1)。这样，由于位运算更快，可以更加快速的找到每一个节点对应的槽位。 增/改public V put(K key, V value)123456789101112 /** * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; //evict 为true,代表是在创建 hashMap 后才调用这个函数 /** onlyIfAbsent 表示是否只在没有该节点映射时，put 才生效（是否允许覆盖）。 false 表示允许覆盖操作 **/ return putVal(hash(key), key, value, false, true);&#125; 如果 HashMap 中已经存在该节点的映射（更新操作），返回值会是旧节点的 value。如果不存在该节点的映射（新增操作），返回值会是 null。所以可以用返回值来判断原来的 HashMap 中是否存在关于该节点的映射，在某些时刻很有用。 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果数组没有初始化，或者长度为 0，则重新设置数组长度 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果该节点 key 对应的槽位没有元素，直接新建节点将该元素放入该槽位 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; /** 如果槽位的第一个元素 p 的 key 与 带插入的节点的 key 相等，则直接令 e = p，此时 e.value 被 p.value 替代，相当于更新操作 **/ if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; /** 如果槽位的元素是树节点，调用树的插入值的方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 槽中元素为链表节点 else &#123; for (int binCount = 0; ; ++binCount) &#123; // 判断 p 是否为尾结点 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 判断新增节点后，是否需要更新数据结构，槽中节点数等于 8 就更新 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果链表中有元素的 key 等于 e.key，则更新 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果 e 不为 null，说明之前存在该 key 的映射， if (e != null) &#123; // existing mapping for key V oldValue = e.value; // 允许覆盖则更新节点值 if (!onlyIfAbsent || oldValue == null) e.value = value; // 为 linkedHashMap 提供的函数，将最近访问的元素置于链表尾部，保证链表有序性 afterNodeAccess(e); return oldValue; &#125; &#125; // 修改次数增加 ++modCount; // 判断是否需要扩容 if (++size &gt; threshold) resize(); // 为 linkedHashMap 提供的函数，回调删除头节点 afterNodeInsertion(evict); return null; &#125; public V putIfAbsent(K key, V value)123public V putIfAbsent(K key, V value) &#123; return putVal(hash(key), key, value, true, true); &#125; 第四个参数 onlyIfAbsent 为 true 表示只允许插入操作，更新操作不生效。 删public V remove(Object key)12345public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; 如果返回值为空，表示 HashMap 中不存在该 key 对应的节点。否则，返回对应节点的 value。 final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; // 判断数组有元素存在且 key 对应的槽位有元素存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 如果槽中第一个元素 p 是要删除的节点，令 node = p if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; // 槽中第一个元素 p 不是要删除的节点，在后继节点中寻找 else if ((e = p.next) != null) &#123; // 槽中元素存储在 RBT 中，令 node 等于从树中查找到的节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); // 槽中元素存储在链表中，令 node 等于从链表中查找到的节点 else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // node 不为空, 且不需要匹配 value 或者成功匹配到 value，删除节点 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); // 如果要删除的节点为槽中第一个节点，则将第二个节点作为首节点 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; 扩容final Node&lt;K,V&gt;[] resize()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果数组已经初始化过且 table.length != 0 if (oldCap &gt; 0) &#123; /** 如果数组原来的长度为 MAXIMUM_CAPACITY，table.length 无法扩大， 修改 threshold = Integer.MAX_VALUE 使 map 可以继续存放元素**/ if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 扩大 table.length = min(oldCap * 2 ,MAXIMUM_CAPACITY) //只有原来 oldCap.length &gt;= 16，会使阈值翻倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //原来 table.length = 0 且 threshold ！= 0 ，在带参非集合初始化时会出现这种情况。 //设置 newCap 为初始化时构造函数中 tableSizeFor() 方法返回的 threshold else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // table.length = 0 &amp;&amp; threshold = 0 ，无参初始化时出现这种情况 // 设置 cap 和 threshold 为默认值 else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //有两种情况会符合该判断 //1. 原来的 table.length * 2 &gt;= MAXIMUM_CAPACITY， 此时将 threshold 设置为 Integer.MAX_VALUE //2. 当原来的 table.length &lt; 16 时，设置 threshold = min(MAXIMUM_CAPACITY,threshold * loadFactor)。(故意设置 loadFactor 很高时，会出现 threshold * loadFactor &gt; MAXIMUM_CAPACITY) if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 移动元素 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; //方便 GC oldTab[j] = null; // 如果该槽中只有一个元素，新数组的槽中依然只有它一个元素 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果槽中数据结构为 RBT else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 如果槽中数据结构为链表 else &#123; // preserve order // Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 确定该节点在 resize 之后是否会改变索引值 // e.hash &amp; oldCap = 0，说明索引值在 resize 之后不会改变 if ((e.hash &amp; oldCap) == 0) &#123; //将元素放在索引为 index 的链表尾部 if (loTail == null) loHead = e; else //这里说明了 JDK8 是尾插法 loTail.next = e; loTail = e; &#125; else &#123; //将元素放在索引为 index + oldCap 的链表尾部 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //将链表尾结点置空 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 其中有一些很关键的点， 移动元素前会使判断 e.hash &amp; oldCap 是否等于 0。假设 oldCap 为 2^k ，元素m 的索引为 oldIndex，hash(m) 的值为 hash，存在 oldIndex= hash &amp; ( 2^k - 1 )。(2^k - 1) 的结果的二进制表达式是 k 个 1。所以 oldIndex的结果等于 hash 的二进制表达式的后 k 位的值。 扩容后，新数组的长度 newCap 为 2^(k + 1)。元素 m 在新数组中的索引 newIndex = hash &amp; (2 ^ (k + 1) - 1)。同上，newIndex 的结果等于 hash 的二进制表达式的后 k + 1 位的值。用 b 代表 hash二进制表示的第k + 1位，那么 newIndex - oldIndex 就等于 b 代表的值。 b的权值为 2^k等于 oldCap。当 b = 0时， newIndex = oldIndex；当 b = 1时，newIndex = oldIndex + oldCap。所以，元素m在新数组的位置就由hash的第 k + 1位的值确定，这个值就等于hash &amp; oldCap。 因此，我们在扩充 HashMap 的时候，不需要像 JDK1.7 的实现那样重新计算 hash 了。 另一个关键点，在于移动时，使用尾插法，保持了元素在原来槽中的相对顺序。这个方法，解决了 JDK1.7 中多线程访问 HashMap 时，resize 过程中会出现的循环链表的问题。但 HashMap 仍然不是线程安全的。 重要问题JDK8 相对于 JDK7 优化点有哪些JDK8 对于 HashMap 的改动很大。主要的优化点在： HashMap 中使用的数据结构新增红黑树，当哈希冲突严重时，查找元素的耗时也不会恶化到 O(n) 级别。 插入元素的方式。JDK8 采用了尾插法插入元素，在扩容时保持了原来元素的相对顺序。而 JDK7 采用的是头插法，多线程扩容时可能会导致产生闭环问题。 扩容时，HashMap 中元素索引直接由元素 hashcode来计算是原位置或者是原位置 + 数组长度。而在 JDK7 中，元素扩容时，都会调用 hash() 方法重新计算元素的 hashcode ，再决定元素在数组中的索引。 JDK1.7 死循环问题当多线程添加元素并且引起扩容时，可能会触发 HashMap 中某个链表死循环。主要的原因是 JDK1.7 使用的头插法，导致原来两个节点的顺序在扩容后被翻转，多线程操作时就可能引起死循环。而在 JDK1.8 中，扩容时使用的是尾插法插入元素，这样元素的相对顺序不会改变，所以不会再出现死循环的问题。 JDK8 线程安全问题JDK8 中解决了 HashMap 死循环之后，依然不是线程的。 举两个例子： 当多线程放入两个 hashcode 一致的元素时，两个元素会放入相同的槽中，当他们获取到了同一个链表尾部元素时，会将各自的元素标记为链表尾部，导致其中一个元素丢失。 当多线程放入新元素时，都会执行到 ++size 这一步，表示 HashMap 中保存的元素数量增加了，但是 size 并不是 volatile 修饰的，多线程操作时可能会导致值被覆盖，从而 size 与实际数据不对。","tags":[{"name":"SourceCode","slug":"SourceCode","permalink":"http://changleamazing.com/tags/SourceCode/"},{"name":"HashMap","slug":"HashMap","permalink":"http://changleamazing.com/tags/HashMap/"}]},{"title":"Redis 分布式锁","date":"2019-11-17T16:52:50.000Z","path":"2019/11/18/分布式锁/","text":"在单节点中，需要用并发线程都能访问到的资源的状态变化来控制同步。在分布式应用中，使用应用所有节点都能访问到的 Redis 中的某个 key 来控制多节点访问。 单节点 Redis 分布式锁setnxsetnx 指令会在 key 不存在的情况下放入 redis，如果存在则不会设置。 123456&gt;setnx lock:distributed trueOK...other code...&gt;del lock:distributed 这种方式的问题在于，执行到 other code 时，程序出现异常，导致 del 指令不会被执行，key 没有被释放，这样会陷入死锁。 setnx then expire为了解决死锁，乍一看可以使用 expire 来给 key 设置超时时间。 1234567&gt;setnx lock:distributed trueOK&gt;expire lock:distributed 5...other code...&gt;del lock:distributed 这种处理其实仍然有问题，因为 setnx 与 expire 不是原子操作， 执行 expire 语句之前可能发生异常。死锁仍然会出现。 set and expire为了解决非原子性操作被中断的问题，在 Redis 2.8 中加入了 setnx 与 expire 组合在一起的原子指令。 123456&gt;set lock:distributed true ex 5 nxOK...other code...&gt;del lock:distributed 这种方式保证了加锁并设置有效时间操作的原子性，但是依然有问题。 假设我们在加锁与释放锁之间的业务代码执行时间超过了设置的有效时间，此时锁会因为超时被释放。会导致两种情况： 其他节点 B 获取锁之后，执行超时节点 A 执行完成，释放了 B 的锁。 其它节点获取到了锁，执行临界区代码时就可能会出现并发问题。 解决锁被其他线程释放问题因为在加锁时，各个节点使用的同一个 key，所以会存在超时节点释放了当前加锁节点的锁的情况。这种情况下，可以给加锁的 key 设置一个随机值，删除的时候需要判断 key 当前的 value 是不是等于随机值。 12345678910val &#x3D; Random.nextInt();if( redis.set(key,val,true,5) )&#123; ... other code ... value &#x3D; redis.get(key); if(val &#x3D;&#x3D; value)&#123; redis.delete(key); &#125;&#125; 上述代码实现了根据随机值删除的逻辑，但是获取 value 直到 delete 指令并非是原子指令，仍然可能有并发问题。这时候需要使用 lua 脚本处理，因为 lua 脚本可以保证连续多个指令原子执行。 12345if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1])else return 0end 这种方式可以避免锁被其他线程释放的问题。 临界区并发问题临界区代码出现并发问题的本质是业务代码执行时间大于锁过期时间。 我们可以定时刷新加锁时间，保证业务代码在锁过期时间内执行完成。 12345678910111213141516171819202122232425262728293031private volatile boolean isFlushExpiration = true;while(redis.set(lock, val, NOT_EXIST, SECONDS, 20))&#123; Thread thread = new Thread(new FlushExpirationTherad()); thread.setDeamon(true); thread.start(); ... other code ... &#125;isFlushExpiration = false;String deleteScript = \"if redis.call(\"get\",KEYS[1]) == ARGV[1] then\" + \"return redis.call(\"del\",KEYS[1])\" + \"else return 0 end\";redis.eval(deleteScript,1,key,val); private class FlushExpirationTherad implements Runnable&#123; @Override public void run()&#123; while(isFlushExpiration)&#123; String checkAndExpireScript = \"if redis.call('get', KEYS[1]) == ARGV[1] then \" + \"return redis.call('expire',KEYS[1],ARGV[2]) \" + \"else return 0 end\"; redis.eval(checkAndExpireScript,1,key,val,\"20\"); // 每隔十秒检查是否完成 Thread.sleep(10); &#125; &#125; &#125; 这种实现是用一个线程定期监控客户端是否执行完成。也可以由服务端实现心跳检测机制来保证业务完成（Zookeeper)。 所以实现单节点 Redis 分布式锁要关注三个关键问题： 获取锁与设置超时时间实现为原子操作（Redis2.8 开始已支持） 设置随机字符串保证释放锁时能保证只释放自己持有的锁（给对应的 key 设置随机值） 判断与释放锁必须实现为原子操作（lua 脚本实现） 多节点 Redis 分布式锁为了保证项目的高可用性，项目一般都配置了 Redis 集群，以防在单节点 Redis 宕机之后，所有客户端都无法获得锁。 在集群环境下，Redis 存在 failover 机制。当 Master 节点宕机之后，会开始异步的主从复制（replication），这个过程可能会出现以下情况： 客户端 A 获取了 Master 节点的锁。 Master 节点宕机了，存储锁的 key 暂未同步到 Slave 上。 Slave 节点升级为 Master 节点。 客户端 B 从新的 Master 节点上获取到了同一资源的锁。 在这种情况下，锁的安全性就会被打破，Redis 作者 antirez 针对此问题设计了 Redlock 算法。 Redlock 算法Redlock 算法获取锁时客户端执行步骤： 获取当前时间（start）。 依次向 N 个 Redis 节点请求锁。请求锁的方式与从单节点 Redis 获取锁的方式一致。为了保证在某个 Redis 节点不可用时该算法能够继续运行，获取锁的操作都需要设置超时时间，需要保证该超时时间远小于锁的有效时间。这样才能保证客户端在向某个 Redis 节点获取锁失败之后，可以立刻尝试下一个节点。 计算获取锁的过程总共消耗多长时间（consumeTime = end - start）。如果客户端从大多数 Redis 节点（&gt;= N/2 + 1) 成功获取锁，并且获取锁总时长没有超过锁的有效时间，这种情况下，客户端会认为获取锁成功，否则，获取锁失败。 如果最终获取锁成功，锁的有效时间应该重新设置为锁最初的有效时间减去 consumeTime。 如果最终获取锁失败，客户端应该立刻向所有 Redis 节点发起释放锁的请求。 在释放锁时，需要向所有 Redis 节点发起释放锁的操作，不管节点是否获取锁成功。因为可能存在客户端向 Redis 节点获取锁时成功，但节点通知客户端时通信失败，客户端会认为该节点加锁失败。 Redlock 算法实现了更高的可用性，也不会出现 failover 时失效的问题。但是如果有节点崩溃重启，仍然对锁的安全性有影响。假设共有 5 个 Redis 节点 A、B、C、D、E： 客户端 A 获取了 A、B、C 节点的锁，但 D 与 E 节点的锁获取失败。 节点 C 崩溃重启，但是客户端 A 在 C 上加的锁没有持久化下来，重启后丢失 节点 C 重启后，客户端 B 锁住了 C、D、E，获取锁成功。 在这种情况下，客户端 A 与 B 都获取了访问同一资源的锁。 这里第 2 步中节点 C 锁丢失的问题可能由多种原因引起。默认情况下，Redis 的 AOF 持久化方式是每秒写一次磁盘（fsync），这情况下就有可能丢失 1 秒的数据。我们也可以设置每次操作都触发 fsync，这会影响性能，不过即使这样设置，也有可能由于操作系统的问题导致操作写入失败。 为了解决节点重启导致的锁失效问题，antirez 提出了延迟重启的概念，即当一个节点崩溃之后并不立即重启，而是等待与分布式锁相关的 key 的有效时间都过期之后再重启，这样在该节点重启后也不会对现有的锁造成影响。 一些插曲关于 Redlock 的安全性问题，在分布式系统专家 Martin Kleppmann 和 Redis 的作者 antirez 之间发生过一场争论，这个问题引发了激烈的讨论。关于这场争论的内容可以关注 基于Redis的分布式锁到底安全吗 这篇文章。最后得出的结论是 Redlock 在效率要求的应用中是合理的，所以在 Java 项目中可以使用 Redlock 的 Java 版本 Redission 来控制多节点访问共享资源。但是仍有极端情况会造成 Redlock 的不安全，我们应该知道它在安全性上有哪些不足以及会造成什么后果。如果需要进一步的追求正确性，可以使用 Zookeeper 分布式锁。 相关链接 基于Redis的分布式锁到底安全吗","tags":[{"name":"Redis","slug":"Redis","permalink":"http://changleamazing.com/tags/Redis/"},{"name":"Distributed Lock","slug":"Distributed-Lock","permalink":"http://changleamazing.com/tags/Distributed-Lock/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://changleamazing.com/tags/Distributed-System/"}]}]