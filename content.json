[{"title":"消息消费","date":"2020-04-19T16:45:00.000Z","path":"2020/04/20/消息消费/","text":"消息拉取消息消费一般有两种模式，推模式与拉模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。Kafka 中的消息消费基于拉模式。 拉模式主要方法为 poll() 方法： 1ConsumerRecords&lt;K, V&gt; poll(Duration timeout); 当有消息待消费时，该方法会立即返回。否则，会等待参数 timeout 指定的时间。从 Kafka 2.0.0 开始，timeout 类型从 long 变为 JDK8 中新的时间类型 Duration。 Kafka 的消息消费是不断轮询的过程，消费者会重复调用 poll() 方法，获取所订阅的主题的一组消息。 消费者消费到的每条消息类型为 ConsumerRecord，与发送者发送的消息类型 ProducerRecord 对应。 消费位移Kafka 的分区中，每条消息都有唯一的 offset。这个 offset 在消费端也存在，用来表示消费到的消息在所在分区中的位置。 消费位移对应图中的 position，表示下一条需要拉取的消息的位置。在消费端拉取消息时，返回的是没有被消费过的消费集，这需要记录已经被消费的消息的位移，Kafka 将该位移持久化保存，否则当消费者重启之后就无法知道消费位移。另一种情况是当消费端有新的消费者加入之后，分区与消费者的关系会被重新分配，如果不保存分区的消费位移，新绑定的消费者就无法知道从哪个消息开始消费。 消费位移的存储从 Zookeeper 中转移到了 Kafka 内部主题 _consumer_offsets 中。 位移提交将消费位移持久化的动作称为“提交”，消费者在消费完消息之后需要执行消费位移的提交。 确定位移提交的时机是一个难题。 位移提交 在上图情况下，消费者拉取到了区间 [x+2,x+7] 的消息，并且正在处理 x+5 对应的消息。 消息处理前提交假设在 poll() 方法拉取到消息之后立刻进行位移提交，即消费位移置为 x+8。如果对 x+5 对应消息处理出现异常，在故障恢复之后，重新拉取的消息会从 x+8 开始的，这会导致区间 [x+5,x+7] 的消息未被消费，发生消息丢失的情况。 消息处理后提交假设在 poll() 方法拉取到消息并且将消息处理完毕再提交，如果仍然是处理到 x+5 时发生异常，故障恢复后从 x+2 位置重新开始消费，那么区间 [x+2,x+4] 的消息会被二次消费，发生重复消费的情况。 自动提交Kafka 默认的位移提交方式是自动提交，对应的参数为 enable.auto.commit，值为 true。自动提交的周期由参数 auto.commit.interval.ms 控制，默认值为 5s。 在默认的方式下，消费者每隔 5s 会将每个分区中最大的消息位移进行提交。提交的动作是在 poll() 方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交。 自动提交不需要我们进行额外的处理，但是自动提交也会带来重复消费与消息丢失的问题。当消费一批消息后而在位移提交前消费者崩溃了，那么故障恢复后，这些消息又会被重新消费一次。消息丢失的发生条件会苛刻一些。假设消费者端在拉取到了消息之后，将消息不断地放入本地缓存中，比如 BlockingQueue 中，而另一个线程对 BlockingQueue 中的数据进行处理。此时拉取线程直接返回处理完毕，在下一次拉取时进行位移提交。如果此时 BlockingQueue 中上一次拉取的数据还未被处理，且此时处理线程发生了异常，会导致之前被拉取的消息丢失了。 控制消费在某些应用场景下需要暂停某些分区消费先消费其它分区，之后再恢复该分区的消费。KafkaConsumer 中使用 pause() 与 resume() 方法实现暂停分区消费与恢复分区消费的操作，除此之外，还提供了 paused() 方法返回被暂停消费的分区集合。 KafkaConsumer 提供了 wakeup() 方法让其他线程安全调用，退出拉取消息的逻辑，抛出 WakeupException。 当消费端发生异常跳出循环之后，我们必须显示地执行关闭动作来释放占用的资源。KafkaConsumer 提供了 close() 方法来实现资源释放。 123public void close();public void close(Duration timeout); 第一种方法调用之后，会等待 Kafka 进行一些需要的清理操作，如果开启了自动提交消费位移，这里还会触发一次提交，虽然方法参数没有设置时间，但是 Kafka 内部默认等待时间为 30s。第二种方式则限制了在指定时间内完成对 Kafka 关闭的收尾工作，如果时间很短，消费者会在没有自动提交消费位移的情况下被强制关闭。wakeup() 方法不应该被用来中断 close() 方法。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"序列化与反序列化","date":"2020-04-18T16:45:00.000Z","path":"2020/04/19/序列化与反序列化/","text":"Kafka 内部都是以字节数组的形式来传播消息的。生产者使用序列化器将对象转换成字节数组，消费者使用反序列化器将字节数组转换成相应的对象。 序列化接口最常使用的是 StringSerializer，Kafka 也提供了对于 ByteArray、ByteBuffer、Bytes、Double、Integer、Long 这几种类型的序列化器，它们都实现了 org.apache.kafka.common.serialization.Serializer 接口。 该接口有四个方法： 1234567891011121314151617181920212223242526272829303132333435363738394041public interface Serializer&lt;T&gt; extends Closeable &#123; /** * Configure this class. * @param configs configs in key/value pairs * @param isKey whether is for key or value */ default void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; &#125; /** * Convert &#123;@code data&#125; into a byte array. * * @param topic topic associated with data * @param data typed data * @return serialized bytes */ byte[] serialize(String topic, T data); /** * Convert &#123;@code data&#125; into a byte array. * * @param topic topic associated with data * @param headers headers associated with the record * @param data typed data * @return serialized bytes */ default byte[] serialize(String topic, Headers headers, T data) &#123; return serialize(topic, data); &#125; /** * Close this serializer. * &lt;p&gt; * This method must be idempotent as it may be called multiple times. */ @Override default void close() &#123; &#125; configure() 方法用来配置当前类，serialize() 方法用来执行序列化操作，close() 方法用来关闭当前的序列化器，一般情况下实现类中都不会重写close() 方法，如果重写该方法，必须确保该方法的幂等性，因为这个方法很可能会被 KafkaProducer 调用多次。 自定义序列化器自定义序列化器非常简单，当 Kafka 提供的序列化器的序列化方法不满足我们的需求时，我们可以通过实现 org.apache.kafka.common.serialization.Serializer 接口，并重写其中的 serialize 方法即可。 之后只需要定义 KafkaProducer 的序列化配置。 1props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, CustomSerializer.class.getName()); 总结上述都是以生产者序列化器作为例子来说明，反序列化器与序列化器的逻辑及自定义方式一致。如果没有特殊需要，不建议自定义序列化器与反序列化器，这样会增加生产者与消费者的耦合度，升级换代容易出错。如果需要自定义序列化器与反序列化器，那么尽量在序列化方法中使用通用的序列化工具来包装，例如 Thrift、ProtoBuf 等。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"消费组与消费者","date":"2020-04-17T16:45:00.000Z","path":"2020/04/18/消费者与消费组/","text":"消费者负责订阅 Kafka 中的主题，并且从订阅的主题上拉取消息。每个消费者都有一个对应的消费组，当消息发布到主题后，只会被投递给订阅它的消费组中的一个消费者。 如上图所示，某个主题中共有 4 个分区。消费组 A 和 B 都订阅了这个主题，消费组 A 中有4个消费者，消费组B中有 2 个消费者。按照 Kafka 默认的规则，最后的分配结果是消费组 A 中的每一个消费者分配到 1 个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息，即每一个分区只能被一个消费组中的一个消费者所消费。 对于需要订阅同一个主题的不同服务来说，需要配置不同的消费组，否则会导致同一个消费组内的不同服务都只能消费到该主题中部分分区的消息，导致数据不完整或者状态错误。 消费组会根据组内消费者的个数，为每个消费者动态分配消费的主题分区个数。这种模型可以让整体消费能力具备横向伸缩性，可以通过增减消费者的个数来控制整体的消费能力。 如果出现消费组中消费者个数大于主题分区数的情况，会导致某些消费者分配不到分区而无法消费任何消息。 partition.assignment.strategy 参数用来指定分区分配策略。 Kafka 支持两种消息投递模式： 点对点模式（P2P） 点对点基于队列，消息生产者发送消息到队列中，消费者从队列中接收消息。一个队列可以存在多个消费者，但是一条消息只有一个消费者能消费到。 发布订阅模式（Pub/Sub） 发布订阅模式基于中间节点，对于 Kafka 来说即主题，这种模式下发布的消息能被所有订阅了该主题的消费者消费。 当所有的消费者都属于同一个消费组时，每个分区的消息只会被一个消费者处理，这属于点对点模式的应用。 当同一个主题的消费者不属于同一个消费组时，消息会被广播给所有的消费者，即每条消息会被所有的消费者处理，这属于发布/订阅模式的应用。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"生产者整体架构","date":"2020-04-14T01:11:00.000Z","path":"2020/04/14/生产者客户端原理/","text":"消息在发往 Kafka 之前，可能需要经历拦截器、序列化器和分区器等一系列的处理，生产者客户端的整体架构如下图所示： 生产者整体架构 可以看到，在生产者端主要有两个线程协调运行，分别为主线程与 Sender 线程。 其中主线程的作用是处理 KafkaProducer 创建消息，通过拦截器、序列化器和分区器的作用之后缓存消息到消息累加器（也称为消息收集器）中；而 Sender 线程则负责从消息收集器中获取消息并将其发送到 Kafka 中，其中 InFlightRequests 用来缓存已经被 Sender 线程发送但是还没有收到相应的请求。 RecordAccumulatorRecordAccumulator 是消息收集器，主要用来缓存要被发送至 Kafka 的消息，便于 Sender 线程批量发送，这样可以减少网络传输的资源消耗。RecordAccumulator 缓存的大小通过生产者客户端参数 buffer.memory 配置，默认大小为 32MB。 当生产者生产消息速度超过 Sender 线程发送至服务器的速度时，会导致消息积压在消息收集器中。当消息收集器缓存空间被填满时，KafkaProducer 调用 sender() 方法会被阻塞，当阻塞时间超过 max.block.ms 指定的时间就会抛出异常，默认为 60 秒。 RecordAccumulator 内部为每个分区都维护了一个双端队列，队列中存放的内容为 ProducerBatch，ProducerBatch 是一个消息批次，可以包含一个或多个 ProducerRecord。主线程中发送过来的消息都会被追加到双端队列中，其中较小的 ProducerRecord 会被拼凑成一个较大的 ProducerBatch。 消息都是以字节的形式传输的，在发送之前需要创建一块内存区域保存对应的消息。内存频繁创建与释放非常耗资源，在 RecordAccumulator 内部有一个 BufferPool，用来实现 ByteBuffer 的复用。BufferPool 只管理 batch.size 指定大小以下的 ByteBuffer，默认值为 16KB。 消息发送至 RecordAccumulator 中包括以下几个步骤： 当一条消息（ProducerRecord）流入 RecordAccumulator 时，会先寻找与消息分区所对应的双端队列（如果没有则新建） 从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建） 查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的 ProducerBatch 在新建 ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch，这样在使用完这段内存区域之后，可以通过 BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建 ProducerBatch，这段内存区域不会被复用。 InFlightRequestsInFlightRequests 用来缓存已经被 Sender 线程发送但是还没有收到响应的请求。 在介绍 InFlightRequests 存放的消息类型之前，要说明消息的保存形式的转换。 在主线程发送消息时，我们关注的是消息发往哪个分区，所以在 RecordAccumulator 中消息的保存形式为 &lt;Partition,Deque&lt;ProducerBatch&gt;&gt;，但是被 Sender 线程处理时，关注的是发送到 broker 集群中的哪个节点，所以消息保存的形式被转为 &lt;Node,List&lt;ProducerBatch&gt;&gt;，之后会被进一步封装成 &lt;Node,Request&gt;，其中 Request 是指 Kafka 的各种协议请求，对于消息发送来说就是 ProduceRequest。 当 Sender 线程将未收到响应的请求保存至 InFlightRequests 中，InFlightRequests 保存对象的形式是 Map&lt;NodeId,Deque&gt;。InFlightRequests 可以通过配置参数 max.in.flight.requests.per.connection 限制每个连接最多缓存的请求数，默认值为 5。即当某个连接中缓存了五个未响应的请求之后，就不能再向该连接发送更多的请求了，除非之后缓存的请求中收到了回复。 InFlightRequests 还可以获得 leastLoadedNode，即负载最小的 Node。负载最小是通过比较 Node 在 InFlightRequests 中未确认的请求决定的，未确认的请求越多，则认为负载越大。所以 Kafka 会选择 leastLoadedNode 发送请求，以便于能够尽快发出。 对于消息发送来说，在 RecordAccumulator 中就已经确定了分区，确定分区之后，要发送的 broker 即为该分区的 leader 副本所在的 broker，所以也就确定了 Node，无法根据 leastLoadedNode 来切换节点。这里 leastLoadedNode 是用来处理元数据请求、消费者组播协议的交互。 元数据是指 Kafka 集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的 leader 副本分配在哪个节点上，follower 副本分配在哪些节点上，哪些副本在 AR、ISR 等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 客户端可以自己发现 broker 节点的地址，这一过程也属于元数据相关的更新操作。与此同时，分区数量及 leader 副本的分布都会动态地变化，客户端也需要动态地捕捉这些变化。 而元数据的更新是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出 leastLoadedNode，然后向这个 Node 发送 MetadataRequest 请求来获取具体的元数据信息。这个更新操作是由 Sender 线程发起的，在创建完 MetadataRequest 之后同样会存入 InFlightRequests，之后各个 broker 会同步元数据。 生产者参数acks指定分区必须有几个副本收到消息生产者才会认为这条消息成功写入。该值设置到消息可靠性和吞吐量之间的权衡。 acks 有三种类型的字符串值： acks = 1。这是默认值。即只要分区的 leader 副本成功写入就会收到来自己服务器的成功响应。如果消息已写入 leader 副本，但是在被其它 follower 拉取之前 leader 节点崩溃了，这条消息就丢失掉了。这是可靠性与吞吐量之间的折中方案。 acks = 0。生产者发送消息之后不需要等待服务端的响应。消息从发送到写入 Kafka 的过程中出现异常就会丢失。当其它配置相同时，acks 设置为 0 可以达到最大的吞吐量。 acks = -1 或 acks = all。需要等待 ISR 中的所有副本都成功写入消息之后才能收到服务器的成功响应。其它配置相同时，acks 设置为 -1 可以保证最强的可靠性。但是有可能出现 ISR 集合中只有 leader 副本的情况，这与 acks = 1 情况一致。 Kafka 可以保证分区消息有序，如果生产者按照一定的顺序发送消息，那么这些消息也会顺序的写入分区。 但是如果 acks 参数为非零值，即必须要有副本确认收到消息，并且 max.in.flight.requests.per.connection 参数（即 InFlightRequests 中缓存的请求数量）配置大于 1，就可能会出现消息错序的情况。 假设第一批次消息写入失败，而第二批次消息发送成功，生产者就会重新发送第一批次的消息，导致第二批次消息比第一批次消息更早写入分区中，消息发生错序。 如果需要保证消息顺序时，建议把 max.in.flight.requests.per.connection 配置为 1，而不是配置 acks = 0，这样第一批次消息被缓存在 InFlightRequests 中时，第二批次消息无法发送，直到第一批次消息重试成功或者超过重试次数时才会发送第二批次消息。 max.request.size限制生产者客户端能发送的消息的最大值，默认为 1048576B，即 1MB。 该值与其它配置值有联动关系，一般情况下不改动。比如 broker 的 message.max.bytes 参数，如果 message.max.bytes 配置为 10B，而 max.request.size 配置为 20B。此时发送一条 15B 的消息时，生产者客户端也会收到异常。 retries指定发生异常时生产者重试的次数，默认为 0，即不进行任何重试。 retry.backoff.ms指定两次重试之间的时间间隔，避免无效的频繁重试。 compression.type指定消息的压缩方式，默认值为 none，即不压缩消息。该参数可以配置为 gzip/snappy/lz4。对消息进行压缩可以减少网络传输量，提高整体性能，但是压缩会耗费一定的时间，如果对时间有要求，则不推荐对消息进行压缩。 connections.max.idle.ms指定连接的最大闲置时间，默认为 540000ms，即 9 分钟。 linger.ms指定生产者发送 ProducerBatch 之前等待更多 ProducerRecord 加入 ProducerBatch 的时间，默认值为 0。即客户端会在ProducerBatch 被填满或者等待时间超过 linger.ms 配置值之后发送出去。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"Kafka 基础","date":"2020-04-12T16:45:00.000Z","path":"2020/04/13/Kafka 基础/","text":"Kafka 是 LinkedIn 开发的一个多分区、多副本且基于 ZooKeeper 协调的分布式消息系统。 Kafka 主要用三个用途： 消息系统Kafka 与消息中间件一样具备系统解耦、流量削峰、异步通信等功能。除此之外，Kafka 还提供了消息顺序性保障以及回溯消费的功能 存储系统消息持久化到磁盘中，相比于内存存储的系统降低了数据丢失的风险 流式处理平台Kafka 为流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库 基本概念 Kafka 结构 如上图，Kafka 体系结构主要包括三个部分： Producer生产者，消息发送方 Consumer消费者，消息接收方 Broker服务代理节点。 Kafka 中还有两个特别重要的概念：主题（Topic）与 分区（Partition）。Kafka 中的消息以 Topic 为单元进行归类，Producer 将消息发送到指定的 Topic，Consumer 则订阅 Topic 消费在该单元上的消息。 主题是逻辑概念，一个主题可以细分为多个分区，每个分区包含的消息是不同的，分区在存储层面相当于可追加的日志文件，消息被追加到分区日志文件的时候都会被分配一个特定的偏移量（offset）。offset 是消息在分区中的唯一标识，Kafka 用它来保证消息在分区中的顺序性(Kafka 保证分区有序而非主题有序）。 主题中的分区 如上图所示，假设主题中有四个分区，消息被顺序追加到分区日志文件尾部，offset 从 0 开始。一个主题中的分区可以分布在不同的 broker 上。 Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。 多副本机制 副本之间的关系是一主多从，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本同步消息，follower 副本中的消息与 leader 副本数据同步存在一定延迟。Kafka 通过算法来实现副本在 broker 上均匀分布，所以当 Kafka 集群中某个 broker 失效时，如果该 broker 中存在某分区的 leader 副本时，多副本机制能从该分区分布在其它 broker 上的 follower 副本中选举出新的 leader 副本，实现了故障的自动转移。 Kafka 消费端也具备一定的容灾能力。Consumer 使用拉模式从服务获取消息，并且会保存消费的具体位置。消费者在宕机后恢复上线时，可以根据之前保存的位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。 分区中所有副本集合称为 AR（Assigned Replicas），所有与 leader 副本保持一定程度同步的副本（包括 leader 副本）集合称为 ISR（In-Sync Replicas），这里的一定程度是指可忍受的同步滞后范围，通过参数可以配置。同步滞后过多的部分组成 OSR（Out-of-Sync Replicas）。ASR = ISR + OSR。正常情况下，所有 follower 副本都应该与 leader 副本保持一定程序的同步，此时 AR = ISR，OSR 集合为空。ISR 与 OSR 集合中的副本状态可能会发生变化，当 ISR 集合中副本滞后太多时，会被转移到 OSR 集合中。当 OSR 集合中副本在可忍受的滞后时间内又同步到了 leader 副本最新状态，它就会被转移到 ISR 集合中。默认情况下，若 leader 副本发生故障时，ISR 集合中副本才有资格被选举为新的 leader。 ISR 与 HW（High Watermark） 和 LEO（Log End Offset） 也有关系。 HW 和 LEO LEO 标识当前日志文件中下一条待写入消息的 offset。ISR 集合中每个副本都会维护自身的 LEO，而这些副本中最小的 LEO 即为分区的 HW，消费者只能消费 HW 之前的消息。","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://changleamazing.com/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"http://changleamazing.com/tags/Message-Queue/"}]},{"title":"百度分布式 ID 生成器","date":"2020-02-03T02:18:47.000Z","path":"2020/02/03/分布式 id 生成器/","text":"分布式 ID 生成器是分布式项目开发中的常用工具，弄懂其原理对理解分布式有一定的帮助。 绝大多数公司使用的分布式 ID 生成器都是依赖于雪花算法（snowflake）实现的。 snowflake snowflake 如上图所示，雪花算法生成 id 为 64 位二进制串，由几个部分组成： 1 位标识位，默认为 0，因为二进制首位为符号位，一般生成的 id 都要求是正数，所以固定为 0 41 位时间戳，可以表示的时间为 69 年 10 位工作机器 id，记录工作机器 id。可以部署在 $2^{10}$ (1024) 个节点上，包括 5 位 datacenterId 和 5 位 workerId 12 位自增序列。记录同一时间戳内产生的不同 id，支持的序号为$2^{12}$（4096）个 这样的设计可以保证所有生成的 id 按照时间趋势递增，并且不会产生重复的 id，也可以根据实际节点数扩展或缩减工作机器 id 部分的位数。 雪花算法的主要缺点是时钟回拨问题。 时钟回拨是指服务器时间因为某些原因导致时间回退。可能导致时钟回拨的原因有多种，比如服务器使用了本地时间，然后服务器校时服务修正了系统时间。 这样会导致生成一个已经使用过的 ID。 百度 UidGenerator百度的 UidGenerator 也是基于 snowflake 来实现的，不过调整了生成的 id 中的组成部分顺序。 UIDGenerator 如上图所示，UidGenerator 生成的 64 位二进制串主要包括以下几个部分： sign：1 位，与 snowflake 一致，固定为 1，即生成的 UID 为正数 delta seconds：28 位，时间戳，相对于时间基点 2016-05-20 的增量值，单位：秒，最多可支持约 8.7 年 worker id：22位，机器 id，每次机器启动（包括重启）时由数据库（MySQL 内置 WorkerID 分配器）分配（也可以自定义实现）。 sequence：13位，同一个时间戳的并发序列，可以支持$2^{13}$ (8192) 个并发。 UidGenerator 有两种实现方式：DefaultUidGenerator 和 CachedUidGenerator。 DefaultUIDGeneratordelta seconds这个值是当前时间与 epoch时间的时间差，单位为秒。epoch 时间默认为 2016-09-20，需要将它配置为生成分布式 ID 服务上线的时间。 worker idworker id 是在机器启动时通过 MySQL 的内置 WorkerID 分配器分配的。UidGenerator 会在生成分布式 ID 的实例启动的时候，向数据库的表中插入一行数据，数据的 ID 值就是 workerId 的值。由于 workerId 默认为 22 位，所以所有实例重启次数不超过 $2^{22} - 1$ 次。 sequence生成 sequence 部分的代码通过 synchronized 关键字保证线程安全，通过简单的异常处理来避免时钟回拨问题。 1234567891011121314151617protected synchronized long nextId() &#123; long currentSecond = getCurrentSecond(); if (currentSecond &lt; lastSecond) &#123; long refusedSeconds = lastSecond - currentSecond; throw new UidGenerateException(\"Clock moved backwards. Refusing for %d seconds\", refusedSeconds); &#125; if (currentSecond == lastSecond) &#123; sequence = (sequence + 1) &amp; bitsAllocator.getMaxSequence(); if (sequence == 0) &#123; currentSecond = getNextSecond(lastSecond); &#125; &#125; else &#123; sequence = 0L; &#125; lastSecond = currentSecond; return bitsAllocator.allocate(currentSecond - epochSeconds, workerId, sequence);&#125; 如果当前时间与上一次生成 id 时间为同一个时间戳，则增加 sequence。如果 sequence 自增值超过 $，就会通过自旋等待下一秒，而不是直接抛出异常。 如果当前时间是新的一秒，那么将 sequence 置为 0，重新开始分配该秒对应的 id。 CachedUIDGeneratorCachedUidGenerator 是 UidGenerator 的重要改进实现。它利用了 RingBuffer（与 disruptor 一致)。 RingBufferRingBuffer 本质上是一个数组，数组中的每个项被称为 slot。CachedUidGenerator 设计了两个 RingBuffer，一个用来保存唯一 ID，一个保存 flag。 每个 RingBuffer 容量为 SnowFlake 算法中 sequence 部分最大值，且为 $2^{n}$，对于 UidGenerator 默认设计来说，即为 $2^{13}$。 UID-RingBuffer 中 Tail 与 Cursor 指针用来读写 slot。其中，Tail 指针表示 Producer 生成的最大序号（此序号从 0 开始，持续递增）。Cursor 指针表示 Consumer 消费到的最小序号。 这两个指针不能超过对方。若 Cursor 指针超过 Tail，则说明消费了还未生产序号，所以当 Cursor 赶上 Tail 时，应该通过 RejectedTakeBufferHandler 指定 TakeRejectPolicy。 若 Tail 指针超过 Cursor 指针，则说明生产者覆盖了还未消费的 slot。所以当 Tail 赶上 Cursor 时，应该通过 RejectedPutBufferHandler 指定 PutRejectPolicy。 Flag-Ringbuffer 用来记录每个 slot 的状态（是否可填充、是否可消费）。 由于数组元素在内存中是连续分配的，这样可以最大程度利用 Cpu Cache 提升性能，但是会带来 伪共享 问题。 为了解决该问题，Uid-Generator 在 Tail、Cursor、Flag-RingBuffer 中采用 CacheLine 补齐方式。 FalseSharing 这里的说明可以看 RingBuffer 中补齐问题 。 RingBuffer 填充时机RingBuffer 共有三种填充方式 初始化预填充 RingBuffer 初始化时，预先填充整个 RingBuffer。 即时填充 消费 slot 时，即时检查剩余可以消费的 slot（tail - cursor)。如果小于设定阈值，则填充空余 slots。 周期填充 通过 Schedule 线程，定时补全空闲 slots。 上面分析了 CachedUidGenerator 依赖的数据结构，下面分析它的实现。实际上它继承了 DefaultUidGenerator，所以它是对 DefaultUidGenerator 的增强。 初始化CachedUidGenerator 在初始化时会给 workerId 赋值，方式与 DefaultUidGenerator 一致。还会初始化 RingBuffer，这个过程包括的操作有： 根据 boostPower 确定 RingBuffer 的 size 构造 RingBuffer，默认 paddingFactor 为 50。即当 RingBuffer 中剩余可用 ID 数量少于 50% 时，就触发一个异步线程往 RingBuffer 中填充新的 ID，直到填满为止 判断是否配置了 scheduleInterval 属性值，这个值表示检查填充的周期。默认不配置 初始化 Put 操作拒绝策略，对应属性 rejectedPutBufferHandler。即当 RingBuffer 已满，无法继续填充时的操作策略。默认情况下会丢弃Put 操作，记录日志。如果有需求，可以自定义实现 RejectedPutBufferHandler 接口 初始化 Take 操作拒绝策略，对应属性 rejectedTakeBufferHandler。即 RingBuffer 中没有可以使用的 ID 时的操作策略。默认情况下会记录日志并抛出 UidGenerateException 异常。如果有需求，可以自定义实现 RejectedTakeBufferHandler 接口 初始化填满 RingBuffer 中所有 slot 开启 buffer 补丁线程（需配置 scheduleInterval ） 第二步中的异步线程实现是 UidGenerator 解决时钟回拨的关键。在满足填充新的 ID 条件时，通过时间值递增得到新的时间值，而不是获取当前时间。 取值RingBuffer 初始化之后，就是取值过程了： 如果剩余可用 ID 百分比低于 paddingFactor 参数指定值，就会异步生成若干个 ID 集合，直到将 RingBuffer 填满。 如果获取值的位置追上了 tail 指针，就会执行 Task 操作的拒绝策略。 获取 slot 中的分布式 ID。 将该 slot 对应的 flag 设置为 CAN_PUT_FLAG。 总结绝大多数分布式 ID 生成器都是基于 SnowFlake 来实现的，而 SnowFlake 也有一些缺点。 本文中提到 Uid-Generator 通过自增列、RingBuffer 以及时间递增的措施解决了 SnowFlake 的传统问题。 这其中也涉及到一些计算机底层原理，关于该部分知识的解析会在其它文章中继续分析。","tags":[{"name":"Distributed system","slug":"Distributed-system","permalink":"http://changleamazing.com/tags/Distributed-system/"}]},{"title":"Redis 数据持久化","date":"2020-01-01T18:02:32.000Z","path":"2020/01/02/Redis 数据持久化/","text":"Redis 是一种内存型数据库，一旦服务器进程退出，数据库的数据就会丢失。为了解决这个问题，Redis 提供了 RDB 持久化、AOF 持久化、RDB-AOF 混合持久化等多种持久化方式，将内存中的数据保存到磁盘中，避免数据的丢失。 RDB 持久化RDB 持久化是 Redis 默认的持久化方式。 RDB 持久化会创建一个经过压缩的以 .rdb 结尾的二进制文件，其中包含了服务器在各个数据库中存储的键值对数据等信息。 RDB 文件创建创建 RDB文件有多种方式。用户可以使用 SAVE 或者 BGSAVE 命令手动创建 RDB 文件，也可以通过配置 save 配置项使服务器在满足指定条件时自动执行 BGSAVE 命令。 SAVE用户可以通过 SAVE 命令让 Redis 服务器以同步方式创建 RDB 文件。 SAVE 是无参数命令，如下： 12redis&gt; SAVEOK 在 SAVE 命令执行期间， Redis 服务器将阻塞，直到 RDB 文件创建完毕。 当执行 SAVE 命令时，如果本地已经存在相应的 RDB 文件，则会在新的 RDB 文件创建完成之后删除原有的 RDB 文件。 SAVE 命令的复杂度为 O(N)， N 表示 Redis 服务器所有数据库包含的键值对的总数。 BGSAVE因为 SAVE 命令是同步操作，会阻塞服务器，导致执行此命令期间 Redis 无法执行其它命令。所以 Redis 提供了 SAVE 命令的异步版本 — BGSAVE。BGSAVE 会使用子进程创建 RDB 文件。 BGSAVE 也是无参数命令： 12redis&gt; BGSAVEOK 虽然 BGSAVE 的异步操作不会使服务器在创建 RDB 文件过程中阻塞，但是创建子进程的过程会造成短时间的阻塞。 父进程调用操作系统 fork 函数创建一个子进程，而 fork 函数在父进程占用内存越大时，创建子进程耗时越长。 BGSAVE 命令的复杂度为 O(N)， N 表示 Redis 服务器所有数据库包含的键值对的总数。 配置自动创建 RDB 文件除了通过 SAVE 与 BGSAVE 命令手动创建 RDB 文件外，还可以通过在配置文件中配置 save 选项，让服务器在满足指定条件时自动执行 BGSAVE 命令。 save 配置项选项如下： 1save &lt;seconds&gt; &lt;changes&gt; seconds 参数指定触发持久化操作的周期，changes 参数用来指定触发持久化操作所需要的修改次数。 1save 60 10000 这个配置表示服务器在 60s 内至少执行了 10000 次修改时，服务器会自动执行 BGSAVE 命令。 Redis 默认持久化方式为 RDB，如果不改变默认配置，那么 Redis 使用的 save 选项为： 123save 60 10000save 300 100save 3600 1 这里配置了多个 save 选项，当其中任意一个条件被满足时就会触发服务器执行 BGSAVE 命令。 为了避免由于同时使用多个 RDB 文件创建方式或者配置多个 save 选项导致服务器频繁创建 RDB 文件，Redis 服务器在每次成功创建 RDB 文件后，会将负责自动触发 BGSAVE 命令的时间计数器以及修改次数计数器清零并重新开始计数。 SAVE 与 BGSAVE 的选择由于 SAVE 命令会阻塞 Redis 服务器向其它客户端服务，所以如果我们需要创建 RDB 文件时同时为其它客户端服务，就只能使用 BGSAVE 命令创建 RDB 文件。 而 SAVE 命令更适合维护离线 Redis 服务器，因为它不会创建子进程而消耗额外的内存。 RDB 优缺点RDB 持久化可以生成紧凑的 RDB 文件，并且使用 RDB 文件恢复数据也很快. 但是无论是 SAVE 命令还是 BGSAVE 命令，当服务器停机时，服务器丢失的数据量取决于创建 RDB 文件的时间间隔：间隔越长，丢失数据越多。如果提高执行 SAVE 或者 BGSAVE 命令的频率，会导致 Redis 服务器性能骤降，甚至低于传统关系型数据库。 所以 RDB 持久化更像是一种备份手段而不是一种常规数据持久化方案。 AOF 持久化RDB 持久化是全量式操作，而 AOF 是增量操作。 服务器每次执行完写命令之后，都会以协议文本的方式将被执行的写命令追加到 AOF 文件的结尾。在服务器停机之后，只需要重新执行 AOF 文件中保存的 Redis 命令，就可以将数据库恢复至停机之前的状态。 AOF 文件中唯一不是用户执行的命令是 SELECT，这是服务器根据用户正在使用的数据库号码自动加上的。 同步命令到 AOF 文件的整个过程可以分为三个阶段： 命令传播：Redis 将执行完的命令、命令的参数、命令的参数个数等信息发送到 AOF 程序中。 缓存追加：AOF 程序根据接收到的命令数据，将命令转换为网络通讯协议的格式，然后将协议内容追加到服务器的 AOF 缓存中。 文件写入和保存：AOF 缓存中的内容被写入到 AOF 文件末尾，如果设定的 AOF 保存条件被满足的话， fsync 函数或者 fdatasync 函数会被调用，将写入的内容真正地保存到磁盘中。 由于默认持久化方式为 RDB，所以用户需要配置 appendonly 选项来打开 AOF 持久化功能： 1appendonly yes 打开 AOF 持久化功能之后，Redis 在默认情况下会创建一个名为 appendonly.aof 的文件作为 AOF 文件。 AOF 文件冲洗频率 为了提高程序的写入性能，现代化的操作系统通常会把针对硬盘的多次写操作优化成一次写操作。当程序调用 write 系统调用对文件进行写入时，系统会先将数据写入位于内存的缓冲区中，当到达指定的时限或者满足某些写入条件时，系统才会调用 fsync 或者 fdatasync 函数，将缓冲区数据冲洗至硬盘。 上述机制虽然能提高写入性能，但是对于持久化功能来说，两次执行冲洗操作的间隔会影响持久化的安全性。 Redis 提供了 appendfsync 选项来控制系统冲洗 AOF 文件的频率。 1appendfsync always|everysec|no appendfsync 有三个可选值，分别代表的意义如下： always：每执行一个写命令，就对 AOF 文件执行一次冲洗操作 everysec：每隔 1s，就对 AOF 文件执行一次冲洗操作 no：不主动对 AOF 文件执行冲洗操作，由操作系统决定何时对 AOF 进行冲洗。 对于这三种冲洗策略来说，不同的安全性对应着不同的性能： always：最多只会丢失一个命令的数据，但是由于对磁盘的频繁写入，导致 Redis 服务器性能骤降至关系型数据库的水平 everysec：最多丢失 1s 之内产生的命令数据，这是一种兼顾性能与安全性的折中方案 no：最多丢失服务器最后一次冲洗 AOF 文件之后产生的所有命令数据，数据量的大小取决于系统冲洗 AOF 文件的频率，不安全 对比之下，Redis 选择 everysec 作为默认的冲洗策略，除非有明确的需求，否则也不应该修改该选项值。 AOF 重写由于 AOF 的增量特性，AOF 文件会越来越大，其中也会存在一些对相同键执行过的多次修改操作，导致有一部分命令是冗余的。 例如： 12345SELECT 0SET msg &quot;hello world!&quot;SET msg &quot;good bye&quot; 实际上，上述三条命令可以直接将第二条去掉，执行后最终效果与原来是一致的。这种冗余命令的存在增加了 AOF 文件的体积，恢复数据时耗费时间也越多。 为了减少冗余命令，Redis 提供了 AOF 重写功能，该功能会能够生成一个全新的 AOF 文件，其中只包含恢复当前数据库所需要的尽可能少的命令。 对于上面的三条命令来说，AOF 重写之后就会变成如下所示： 123SELECT 0SET msg &quot;good bye&quot; AOF 重写操作可以通过执行 BGREWRITEAOF 命令或者配置选项来触发。 BGREWRITEAOFBGREWRITEAOF 是一个无命令参数： 12redis&gt; BGREWRITEAOFBackground append only file rewriting started 复杂度为 O(N),N 表示服务器所有数据库包含的键值对总数。 BGREWRITEAOF 是一个异步命令，Redis 服务器接收到该命令之后会创建一个子进程来扫描数据库并生成新的 AOF 文件。当新的 AOF 文件生成完毕，子进程就会退出并通知 Redis，Redis 就会使用新的 AOF 文件代替原有 AOF 文件。 如果发送 BGREWRITEAOF 请求时，服务器正在创建 RDB 文件，那么服务器会将 AOF 重写操作延后到 RDB 文件创建完毕之后再执行，避免两个写操作同时执行导致服务器性能下降。 如果服务器在执行重写操作的过程中，又收到了新的 BGREWRITEAOF 命令，那么会返回以下错误： 12redis&gt; BGREWRITEAOF(error) ERR Background append only file rewriting already in progress AOF 重写配置选项以下两个配置选项可以设置 Redis 触发 BGREWRITEAOF 命令的条件： 12auto-aof-rewrite-min-size &lt;value&gt;auto-aof-rewirte-percentage &lt;value&gt; auto-aof-rewrite-min-size 选项用于设置触发 AOF 重写所需要的最小 AOF 文件体积。 例如对于该选项默认值来说： 1auto-aof-rewrite-min-size 64mb 当 AOF 文件体积小于 64mb 时，服务器不会自动执行 BGREWRITEAOF 命令。 auto-aof-rewirte-percentage 选项配置的值是触发 AOF 重写所需要的文件体积增大比例。 例如对于该选项默认值来说： 1auto-aof-rewirte-percentage 100 表示当前 AOF 文件体积比最后一次 AOF 文件重写之后的体积增大一倍时，会触发 BGREWRITEAOF 命令。 如果 Redis 还没有执行过 AOF 文件重写操作，那就会把启动服务器时使用的 AOF 文件体积当做最后一次 AOF 文件重写的体积。 假设 AOF 文件上次重写之后体积为 300MB，当前 AOF 文件达到 600MB 时,才会触发 AOF 重写操作。 AOF 持久化优缺点AOF 持久化的安全性是 RDB 望尘莫及的，正常情况下配置 appendonly everysec 可以将数据丢失的时间压缩至 1s 以内。 当然，AOF 也有相应的缺点： AOF 使用协议文本来存储操作,所以文件体积相对于包含相同数据的 RDB 文件来说会大得多，生成 AOF 文件所需的时间也比生成 RDB 文件时间更长 AOF 持久化需要通过执行 AOF 文件中保存的命令来恢复数据库，所以 AOF 持久化数据恢复速度比 RDB 文件恢复慢很多，并且数据库体积越大，差距越明显 AOF 使用的 BGREWRITEAOF 命令也需要创建子进程，如果数据库体积较大，进行 AOF 文件重写会占用大量资源，并导致服务器短暂阻塞。 RDB-AOF 混合持久化由于 RDB 持久化与 AOF 持久化都有各自优缺点，用户也较难抉择。 Redis4.0 开始，引入了 RDB-AOF 混合持久化模式，这种模式基于 AOF 持久化模式构建。所以需要用户打开 AOF 持久化功能，并且配置 1aof-use-rdb-preamble yes 此后，当 Redis 执行 AOF 重写操作时，会根据数据库当前的状态生成出对应的 RDB 数据，并且将这部分数据写入新建的 AOF 文件当中，而在此之后执行的写操作，会以协议文本的方式追加到新的 AOF 文件末尾（即 RDB 数据后）。 当支持 RDB-AOF 混合持久化模式的 Redis 服务器启动并载入 AOF 文件时，首先会检查 AOF 文件头部是否包含 RDB 格式的内容。如果包含，那服务器就会先载入 RDB 数据，之后再载入 AOF 数据。 RDB-AOF 混合持久化综合了 RDB 持久化与 AOF 持久化的优点。既可以通过 AOF 文件中的 RDB 数据快速恢复数据，又可以通过 AOF 包含的 AOF 数据将丢失数据的时间压缩至 1s 之内。 Redis 现在已发布 5.0 版本，默认是没有打开 RDB-AOF 混合持久化功能的。不过 Redis 作者声称该持久化方式之后会取代 RDB 持久化成为 Redis 默认持久化方式。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://changleamazing.com/tags/Redis/"},{"name":"Data Persistence","slug":"Data-Persistence","permalink":"http://changleamazing.com/tags/Data-Persistence/"}]},{"title":"HashMap 源码解析","date":"2019-12-11T16:52:50.000Z","path":"2019/12/12/HashMap/","text":"本文基于 JDK8 源码深入分析 HashMap 的结构与重要操作，并梳理一些面试中的常见问题。 类结构 成员变量DEFAULT_INITIAL_CAPACITY1static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; HashMap 中槽数量的默认值，即 HashMap 中 table 数组的 table.length; HashMap 初始化时，如果未指定 capacity 时，即设定 capacity 为此值。 MAXIMUM_CAPACITY1static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; HashMap 中槽数量的最大值。 HashMap 初始化时，如果指定的 capacity 大于该值，则将 capacity 设置为该值。 table1transient Node&lt;K,V&gt;[] table; HashMap 中装载数据的桶的数组。table.length 在分配数据之后长度总是 2 的幂。（除了在自举机制中一些操作允许长度为 0） entrySet1transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; 装载了所有键值对的集合，在遍历 HashMap 时一般使用这个集合，比使用 keySet 集合遍历速度大约快 1 倍。 size1transient int size; HashMap 中目前存放的键值对的个数。 modCount1transient int modCount; HashMap 结构更改的次数（即增删操作的次数）。用于在遍历时保证fail-fast 机制生效。 thresholdHashMap 中存放键值对的阈值，当 size &gt; threshold 时，会触发 table 数组扩容操作。 table 数组没有被分配数据时，threshold 值等于 0 或者是 table.length。而在 table 数组被分配数据之后，它的值等于 table.length * loadFactor。 loadFactor装载因子：即负载率；默认为 0.75。 JDK 1.7 中提到， As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur. 意思是 0.75 在时间和空间上提供了很好的折中。由于 threshold = capacity * loadFactor ，如果 loadFactor 设置过高，可以节省少量空间，但是会导致 threshold 和 capacity 非常接近， Hash 碰撞 的概率增大，一定程度上提高了 put 和 get 操作的耗时；如果 loadFactor 设置过低，则会产生相反的效果。 TREEIFY_THRESHOLD1static final int TREEIFY_THRESHOLD = 8; table数组中的每个槽中存储的数据量，在 大于 该值时，槽中存储数据的数据结构会变为红黑树。 ### UNTREEIFY_THRESHOLD 1static final int UNTREEIFY_THRESHOLD = 6; table`数组中的每个槽中存储的数据量，在 小于 该值时，槽中存储数据的数据结构会变为链表。 构造方法public HashMap()123public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; 最常用的构造方法。 loadFactor 为默认值 0.75 ; public HashMap(int initialCapacity)123public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; 调用 public HashMap(int initialCapacity, float loadFactor); 手动设置 HashMap 的初始化容量。 loadFactor 为默认值 0.75 ; public HashMap(int initialCapacity, float loadFactor)123456789101112public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; loadFactor 为自定义值 ; static final int tableSizeFor(int cap)123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 从注释可知，该方法的返回值为 2 的幂。 实际上这个算法返回的是，大于 cap 的最接近 cap 的 2 的 次幂。 该构造方法，实际上控制的是 threshold 而不是直接设置 capacity。 public HashMap(Map&lt;? extends K, ? extends V&gt; m)1234public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; 用另一个 map 构造一个新的 hashmap。 loadFactor 为默认值。 final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict)1234567891011121314151617181920/** 若 evict 为 false,代表是在创建 hashMap 时调用了这个函数;若 evict 为true,代表是在创建 hashMap 后才调用这个函数，例如 putAll 函数。*/final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; if (table == null) &#123; // pre-size float ft = ((float)s / loadFactor) + 1.0F;// 阿里推荐初始化值 int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); if (t &gt; threshold) threshold = tableSizeFor(t); &#125; else if (s &gt; threshold) resize(); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; 初始化时， ft = ((float)s / loadFactor) + 1.0F,即为 table.length + 1。然后调用 threshold = tableSizeFor(t)；这样 threshold 的值，是 2 * table.length。所以，新增元素不会立刻导致扩容。 查public V get(Object key)1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; static final int hash(Object key)1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 该函数主要是调用了 key.hashCode( ) 方法，实际就是 Object 类中的 hashCode() 方法。作用是将对象的地址映射成一个整数值，尽量保证随机性。而 HashMap 中没有直接使用 Object 中的 hashCode() 的返回值作为 hash() 函数的结果，而是增加了 (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16) 这一步,将 hashCode() 的返回值与向右移动 16 位的 h 做异或运算。这里，(h &gt;&gt;&gt; 16) 叫做 扰动函数，该扰动函数保证了函数最后的返回值的后十六位中，是高位与低位共同运算出的结果。增加了节点在 table 数组中分布的随机性。 结果显示，当 HashMap 数组长度为 512 时，这个时候会取低 9 位的值来决定新增节点的位置。在有扰动函数的情况下，碰撞会减少 10%。 final Node&lt;K,V&gt; getNode(int hash, Object key)123456789101112131415161718192021222324final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; /* 条件判断，判断 `table` 数组不为空，且有元素存在；在该节点对应的槽里面也要有元素存在 */ if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断 key 是不是该槽位中的第一个元素。 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 如果该槽位中不止存在一个值，判断该槽位的节点是不是树节点 if ((e = first.next) != null) &#123; //如果是树节点，直接调用树查找节点的方法。并返回查找到的值 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 如果该槽位中的数据仍用链表存储。则直接遍历判断元素的 key 是不是等于要查找的 key do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 这是 HashMap 查找方法的主流程，相对比较简单。在进行最外层判断时，使用到了 tab[(n - 1) &amp; hash] 这段代码。tab[(n - 1) &amp; hash] 是 table.length 必须保持 2 的 次幂的关键。在得到一个元素 key 哈希运算返回值 hash 后，为了找到该元素在 table 中具体分布在哪个槽中，一般会使用 hash % table.length。当 table.length 等于 2^n 时， 存在hash % table.length = hash &amp; (table.length - 1)。这样，由于位运算更快，可以更加快速的找到每一个节点对应的槽位。 增/改public V put(K key, V value)123456789101112 /** * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; //evict 为true,代表是在创建 hashMap 后才调用这个函数 /** onlyIfAbsent 表示是否只在没有该节点映射时，put 才生效（是否允许覆盖）。 false 表示允许覆盖操作 **/ return putVal(hash(key), key, value, false, true);&#125; 如果 HashMap 中已经存在该节点的映射（更新操作），返回值会是旧节点的 value。如果不存在该节点的映射（新增操作），返回值会是 null。所以可以用返回值来判断原来的 HashMap 中是否存在关于该节点的映射，在某些时刻很有用。 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果数组没有初始化，或者长度为 0，则重新设置数组长度 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果该节点 key 对应的槽位没有元素，直接新建节点将该元素放入该槽位 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; /** 如果槽位的第一个元素 p 的 key 与 带插入的节点的 key 相等，则直接令 e = p，此时 e.value 被 p.value 替代，相当于更新操作 **/ if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; /** 如果槽位的元素是树节点，调用树的插入值的方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 槽中元素为链表节点 else &#123; for (int binCount = 0; ; ++binCount) &#123; // 判断 p 是否为尾结点 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 判断新增节点后，是否需要更新数据结构，槽中节点数等于 8 就更新 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果链表中有元素的 key 等于 e.key，则更新 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果 e 不为 null，说明之前存在该 key 的映射， if (e != null) &#123; // existing mapping for key V oldValue = e.value; // 允许覆盖则更新节点值 if (!onlyIfAbsent || oldValue == null) e.value = value; // 为 linkedHashMap 提供的函数，将最近访问的元素置于链表尾部，保证链表有序性 afterNodeAccess(e); return oldValue; &#125; &#125; // 修改次数增加 ++modCount; // 判断是否需要扩容 if (++size &gt; threshold) resize(); // 为 linkedHashMap 提供的函数，回调删除头节点 afterNodeInsertion(evict); return null; &#125; public V putIfAbsent(K key, V value)123public V putIfAbsent(K key, V value) &#123; return putVal(hash(key), key, value, true, true); &#125; 第四个参数 onlyIfAbsent 为 true 表示只允许插入操作，更新操作不生效。 删public V remove(Object key)12345public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; 如果返回值为空，表示 HashMap 中不存在该 key 对应的节点。否则，返回对应节点的 value。 final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; // 判断数组有元素存在且 key 对应的槽位有元素存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 如果槽中第一个元素 p 是要删除的节点，令 node = p if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; // 槽中第一个元素 p 不是要删除的节点，在后继节点中寻找 else if ((e = p.next) != null) &#123; // 槽中元素存储在 RBT 中，令 node 等于从树中查找到的节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); // 槽中元素存储在链表中，令 node 等于从链表中查找到的节点 else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // node 不为空, 且不需要匹配 value 或者成功匹配到 value，删除节点 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); // 如果要删除的节点为槽中第一个节点，则将第二个节点作为首节点 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; 扩容final Node&lt;K,V&gt;[] resize()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果数组已经初始化过且 table.length != 0 if (oldCap &gt; 0) &#123; /** 如果数组原来的长度为 MAXIMUM_CAPACITY，table.length 无法扩大， 修改 threshold = Integer.MAX_VALUE 使 map 可以继续存放元素**/ if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 扩大 table.length = min(oldCap * 2 ,MAXIMUM_CAPACITY) //只有原来 oldCap.length &gt;= 16，会使阈值翻倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //原来 table.length = 0 且 threshold ！= 0 ，在带参非集合初始化时会出现这种情况。 //设置 newCap 为初始化时构造函数中 tableSizeFor() 方法返回的 threshold else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // table.length = 0 &amp;&amp; threshold = 0 ，无参初始化时出现这种情况 // 设置 cap 和 threshold 为默认值 else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //有两种情况会符合该判断 //1. 原来的 table.length * 2 &gt;= MAXIMUM_CAPACITY， 此时将 threshold 设置为 Integer.MAX_VALUE //2. 当原来的 table.length &lt; 16 时，设置 threshold = min(MAXIMUM_CAPACITY,threshold * loadFactor)。(故意设置 loadFactor 很高时，会出现 threshold * loadFactor &gt; MAXIMUM_CAPACITY) if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 移动元素 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; //方便 GC oldTab[j] = null; // 如果该槽中只有一个元素，新数组的槽中依然只有它一个元素 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果槽中数据结构为 RBT else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 如果槽中数据结构为链表 else &#123; // preserve order // Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 确定该节点在 resize 之后是否会改变索引值 // e.hash &amp; oldCap = 0，说明索引值在 resize 之后不会改变 if ((e.hash &amp; oldCap) == 0) &#123; //将元素放在索引为 index 的链表尾部 if (loTail == null) loHead = e; else //这里说明了 JDK8 是尾插法 loTail.next = e; loTail = e; &#125; else &#123; //将元素放在索引为 index + oldCap 的链表尾部 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //将链表尾结点置空 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 其中有一些很关键的点， 移动元素前会使判断 e.hash &amp; oldCap 是否等于 0。假设 oldCap 为 2^k ，元素m 的索引为 oldIndex，hash(m) 的值为 hash，存在 oldIndex= hash &amp; ( 2^k - 1 )。(2^k - 1) 的结果的二进制表达式是 k 个 1。所以 oldIndex的结果等于 hash 的二进制表达式的后 k 位的值。 扩容后，新数组的长度 newCap 为 2^(k + 1)。元素 m 在新数组中的索引 newIndex = hash &amp; (2 ^ (k + 1) - 1)。同上，newIndex 的结果等于 hash 的二进制表达式的后 k + 1 位的值。用 b 代表 hash二进制表示的第k + 1位，那么 newIndex - oldIndex 就等于 b 代表的值。 b的权值为 2^k等于 oldCap。当 b = 0时， newIndex = oldIndex；当 b = 1时，newIndex = oldIndex + oldCap。所以，元素m在新数组的位置就由hash的第 k + 1位的值确定，这个值就等于hash &amp; oldCap。 因此，我们在扩充 HashMap 的时候，不需要像 JDK1.7 的实现那样重新计算 hash 了。 另一个关键点，在于移动时，使用尾插法，保持了元素在原来槽中的相对顺序。这个方法，解决了 JDK1.7 中多线程访问 HashMap 时，resize 过程中会出现的循环链表的问题。但 HashMap 仍然不是线程安全的。 重要问题JDK8 相对于 JDK7 优化点有哪些JDK8 对于 HashMap 的改动很大。主要的优化点在： HashMap 中使用的数据结构新增红黑树，当哈希冲突严重时，查找元素的耗时也不会恶化到 O(n) 级别。 插入元素的方式。JDK8 采用了尾插法插入元素，在扩容时保持了原来元素的相对顺序。而 JDK7 采用的是头插法，多线程扩容时可能会导致产生闭环问题。 扩容时，HashMap 中元素索引直接由元素 hashcode来计算是原位置或者是原位置 + 数组长度。而在 JDK7 中，元素扩容时，都会调用 hash() 方法重新计算元素的 hashcode ，再决定元素在数组中的索引。 JDK1.7 死循环问题当多线程添加元素并且引起扩容时，可能会触发 HashMap 中某个链表死循环。主要的原因是 JDK1.7 使用的头插法，导致原来两个节点的顺序在扩容后被翻转，多线程操作时就可能引起死循环。而在 JDK1.8 中，扩容时使用的是尾插法插入元素，这样元素的相对顺序不会改变，所以不会再出现死循环的问题。 JDK8 线程安全问题JDK8 中解决了 HashMap 死循环之后，依然不是线程的。 举两个例子： 当多线程放入两个 hashcode 一致的元素时，两个元素会放入相同的槽中，当他们获取到了同一个链表尾部元素时，会将各自的元素标记为链表尾部，导致其中一个元素丢失。 当多线程放入新元素时，都会执行到 ++size 这一步，表示 HashMap 中保存的元素数量增加了，但是 size 并不是 volatile 修饰的，多线程操作时可能会导致值被覆盖，从而 size 与实际数据不对。","tags":[{"name":"HashMap","slug":"HashMap","permalink":"http://changleamazing.com/tags/HashMap/"},{"name":"Source Code","slug":"Source-Code","permalink":"http://changleamazing.com/tags/Source-Code/"}]},{"title":"Redis 分布式锁","date":"2019-11-17T16:52:50.000Z","path":"2019/11/18/分布式锁/","text":"在单节点中，需要用并发线程都能访问到的资源的状态变化来控制同步。在分布式应用中，使用应用所有节点都能访问到的 Redis 中的某个 key 来控制多节点访问。 单节点 Redis 分布式锁setnxsetnx 指令会在 key 不存在的情况下放入 redis，如果存在则不会设置。 123456&gt;setnx lock:distributed trueOK...other code...&gt;del lock:distributed 这种方式的问题在于，执行到 other code 时，程序出现异常，导致 del 指令不会被执行，key 没有被释放，这样会陷入死锁。 setnx then expire为了解决死锁，乍一看可以使用 expire 来给 key 设置超时时间。 1234567&gt;setnx lock:distributed trueOK&gt;expire lock:distributed 5...other code...&gt;del lock:distributed 这种处理其实仍然有问题，因为 setnx 与 expire 不是原子操作， 执行 expire 语句之前可能发生异常。死锁仍然会出现。 set and expire为了解决非原子性操作被中断的问题，在 Redis 2.8 中加入了 setnx 与 expire 组合在一起的原子指令。 123456&gt;set lock:distributed true ex 5 nxOK...other code...&gt;del lock:distributed 这种方式保证了加锁并设置有效时间操作的原子性，但是依然有问题。 假设我们在加锁与释放锁之间的业务代码执行时间超过了设置的有效时间，此时锁会因为超时被释放。会导致两种情况： 其他节点 B 获取锁之后，执行超时节点 A 执行完成，释放了 B 的锁。 其它节点获取到了锁，执行临界区代码时就可能会出现并发问题。 解决锁被其他线程释放问题因为在加锁时，各个节点使用的同一个 key，所以会存在超时节点释放了当前加锁节点的锁的情况。这种情况下，可以给加锁的 key 设置一个随机值，删除的时候需要判断 key 当前的 value 是不是等于随机值。 12345678910val &#x3D; Random.nextInt();if( redis.set(key,val,true,5) )&#123; ... other code ... value &#x3D; redis.get(key); if(val &#x3D;&#x3D; value)&#123; redis.delete(key); &#125;&#125; 上述代码实现了根据随机值删除的逻辑，但是获取 value 直到 delete 指令并非是原子指令，仍然可能有并发问题。这时候需要使用 lua 脚本处理，因为 lua 脚本可以保证连续多个指令原子执行。 12345if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1])else return 0end 这种方式可以避免锁被其他线程释放的问题。 临界区并发问题临界区代码出现并发问题的本质是业务代码执行时间大于锁过期时间。 我们可以定时刷新加锁时间，保证业务代码在锁过期时间内执行完成。 12345678910111213141516171819202122232425262728293031private volatile boolean isFlushExpiration = true;while(redis.set(lock, val, NOT_EXIST, SECONDS, 20))&#123; Thread thread = new Thread(new FlushExpirationTherad()); thread.setDeamon(true); thread.start(); ... other code ... &#125;isFlushExpiration = false;String deleteScript = \"if redis.call(\"get\",KEYS[1]) == ARGV[1] then\" + \"return redis.call(\"del\",KEYS[1])\" + \"else return 0 end\";redis.eval(deleteScript,1,key,val); private class FlushExpirationTherad implements Runnable&#123; @Override public void run()&#123; while(isFlushExpiration)&#123; String checkAndExpireScript = \"if redis.call('get', KEYS[1]) == ARGV[1] then \" + \"return redis.call('expire',KEYS[1],ARGV[2]) \" + \"else return 0 end\"; redis.eval(checkAndExpireScript,1,key,val,\"20\"); // 每隔十秒检查是否完成 Thread.sleep(10); &#125; &#125; &#125; 这种实现是用一个线程定期监控客户端是否执行完成。也可以由服务端实现心跳检测机制来保证业务完成（Zookeeper)。 所以实现单节点 Redis 分布式锁要关注三个关键问题： 获取锁与设置超时时间实现为原子操作（Redis2.8 开始已支持） 设置随机字符串保证释放锁时能保证只释放自己持有的锁（给对应的 key 设置随机值） 判断与释放锁必须实现为原子操作（lua 脚本实现） 多节点 Redis 分布式锁为了保证项目的高可用性，项目一般都配置了 Redis 集群，以防在单节点 Redis 宕机之后，所有客户端都无法获得锁。 在集群环境下，Redis 存在 failover 机制。当 Master 节点宕机之后，会开始异步的主从复制（replication），这个过程可能会出现以下情况： 客户端 A 获取了 Master 节点的锁。 Master 节点宕机了，存储锁的 key 暂未同步到 Slave 上。 Slave 节点升级为 Master 节点。 客户端 B 从新的 Master 节点上获取到了同一资源的锁。 在这种情况下，锁的安全性就会被打破，Redis 作者 antirez 针对此问题设计了 Redlock 算法。 Redlock 算法Redlock 算法获取锁时客户端执行步骤： 获取当前时间（start）。 依次向 N 个 Redis 节点请求锁。请求锁的方式与从单节点 Redis 获取锁的方式一致。为了保证在某个 Redis 节点不可用时该算法能够继续运行，获取锁的操作都需要设置超时时间，需要保证该超时时间远小于锁的有效时间。这样才能保证客户端在向某个 Redis 节点获取锁失败之后，可以立刻尝试下一个节点。 计算获取锁的过程总共消耗多长时间（consumeTime = end - start）。如果客户端从大多数 Redis 节点（&gt;= N/2 + 1) 成功获取锁，并且获取锁总时长没有超过锁的有效时间，这种情况下，客户端会认为获取锁成功，否则，获取锁失败。 如果最终获取锁成功，锁的有效时间应该重新设置为锁最初的有效时间减去 consumeTime。 如果最终获取锁失败，客户端应该立刻向所有 Redis 节点发起释放锁的请求。 在释放锁时，需要向所有 Redis 节点发起释放锁的操作，不管节点是否获取锁成功。因为可能存在客户端向 Redis 节点获取锁时成功，但节点通知客户端时通信失败，客户端会认为该节点加锁失败。 Redlock 算法实现了更高的可用性，也不会出现 failover 时失效的问题。但是如果有节点崩溃重启，仍然对锁的安全性有影响。假设共有 5 个 Redis 节点 A、B、C、D、E： 客户端 A 获取了 A、B、C 节点的锁，但 D 与 E 节点的锁获取失败。 节点 C 崩溃重启，但是客户端 A 在 C 上加的锁没有持久化下来，重启后丢失 节点 C 重启后，客户端 B 锁住了 C、D、E，获取锁成功。 在这种情况下，客户端 A 与 B 都获取了访问同一资源的锁。 这里第 2 步中节点 C 锁丢失的问题可能由多种原因引起。默认情况下，Redis 的 AOF 持久化方式是每秒写一次磁盘（fsync），这情况下就有可能丢失 1 秒的数据。我们也可以设置每次操作都触发 fsync，这会影响性能，不过即使这样设置，也有可能由于操作系统的问题导致操作写入失败。 为了解决节点重启导致的锁失效问题，antirez 提出了延迟重启的概念，即当一个节点崩溃之后并不立即重启，而是等待与分布式锁相关的 key 的有效时间都过期之后再重启，这样在该节点重启后也不会对现有的锁造成影响。 一些插曲关于 Redlock 的安全性问题，在分布式系统专家 Martin Kleppmann 和 Redis 的作者 antirez 之间发生过一场争论，这个问题引发了激烈的讨论。关于这场争论的内容可以关注 基于Redis的分布式锁到底安全吗 这篇文章。最后得出的结论是 Redlock 在效率要求的应用中是合理的，所以在 Java 项目中可以使用 Redlock 的 Java 版本 Redission 来控制多节点访问共享资源。但是仍有极端情况会造成 Redlock 的不安全，我们应该知道它在安全性上有哪些不足以及会造成什么后果。如果需要进一步的追求正确性，可以使用 Zookeeper 分布式锁。 相关链接 基于Redis的分布式锁到底安全吗","tags":[{"name":"Distributed Lock","slug":"Distributed-Lock","permalink":"http://changleamazing.com/tags/Distributed-Lock/"},{"name":"Redis","slug":"Redis","permalink":"http://changleamazing.com/tags/Redis/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://changleamazing.com/tags/Distributed-System/"}]}]